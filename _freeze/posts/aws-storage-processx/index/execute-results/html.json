{
  "hash": "3d6ccac235964ccc151cb93dbf711863",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Syncing files to aws with R\"\nauthor: \"al\"\ndate: \"2024-05-23\"\ndate-modified: \"2024-05-24\"\ncategories: [news, assets, aws, s3, r, paws]\nimage: \"image.jpg\"\nparams:\n  repo_owner: \"NewGraphEnvironment\"\n  repo_name: \"new_graphiti\"\nformat: \n  html:\n    code-fold: true\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, dpi=60, out.width = \"100%\")\noptions(scipen=999)\noptions(knitr.kable.NA = '--') #'--'\noptions(knitr.kable.NAN = '--')\n```\n:::\n\n\nInspired by https://blog.djnavarro.net/posts/2022-03-17_using-aws-s3-in-r/ by [Danielle Navarro](https://github.com/djnavarro).\n\nNote to self - `/Users/airvine/Projects/repo/new_graphiti/_freeze/posts/aws-storage-processx/index/execute-results/html.json`\nis created when I render this document. This seems to be what is published to website after 1. the `github_actions` workflow is run to generate the `gh-pages` branch (on github runner) 2. the site is published with `gitpages` from `github`.\n\n\"Quick\" post to document where I got to with syncing files to aws with R. Didn't love the `aws.s3::sync` function because\nfrom what I could tell I could not tell it to delete files if they were not present locally or in a bucket (I could be wrong). \n\nThen climbed into `s3fs` which mirrors the `fs` package and seems a bit more user friendly than the `aws.s3` package\nfor managing files.  It is created by [Dyfan Jones](https://github.com/DyfanJones) who also is the top contributor to\n`paws`!!  He seems like perhaps as much of a beast as one of the contributors to `s3fs` who is [Scott Chamberlain](https://github.com/sckott).\n\n\nFor the sync issue figured why not just call the `aws` command line tool from R. `processx` is an insane package that might be the mother of all packages. It allows you to run command line tools from R with flexibility for some things like setting the directory where the command is called from in the `processx` called function (big deal as far as I can tell). \n\n\nWe need to set up our `aws` account online. The blog above from [Danielle Navarro](https://github.com/djnavarro) covers that I believe (I struggled through it a long time ago). I should use a `~/.aws/credentials` file but don't yet.  I have my credentials in my `~/.Renviron` file as well as in my `~/.bash_profile` (probably a ridiculous setup). They are:\n\n    AWS_ACCESS_KEY_ID='my_key'\n    AWS_DEFAULT_REGION='my_region'\n    AWS_SECRET_ACCESS_KEY='my_secret_key'\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# library(aws.s3)\nlibrary(processx)\n# library(paws) #this is the mom.  Couple examples of us hashed out here\nlibrary(s3fs)\n# library(aws.iam) #not useing - set permissions\nlibrary(here) #helps us with working directory issues related to the `environment` we operate in when rendering\n```\n:::\n\n\n# See buckets using the `s3fs` package.  \n\n<br>\n\nCurrent buckets are:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns3fs::s3_dir_ls(refresh = TRUE) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"s3://23cog\"\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# First we set up our AWS s3 file system. I am actually not sure this is necessary but I did it.  Will turn the chunk off\n# to not repeat.\n# s3fs::s3_file_system(profile_name = \"s3fs_example\")\n```\n:::\n\n\n# Create a Bucket\nLet's generate the name of the bucket based on the name of the repo but due to `aws` [bucket naming rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html) we need to swap out our underscores\nfor hyphens!  Maybe a good enough reason to change our naming conventions for our repos on github!!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbucket_name <- basename(here::here()) |> \n  stringr::str_replace_all(\"_\", \"-\") \n\nbucket_path <- s3fs::s3_path(bucket_name)\n\ns3fs::s3_bucket_create( bucket_path)  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"s3://new-graphiti\"\n```\n\n\n:::\n:::\n\n\n# Sync Files to Bucket\nWe build a little wrapper function to help us debug issues when running system commands with `processx`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsys_call <- function(){\n  result <- tryCatch({\n    processx::run(\n      command,\n      args = args,\n      echo = TRUE,            # Print the command output live\n      wd = working_directory, # Set the working directory\n      spinner = TRUE,         # Show a spinner\n      timeout = 60            # Timeout after 60 seconds\n    )\n  }, error = function(e) {\n    # Handle errors: e.g., print a custom error message\n    cat(\"An error occurred: \", e$message, \"\\n\")\n    NULL  # Return NULL or another appropriate value\n  })\n  \n  # Check if the command was successful\n  if (!is.null(result)) {\n    cat(\"Exit status:\", result$status, \"\\n\")\n    cat(\"Output:\\n\", result$stdout)\n  } else {\n    cat(\"Failed to execute the command properly.\\n\")\n  }\n}\n```\n:::\n\n\n<br>\n\nThen we specify our command and arguments. To achieve the desired behavior of including only files in the `assets/*` directory, you need to combine the order of `--exclude` and `--include` flags appropriately (exclude everything first thenn include what we want):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncommand <- \"aws\"\nargs <- c('s3', 'sync', '.', bucket_path, '--delete', '--exclude', '*', '--include', 'posts/*')\n\nworking_directory = here::here() #we could just remove from funciton to get the current wd but its nice to have so we leave\n```\n:::\n\n\nNow lets put a tester file in our directory and sync it to our bucket. We will remove it later to test if it is removed on sync.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfile.create(here::here('posts/test.txt'))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\nRun our little function to sync the files to the bucket.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsys_call()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCompleted 237 Bytes/511.7 KiB (3.0 KiB/s) with 12 file(s) remaining\nupload: posts/_metadata.yml to s3://new-graphiti/posts/_metadata.yml\nCompleted 237 Bytes/511.7 KiB (3.0 KiB/s) with 11 file(s) remaining\nCompleted 2.0 KiB/511.7 KiB (11.9 KiB/s) with 11 file(s) remaining \nupload: posts/logos-equipment/index.qmd to s3://new-graphiti/posts/logos-equipment/index.qmd\nCompleted 2.0 KiB/511.7 KiB (11.9 KiB/s) with 10 file(s) remaining\nCompleted 3.6 KiB/511.7 KiB (20.8 KiB/s) with 10 file(s) remaining\nupload: posts/snakecase/index.qmd to s3://new-graphiti/posts/snakecase/index.qmd\nCompleted 3.6 KiB/511.7 KiB (20.8 KiB/s) with 9 file(s) remaining\nCompleted 8.6 KiB/511.7 KiB (48.5 KiB/s) with 9 file(s) remaining\nupload: posts/snakecase/thumbnail.jpg to s3://new-graphiti/posts/snakecase/thumbnail.jpg\nCompleted 8.6 KiB/511.7 KiB (48.5 KiB/s) with 8 file(s) remaining\nCompleted 13.9 KiB/511.7 KiB (74.3 KiB/s) with 8 file(s) remaining\nupload: posts/aws-storage-permissions/index.qmd to s3://new-graphiti/posts/aws-storage-permissions/index.qmd\nCompleted 13.9 KiB/511.7 KiB (74.3 KiB/s) with 7 file(s) remaining\nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 7 file(s) remaining\nupload: posts/aws-storage-processx/image.jpg to s3://new-graphiti/posts/aws-storage-processx/image.jpg\nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 6 file(s) remaining\nupload: posts/test.txt to s3://new-graphiti/posts/test.txt        \nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 5 file(s) remaining\nCompleted 26.0 KiB/511.7 KiB (119.1 KiB/s) with 5 file(s) remaining\nupload: posts/aws-storage-processx/index.rmarkdown to s3://new-graphiti/posts/aws-storage-processx/index.rmarkdown\nCompleted 26.0 KiB/511.7 KiB (119.1 KiB/s) with 4 file(s) remaining\nCompleted 34.0 KiB/511.7 KiB (152.0 KiB/s) with 4 file(s) remaining\nupload: posts/aws-storage-permissions/image.jpg to s3://new-graphiti/posts/aws-storage-permissions/image.jpg\nCompleted 34.0 KiB/511.7 KiB (152.0 KiB/s) with 3 file(s) remaining\nCompleted 42.2 KiB/511.7 KiB (180.5 KiB/s) with 3 file(s) remaining\nupload: posts/aws-storage-processx/index.qmd to s3://new-graphiti/posts/aws-storage-processx/index.qmd\nCompleted 42.2 KiB/511.7 KiB (180.5 KiB/s) with 2 file(s) remaining\nCompleted 83.0 KiB/511.7 KiB (256.0 KiB/s) with 2 file(s) remaining\nupload: posts/logos-equipment/image.jpg to s3://new-graphiti/posts/logos-equipment/image.jpg\nCompleted 83.0 KiB/511.7 KiB (256.0 KiB/s) with 1 file(s) remaining\nCompleted 339.0 KiB/511.7 KiB (830.9 KiB/s) with 1 file(s) remaining\nCompleted 511.7 KiB/511.7 KiB (654.4 KiB/s) with 1 file(s) remaining\nupload: posts/snakecase/all.jpeg to s3://new-graphiti/posts/snakecase/all.jpeg\nExit status: 0 \nOutput:\n Completed 237 Bytes/511.7 KiB (3.0 KiB/s) with 12 file(s) remaining\nupload: posts/_metadata.yml to s3://new-graphiti/posts/_metadata.yml\nCompleted 237 Bytes/511.7 KiB (3.0 KiB/s) with 11 file(s) remaining\nCompleted 2.0 KiB/511.7 KiB (11.9 KiB/s) with 11 file(s) remaining \nupload: posts/logos-equipment/index.qmd to s3://new-graphiti/posts/logos-equipment/index.qmd\nCompleted 2.0 KiB/511.7 KiB (11.9 KiB/s) with 10 file(s) remaining\nCompleted 3.6 KiB/511.7 KiB (20.8 KiB/s) with 10 file(s) remaining\nupload: posts/snakecase/index.qmd to s3://new-graphiti/posts/snakecase/index.qmd\nCompleted 3.6 KiB/511.7 KiB (20.8 KiB/s) with 9 file(s) remaining\nCompleted 8.6 KiB/511.7 KiB (48.5 KiB/s) with 9 file(s) remaining\nupload: posts/snakecase/thumbnail.jpg to s3://new-graphiti/posts/snakecase/thumbnail.jpg\nCompleted 8.6 KiB/511.7 KiB (48.5 KiB/s) with 8 file(s) remaining\nCompleted 13.9 KiB/511.7 KiB (74.3 KiB/s) with 8 file(s) remaining\nupload: posts/aws-storage-permissions/index.qmd to s3://new-graphiti/posts/aws-storage-permissions/index.qmd\nCompleted 13.9 KiB/511.7 KiB (74.3 KiB/s) with 7 file(s) remaining\nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 7 file(s) remaining\nupload: posts/aws-storage-processx/image.jpg to s3://new-graphiti/posts/aws-storage-processx/image.jpg\nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 6 file(s) remaining\nupload: posts/test.txt to s3://new-graphiti/posts/test.txt        \nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 5 file(s) remaining\nCompleted 26.0 KiB/511.7 KiB (119.1 KiB/s) with 5 file(s) remaining\nupload: posts/aws-storage-processx/index.rmarkdown to s3://new-graphiti/posts/aws-storage-processx/index.rmarkdown\nCompleted 26.0 KiB/511.7 KiB (119.1 KiB/s) with 4 file(s) remaining\nCompleted 34.0 KiB/511.7 KiB (152.0 KiB/s) with 4 file(s) remaining\nupload: posts/aws-storage-permissions/image.jpg to s3://new-graphiti/posts/aws-storage-permissions/image.jpg\nCompleted 34.0 KiB/511.7 KiB (152.0 KiB/s) with 3 file(s) remaining\nCompleted 42.2 KiB/511.7 KiB (180.5 KiB/s) with 3 file(s) remaining\nupload: posts/aws-storage-processx/index.qmd to s3://new-graphiti/posts/aws-storage-processx/index.qmd\nCompleted 42.2 KiB/511.7 KiB (180.5 KiB/s) with 2 file(s) remaining\nCompleted 83.0 KiB/511.7 KiB (256.0 KiB/s) with 2 file(s) remaining\nupload: posts/logos-equipment/image.jpg to s3://new-graphiti/posts/logos-equipment/image.jpg\nCompleted 83.0 KiB/511.7 KiB (256.0 KiB/s) with 1 file(s) remaining\nCompleted 339.0 KiB/511.7 KiB (830.9 KiB/s) with 1 file(s) remaining\nCompleted 511.7 KiB/511.7 KiB (654.4 KiB/s) with 1 file(s) remaining\nupload: posts/snakecase/all.jpeg to s3://new-graphiti/posts/snakecase/all.jpeg\n```\n\n\n:::\n:::\n\n\nThen we can see our bucket contents - as well as list our bucket contents and capture them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns3fs::s3_dir_tree(bucket_path)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ns3://new-graphiti\n└── posts\n    ├── _metadata.yml\n    ├── test.txt\n    ├── aws-storage-permissions\n    │   ├── image.jpg\n    │   └── index.qmd\n    ├── aws-storage-processx\n    │   ├── image.jpg\n    │   ├── index.qmd\n    │   └── index.rmarkdown\n    ├── logos-equipment\n    │   ├── image.jpg\n    │   └── index.qmd\n    └── snakecase\n        ├── all.jpeg\n        ├── index.qmd\n        └── thumbnail.jpg\n```\n\n\n:::\n\n```{.r .cell-code}\nt <- s3fs::s3_dir_info(bucket_path, recurse = TRUE)\n```\n:::\n\n\nNow we will remove `test.txt`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfile.remove(here::here('posts/test.txt'))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\nNow we sync again.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsys_call()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndelete: s3://new-graphiti/posts/test.txt\nExit status: 0 \nOutput:\n delete: s3://new-graphiti/posts/test.txt\n```\n\n\n:::\n:::\n\n\nList our bucket contents and capture them again\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns3_dir_tree(bucket_path)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ns3://new-graphiti\n└── posts\n    ├── _metadata.yml\n    ├── aws-storage-permissions\n    │   ├── image.jpg\n    │   └── index.qmd\n    ├── aws-storage-processx\n    │   ├── image.jpg\n    │   ├── index.qmd\n    │   └── index.rmarkdown\n    ├── logos-equipment\n    │   ├── image.jpg\n    │   └── index.qmd\n    └── snakecase\n        ├── all.jpeg\n        ├── index.qmd\n        └── thumbnail.jpg\n```\n\n\n:::\n\n```{.r .cell-code}\nt2 <- s3fs::s3_dir_info(bucket_path, recurse = TRUE)\n```\n:::\n\nCompare the file structure before and after our sync.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwaldo::compare(t$key, t2$key)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     old                             | new                                 \n [9] \"posts/snakecase/all.jpeg\"      | \"posts/snakecase/all.jpeg\"      [9] \n[10] \"posts/snakecase/index.qmd\"     | \"posts/snakecase/index.qmd\"     [10]\n[11] \"posts/snakecase/thumbnail.jpg\" | \"posts/snakecase/thumbnail.jpg\" [11]\n[12] \"posts/test.txt\"                -                                     \n```\n\n\n:::\n:::\n\n\nSuccess!! \n\n# To Do\nWe need to build the call to sync the other way (cloud to local) in a way that perhaps nukes local files if they are\nnot on the cloud.  This is because we need to collaborate within our team so we do things like one person will change\nthe name of images so when the other person syncs they will have only the newly named image in their local directory. \n\n<br>\n\nThis all deserved consideration as it could get really messy from a few different angles (ie. one person adds files they \ndon't want nuked and then they get nukes. There are lots of different options for doing things so we will get there.)\n\n# Delete Bucket\nLets delete the bucket.  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#\nHere is the command line approach that we will turn off in favor of the s3fs approach.\nargs <- c('s3', 'rb', bucket_path, '--force')\nsys_call()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Here is the `s3fs` way to \"delete\" all the versions.  \n# list all the files in the bucket\nfl <- s3fs::s3_dir_ls(bucket_path, recurse = TRUE, refresh = TRUE)\n\n# list all the version info for all the files\nvi <- fl |> \n  purrr::map_df(s3fs::s3_file_version_info)\n\ns3fs::s3_file_delete(path = vi$uri)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ns3fs::s3_bucket_delete(bucket_path)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"s3://new-graphiti\"\n```\n\n\n:::\n:::\n\n\nAs we have tried this before we know that if we tell it we want to delete a bucket with versioned files in it we need to\nempty the bucket first including `delete_markers`.  That is easy in the `aws console` with th UI but seems tricky.\nThere is a bunch of discussion on options to this here\nhttps://stackoverflow.com/questions/29809105/how-do-i-delete-a-versioned-bucket-in-aws-s3-using-the-cli .  Thinking a\ngood way around it (and a topic for another post) would be to apply a `lifecycle-configuration` to the bucket that\ndeletes all versions of files after a day - allowing you to delete bucket after they expire (as per the above post).\nReally we may want to have a `lifecycle-configuration` on all our versioned buckets to keep costs down anyway but\ndeserves more thought and perhaps another post.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# old notes\n# We are going to test creating a bucket with versioning on.  This has large implications for billing with some details\n# of how it works [here](https://aws.amazon.com/blogs/aws/amazon-s3-enhancement-versioning/) with example of costs [here](https://aws.amazon.com/s3/faqs/?nc1=h_ls).  Thinking we may want versioned buckets for things like `sqlite`\n# \"snapshot\" databases but definitely not for things like images. \n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}