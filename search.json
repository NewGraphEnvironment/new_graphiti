[
  {
    "objectID": "posts/aws-storage-permissions/index.html",
    "href": "posts/aws-storage-permissions/index.html",
    "title": "Setting aws bucket permissions with R",
    "section": "",
    "text": "Here we will set up an s3 bucket with a policy that allows the public to read from the bucket, but not from a specific directory, and allows a particular aws_account_id root user as well as a defined user to write to the bucket.\nAlthough we are stoked on the s3fs package for working with s3 buckets, we will use the paws package more than perhaps necessary here - only to learn about how it all works. Seems like s3fs is the way to go for common moves but paws is the ‚Äúmom‚Äù providing the structure and guidance to that package.\n\n\nCode\nlibrary(paws)\nlibrary(here)\n\n\nhere() starts at /Users/airvine/Projects/repo/new_graphiti\n\n\nCode\nlibrary(jsonlite)\nlibrary(stringr)\nlibrary(s3fs)\n\n\nList our current buckets\n\n\nCode\ns3 &lt;- paws::s3()\ns3$list_buckets()\n\n\n$Buckets\n$Buckets[[1]]\n$Buckets[[1]]$Name\n[1] \"23cog\"\n\n$Buckets[[1]]$CreationDate\n[1] \"2023-03-17 00:07:12 GMT\"\n\n\n\n$Owner\n$Owner$DisplayName\n[1] \"al\"\n\n$Owner$ID\n[1] \"f5267b02e31758d1efea79b4eaef5d0423efb3e6a54ab869dc860bcc68ebae2d\"\n\n\n\nCreate Bucket\nLet‚Äôs create a bucket called the same name as this repository.\n\n\nCode\nmy_bucket_name &lt;- basename(here::here()) |&gt; \n  stringr::str_replace_all(\"_\", \"-\") \n\nbucket_path &lt;- s3fs::s3_path(my_bucket_name)\n\n\n\n\nCode\ns3$create_bucket(Bucket = my_bucket_name,\n  CreateBucketConfiguration = list(\n    LocationConstraint = Sys.getenv(\"AWS_DEFAULT_REGION\")\n  ))\n\n\n$Location\n[1] \"http://new-graphiti.s3.amazonaws.com/\"\n\n\n\n\nAdd the policy to the bucket.\n\nImportant - First we need to allow ‚Äúnew public policies‚Äù to be added to the bucket. We do this by deleteing the public access block. This is a security feature that prevents public access to the bucket. We will remove it so we can add our own policy. Took a while to catch this.\n\n\n\nCode\ns3$delete_public_access_block(\n  Bucket = my_bucket_name\n)\n\n\nlist()\n\n\n\nWrite the policy for the bucket Here is a function to make a generic policy for an s3 bucket that allows public to read from the bucket, but not from a specific directory, and allows a particular aws_account_id to write to the bucket. Plus + it allows you to provide Presigned URLs so we can provide temporary access to private objects without having to change the overall bucket or object permissions.\n\nKey thing here is that if we want a user to override the policy placed on a directory or file after we Deny access we need to add a condition to the policy that exempts the user (paste0(\"arn:aws:iam::\", aws_account_id, \":user/\", user_access_permission)) from the Deny.\n\n\nCode\n# https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example1.html\n#https://chatgpt.com/share/16106509-a34d-4f69-bf95-cd5eb2649707\naws_policy_write &lt;- function(bucket_name, \n                             bucket_dir_private, \n                             aws_account_id, \n                             user_access_permission, \n                             write_json = FALSE, \n                             dir_output = \"policy\", \n                             file_name = \"policy.json\") {\n  policy &lt;- list(\n    Version = \"2012-10-17\",\n    Statement = list(\n      list(\n        Effect = \"Allow\",\n        Principal = \"*\",\n        Action = \"s3:GetObject\",\n        Resource = paste0(\"arn:aws:s3:::\", bucket_name, \"/*\")\n      ),\n      list(\n        Effect = \"Deny\",\n        Principal = \"*\",\n        Action = \"s3:GetObject\",\n        Resource = paste0(\"arn:aws:s3:::\", bucket_name, \"/\", bucket_dir_private, \"/*\"),\n        # IMPORTANT - Denies everyone from getting objects from the private directory except for user_access_permission\n        Condition = list(\n          StringNotEquals = list(\n            \"aws:PrincipalArn\" = paste0(\"arn:aws:iam::\", aws_account_id, \":user/\", user_access_permission)\n          )\n        )\n      ),\n      list(\n        Effect = \"Allow\",\n        Principal = list(AWS = paste0(\"arn:aws:iam::\", aws_account_id, \":root\")),\n        Action = c(\"s3:DeleteObject\", \"s3:PutObject\"),\n        Resource = paste0(\"arn:aws:s3:::\", bucket_name, \"/*\")\n      )\n      #going to leave this here for now\n      # list(\n      #   Effect = \"Allow\",\n      #   Principal = list(AWS = paste0(\"arn:aws:iam::\", aws_account_id, \":user/\", user_access_permission)),\n      #   Action = c(\"s3:GetBucketLocation\", \"s3:ListBucket\"),\n      #   Resource = paste0(\"arn:aws:s3:::\", bucket_name)\n      # ),\n      # list(\n      #   Effect = \"Allow\",\n      #   Principal = list(AWS = paste0(\"arn:aws:iam::\", aws_account_id, \":user/\", user_access_permission)),\n      #   Action = \"s3:GetObject\",\n      #   Resource = paste0(\"arn:aws:s3:::\", bucket_name, \"/*\")\n      # )\n    )\n  )\n  \n  json_policy &lt;- jsonlite::toJSON(policy, pretty = TRUE, auto_unbox = TRUE)\n  \n  if (write_json) {\n    dir.create(dir_output, showWarnings = FALSE)\n    output_path &lt;- file.path(dir_output, file_name)\n    write(json_policy, file = output_path)\n    message(\"Policy written to \", output_path)\n  } else {\n    return(json_policy)\n  }\n}\n\n\nNow we can write the policy to the bucket.\n\n\nCode\n# run the function to build the json policy statement\nmy_policy &lt;- aws_policy_write(bucket_name = my_bucket_name, \n                         bucket_dir_private = \"private\",\n                         aws_account_id = Sys.getenv(\"AWS_ACCOUNT_ID\"),\n                         user_access_permission = \"airvine\",\n                         write_json = FALSE\n                         )\n\n# push the policy to the bucket\ns3$put_bucket_policy(\n  Bucket = my_bucket_name,\n  Policy = my_policy,\n  ExpectedBucketOwner = Sys.getenv(\"AWS_ACCOUNT_ID\")\n)\n\n\nlist()\n\n\n\n\nCode\n# this is cool - Check the policy was added correctly.\ns3$get_bucket_policy(my_bucket_name)\n\n\n\n\nAdd some files to the bucket\nFirst we add a photo to the main bucket. Going to use s3fs for this since I haven‚Äôt actually done just one file yet‚Ä¶ We are using the here package to get the path to the image due to rendering complexities.\n\n\nCode\ns3fs::s3_file_copy(\n  path = paste0(here::here(), \"/posts/\", params$post_dir_name, \"/image.jpg\"),\n  bucket_path\n)\n\n\n[1] \"s3://new-graphiti/image.jpg\"\n\n\nThen we add one to the private directory.\n\n\nCode\ns3fs::s3_dir_create(\n  path = paste0(bucket_path, \"/private\")\n)\n\n\n[1] \"s3://new-graphiti/private\"\n\n\nCode\ns3fs::s3_file_copy(\n  path = paste0(here::here(), \"/posts/\", params$post_dir_name, \"/image.jpg\"),\n  paste0(bucket_path, \"/private\")\n)\n\n\n[1] \"s3://new-graphiti/private/image.jpg\"\n\n\n\n\nAccess the bucket\nLet‚Äôs see if we can add the images to this post.\nCreate the paths to the images.\n\n\nCode\n# s3fs::s3_dir_info(bucket_path, recurse = TRUE)\nimage_path &lt;- paste0(\"https://\", my_bucket_name, \".s3.amazonaws.com/image.jpg\")\nimage_path_private &lt;- paste0(\"https://\", my_bucket_name, \".s3.amazonaws.com/private/image.jpg\")\n\n\nAccess the public image.\n\n\nCode\nknitr::include_graphics(image_path)\n\n\n\n\n\n\n\n\n\nGood to go.\nAnd now access the private image.\n\n\nCode\nknitr::include_graphics(image_path_private)\n\n\n\n\n\n\n\n\n\nüí£ Jackpot! We have the image in the ‚Äúprivate‚Äù bucket so can‚Äôt access them from the post without permission.\n\n\nProvide temporary access to an object\nBecause we granted ourselves access to the private directory and our IAM roles have the correct privileges, we can create a Presigned URL to provide temporary access to the private image. We will set the maximum of 7 days for the URL to be valid. That means that at 2024-06-03 14:48 the URL will no longer work and the image below will no longer render in this post!\n\n\nCode\nknitr::include_graphics(\n  s3fs::s3_file_url(\n    s3_dir_ls(paste0(bucket_path, \"/private\")),\n    604800,\n    \"get_object\"\n  )\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# this is the cmd line way\nurl_file_share &lt;- s3_dir_ls(paste0(bucket_path, \"/private\"))\ncommand &lt;- \"aws\"\nargs &lt;- c('s3', 'presign', url_file_share, '--expires-in', '604800')\n\nworking_directory = here::here() #we could just remove from funciton to get the current wd but its nice to have so we leave\n\n# loaded this function from the other file. should put in functions file or package\nsys_call()\n\n\nIn order to rerun our post we need to delete the bucket. When we do rerun - we use the s3fs package to do it\n\n\nCode\n# Dont delete the bucket or the post wont render! ha\n# Burn down the bucket üî•.  If we try to use `s3$delete_bucket(Bucket = my_bucket_name)` we will get an error because the \n# bucket is not empty. \n\n#`s3fs::s3_bucket_delete(bucket_path)` works fine though.\ns3fs::s3_bucket_delete(bucket_path)"
  },
  {
    "objectID": "posts/2024-06-30-land-cover/index.html",
    "href": "posts/2024-06-30-land-cover/index.html",
    "title": "Mapping Land Cover with R",
    "section": "",
    "text": "Visualize and quantify remotely sense land cover data‚Ä¶. Here is a first start. We will use the European Space Agency‚Äôs WorldCover product which provides global land cover maps for the years 2020 and 2021 at 10 meter resolution based on the combination of Sentinel-1 radar data and Sentinel-2 imagery. We will use the 2021 dataset for mapping an area of the Skeena watershed near Houston, British Columbia.\nThis post was inspired - with much of the code copied - from a repository on GitHub from the wonderfully talented Milos Popovic.\nFirst thing we will do is load our packages. If you do not have the packages installed yet you can change the update_pkgs param in the yml of this file to TRUE. Using pak is great because it allows you to update your packages when you want to.\nCode\npkgs_cran &lt;- c(\n  \"usethis\",\n  \"rstac\",\n  \"here\",\n  \"fs\",\n  \"terra\",\n  \"tidyverse\",\n  \"rayshader\",\n  \"sf\",\n  \"classInt\",\n  \"rgl\",\n  \"tidyterra\",\n  \"tabulapdf\",\n  \"bcdata\",\n  \"ggplot\",\n  \"ggdark\",\n  \"knitr\",\n  \"DT\",\n  \"htmlwidgets\")\n\npkgs_gh &lt;- c(\n  \"poissonconsulting/fwapgr\",\n  \"NewGraphEnvironment/rfp\"\n )\n\npkgs &lt;- c(pkgs_cran, pkgs_gh)\n\nif(params$update_pkgs){\n  # install the pkgs\n  lapply(pkgs,\n         pak::pkg_install,\n         ask = FALSE)\n}\n\n# load the pkgs\npkgs_ld &lt;- c(pkgs_cran,\n             basename(pkgs_gh))\ninvisible(\n  lapply(pkgs_ld,\n       require,\n       character.only = TRUE)\n)\n\nsource(here::here(\"scripts/functions.R\"))"
  },
  {
    "objectID": "posts/2024-06-30-land-cover/index.html#downsample-the-land-cover-raster",
    "href": "posts/2024-06-30-land-cover/index.html#downsample-the-land-cover-raster",
    "title": "Mapping Land Cover with R",
    "section": "Downsample the Land Cover Raster",
    "text": "Downsample the Land Cover Raster\nHere we downsample the land cover raster to the same resolution as the DEM for the purposes of rendering our map of the larger area in a reasonable amount of time.\n\n\nCode\n# Here we resample the land cover raster to the same resolution as the DEM.\nland_cover_raster_resampled &lt;- terra::resample(\n    land_cover_raster,\n    dem,\n    method = \"near\",\n    threads = TRUE\n)\n\n# terra::plot(land_cover_raster_resampled)"
  },
  {
    "objectID": "posts/2024-06-30-land-cover/index.html#get-additional-data",
    "href": "posts/2024-06-30-land-cover/index.html#get-additional-data",
    "title": "Mapping Land Cover with R",
    "section": "Get Additional Data",
    "text": "Get Additional Data\nWe could use some data for context such as major streams and the railway. We get the streams and railway from data distribution bc api using the bcdata package. Our rfp package calls just allow some extra sanity checks and convenience moves on the bcdata::bcdc_query_geodata function. It‚Äôs not really necessary but can be helpful (ex. can use small cap layer and column names and will throw an informative error if the name of the columns specified are input incorrectly).\n\n\n\nCode\n# grab all the railways\nl_rail &lt;- rfp::rfp_bcd_get_data(\n    bcdata_record_id = \"whse_basemapping.gba_railway_tracks_sp\"\n) |&gt; \n  sf::st_transform(4326) |&gt; \n  janitor::clean_names() \n\n# streams in the bulkley and then filter to just keep the big ones\nl_streams &lt;- rfp::rfp_bcd_get_data(\n  bcdata_record_id = \"whse_basemapping.fwa_stream_networks_sp\",\n  col_filter = \"watershed_group_code\",\n  col_filter_value = \"BULK\",\n  # grab a smaller object by including less columns\n  col_extract = c(\"linear_feature_id\", \"stream_order\", \"gnis_name\", \"downstream_route_measure\", \"blue_line_key\", \"length_metre\")\n) |&gt; \n  sf::st_transform(4326) |&gt; \n  janitor::clean_names() |&gt; \n  dplyr::filter(stream_order &gt; 4)\n\n\nNow we trim up those layers. We have some functions to validate and repair geometries and then we clip them to our area of interest.\n\n\nCode\nlayers_to_trim &lt;- tibble::lst(l_rail, l_streams)\n\n# Function to validate and repair geometries\nvalidate_geometries &lt;- function(layer) {\n  layer &lt;- sf::st_make_valid(layer)\n  layer &lt;- layer[sf::st_is_valid(layer), ]\n  return(layer)\n}\n\n# Apply validation to the AOI and layers\naoi &lt;- validate_geometries(aoi)\nlayers_to_trim &lt;- purrr::map(layers_to_trim, validate_geometries)\n\n# clip them  with purrr and sf\nlayers_trimmed &lt;- purrr::map(\n  layers_to_trim,\n  ~ sf::st_intersection(.x, aoi)\n)"
  },
  {
    "objectID": "posts/2024-06-30-land-cover/index.html#get-legend-values",
    "href": "posts/2024-06-30-land-cover/index.html#get-legend-values",
    "title": "Mapping Land Cover with R",
    "section": "Get Legend Values",
    "text": "Get Legend Values\nSo we need to map the values in the raster to the actual land cover classes. We can do this by extracting the cross reference table from the pdf provided in the metatdata of the data. We will use the tabulapdf package to extract the table and do some work to collapse it into a cross-referenceing tool we can use for land cover classifications and subsequent color schemes.\n\n\nCode\n# extract the cross reference table from the pdf\npdf_file &lt;- \"https://esa-worldcover.s3.eu-central-1.amazonaws.com/v200/2021/docs/WorldCover_PUM_V2.0.pdf\"\npage &lt;- 15\n\n# table_map &lt;- tabulapdf::locate_areas(pdf_file, pages = page)\n# table_coords &lt;- list(as.numeric(unlist(table_map[[1]])))\n\ntable_coords &lt;- list(c(94.55745,  74.66493, 755.06007, 550.41094))\n\n\nxref_raw &lt;- tabulapdf::extract_tables(\n  pdf_file,\n  pages = page,\n  method = \"lattice\",\n  area = table_coords,\n  guess = FALSE\n)\n\n# ##this is how we make a clean dataframe\nxref_raw2 &lt;- xref_raw |&gt; \n  purrr::pluck(1) |&gt;\n  tibble::as_tibble() |&gt;\n  janitor::row_to_names(1) |&gt;\n  janitor::clean_names()\n\nxref_raw3 &lt;- xref_raw2 |&gt; \n  tidyr::fill(code, .direction = \"down\")\n\n# Custom function to concatenate rows within each group\ncollapse_rows &lt;- function(df) {\n  df |&gt; \n    dplyr::summarise(across(everything(), ~ paste(na.omit(.), collapse = \" \")))\n}\n\n# Group by code and apply the custom function\nxref &lt;- xref_raw3 |&gt;\n  dplyr::group_by(code) |&gt;\n  dplyr::group_modify(~ collapse_rows(.x)) |&gt;\n  dplyr::ungroup() |&gt; \n  dplyr::mutate(code = as.numeric(code)) |&gt; \n  dplyr::arrange(code) |&gt; \n  purrr::set_names(c(\"code\", \"land_cover_class\", \"lccs_code\", \"definition\", \"color_code\")) |&gt; \n  # now we make a list of the color codes and convert to hex. Even though we don't actually need them here...\n  dplyr::mutate(color_code = purrr::map(color_code, ~ as.numeric(strsplit(.x, \",\")[[1]])),\n                color = purrr::map_chr(color_code, ~ rgb(.x[1], .x[2], .x[3], maxColorValue = 255))) |&gt; \n  dplyr::relocate(definition, .after = last_col())\n\n\nmy_dt_table(xref, cols_freeze_left = 0, page_length = 5)\n\n\n\n\n\n\nWe seem to get issues when the colors we have in our tiff does not match our cross-reference table. For this reason we will remove any values in the xref object that are not in the rasters that we are plotting.\n\nAlso - looks like when we combined our tiffs together with terra::mosaic we lost the color table associated with the SpatRaster object. We can recover that table with terra::coltab(land_cover_raster_raw[[1]])"
  },
  {
    "objectID": "posts/2024-06-30-land-cover/index.html#plot",
    "href": "posts/2024-06-30-land-cover/index.html#plot",
    "title": "Mapping Land Cover with R",
    "section": "Plot",
    "text": "Plot\nOk. Let‚Äôs plot it up. We will use ggplot2 and tidyterra to plot the land cover raster and then add the streams and railway on top of that.\n\n\nCode\ncolor_table &lt;- terra::coltab(land_cover_raster_raw[[1]])[[1]]\n\ncoltab(land_cover_raster_resampled) &lt;- color_table\n\nxref_cleaned &lt;- xref |&gt; \n  filter(code %in% sort(unique(terra::values(land_cover_raster_resampled))))\n\n\nmap &lt;- ggplot() +\n  tidyterra::geom_spatraster(\n    data = as.factor(land_cover_raster_resampled),\n    use_coltab = TRUE,\n    maxcell = Inf\n  ) +\n  tidyterra::scale_fill_coltab(\n    data = as.factor(land_cover_raster_resampled),\n    name = \"ESA Land Cover\",\n    labels = xref_cleaned$land_cover_class\n  ) +\n  # geom_sf(\n  #     data = aoi,\n  #     fill = \"transparent\",\n  #     color = \"white\",\n  #     linewidth = .5\n  # ) +\n  geom_sf(\n    data = layers_trimmed$l_streams,\n    color = \"blue\",\n    size = 1\n  ) +\n  geom_sf(\n    data = layers_trimmed$l_rail,\n    color = \"black\",\n    size = 1\n  ) +\n  ggdark::dark_theme_void()\n\n\nmap"
  },
  {
    "objectID": "posts/2024-06-30-land-cover/index.html#plot-refined-area",
    "href": "posts/2024-06-30-land-cover/index.html#plot-refined-area",
    "title": "Mapping Land Cover with R",
    "section": "Plot Refined Area",
    "text": "Plot Refined Area\nFirst we need to clip the landcover raster to the buffered area. We are not going to use the resampled raster because we want a more detailed view of the land cover classes for this much smaller area. The computational time to render the plot will be fine at the original resolution.\n\n\nCode\nland_cover_sample &lt;- terra::crop(land_cover_raster, aoi_refined_buffered, snap = \"near\", mask = TRUE, extend = TRUE)\n#dimensions  : 259, 951, 1  multiple stream segments\n#dimensions  : 262, 958, 1 single stream segment\n\n\n\nWe lose our color values with the crop. We see that with has.colors(land_cover_sample).\n\n\nCode\nhas.colors(land_cover_sample)\n\n\n[1] FALSE\n\n\n\nLet‚Äôs add them back in with the terra::coltab function.\n\n\nCode\ncoltab(land_cover_sample) &lt;- color_table\n\n\n\nNow we should be able to plot what we have. Let‚Äôs re-trim up our extra data layers to the refined area of interest and add those in as well.\n\n\nCode\n# clip them  with purrr and sf\nlayers_trimmed &lt;- purrr::map(\n  layers_to_trim,\n  ~ sf::st_intersection(.x, aoi_refined_buffered)\n) \n\nxref_cleaned &lt;- xref |&gt; \n  dplyr::filter(code %in% sort(unique(terra::values(land_cover_sample))))\n\nmap &lt;- ggplot2::ggplot() +\n  tidyterra::geom_spatraster(\n    data = as.factor(land_cover_sample),\n    use_coltab = TRUE,\n    maxcell = Inf\n  ) +\n  tidyterra::scale_fill_coltab(\n    data = as.factor(land_cover_sample),\n    name = \"ESA Land Cover\",\n    labels = xref_cleaned$land_cover_class\n  ) +\n  ggplot2::geom_sf(\n    data = layers_trimmed$l_rail,\n    color = \"black\",\n    size = 1\n  ) +\n  ggplot2::geom_sf(\n    data = layers_trimmed$l_streams,\n    color = \"blue\",\n    size = 1\n  ) +\n  ggdark::dark_theme_void()\n\n# save the plot\n# ggsave(here::here('posts', params$post_dir_name, \"image.jpg\"), width = 10, height = 10)\n    \nmap"
  },
  {
    "objectID": "posts/2024-06-30-land-cover/index.html#neexdzii-kwah",
    "href": "posts/2024-06-30-land-cover/index.html#neexdzii-kwah",
    "title": "Mapping Land Cover with R",
    "section": "Neexdzii Kwah",
    "text": "Neexdzii Kwah\nNow lets make a table and a simple bargraph to present the results.\n\n\nCode\n# Calculate the area of each cell (assuming your raster is in lat/lon coordinates)\ncell_area &lt;- terra::cellSize(land_cover_raster_resampled, unit = \"ha\")\n\n# Summarize the raster values\nland_cover_summary_raw &lt;- terra::freq(land_cover_raster_resampled, digits = 0) |&gt; \n  dplyr::mutate(area_ha = round(count * cell_area[1]$area),1) |&gt; \n  # make a column that is the percentage of the total area\n  dplyr::mutate(percent_area = round((area_ha / sum(area_ha) * 100), 1))\n\n# now we add the xref code and land_cover_class to the summary\nland_cover_summary &lt;- land_cover_summary_raw |&gt; \n  dplyr::left_join(xref, by = c(\"value\" = \"code\")) |&gt; \n  dplyr::select(land_cover_class, area_ha, percent_area) |&gt; \n  dplyr::arrange(desc(area_ha))\n\n\n\n\nCode\nmy_caption &lt;- \"Land Cover Class by Refined Area of Interest\"\n\nland_cover_summary |&gt; \n  knitr::kable(caption = my_caption)\n\n\n\nLand Cover Class by Refined Area of Interest\n\n\nland_cover_class\narea_ha\npercent_area\n\n\n\n\nTree cover\n194158\n84.3\n\n\nGrassland\n28940\n12.6\n\n\nPermanent water bodies\n3731\n1.6\n\n\nMoss and lichen\n2850\n1.2\n\n\nBuilt-up\n329\n0.1\n\n\nCropland\n264\n0.1\n\n\nBare / sparse vegetation\n158\n0.1\n\n\nHerbaceous wetland\n9\n0.0\n\n\nShrubland\n1\n0.0\n\n\n\n\n\n\n\nCode\nplot_title &lt;- \"Land Cover Class for Area of Interest\"\n\n# \nland_cover_summary |&gt; \n  # convert land_cover_class to factor and arrange based on the area_ha\n  dplyr::mutate(land_cover_class = forcats::fct_reorder(land_cover_class, area_ha)) |&gt;\n  ggplot2::ggplot(ggplot2::aes(x = land_cover_class, y = area_ha)) +\n  ggplot2::geom_col() +\n  ggplot2::coord_flip() +\n  ggplot2::labs(title= plot_title,\n                x = \"Land Cover Class\",\n                y = \"Area (ha)\") +\n  ggplot2::theme_minimal() \n\n\n\n\n\n\n\n\n\nCode\n  # cowplot::theme_minimal_grid()"
  },
  {
    "objectID": "posts/2024-06-30-land-cover/index.html#refined-area-of-interest",
    "href": "posts/2024-06-30-land-cover/index.html#refined-area-of-interest",
    "title": "Mapping Land Cover with R",
    "section": "Refined Area of Interest",
    "text": "Refined Area of Interest\nNow lets make a table and a simple bargraph to present the results.\n\n\nCode\n# Calculate the area of each cell (assuming your raster is in lat/lon coordinates)\ncell_area &lt;- terra::cellSize(land_cover_sample, unit = \"ha\")\n\n# Summarize the raster values\nland_cover_summary_raw &lt;- terra::freq(land_cover_sample, digits = 0) |&gt; \n  dplyr::mutate(area_ha = round(count * cell_area[1]$area),1) |&gt; \n  # make a column that is the percentage of the total area\n  dplyr::mutate(percent_area = round((area_ha / sum(area_ha) * 100), 1))\n\n# now we add the xref codde and land_cover_class to the summary\nland_cover_summary &lt;- land_cover_summary_raw |&gt; \n  dplyr::left_join(xref, by = c(\"value\" = \"code\")) |&gt; \n  dplyr::select(land_cover_class, area_ha, percent_area) |&gt; \n  dplyr::arrange(desc(area_ha))\n\n\n\n\nCode\nmy_caption &lt;- \"Land Cover Class for Refined Area\"\n\nland_cover_summary |&gt; \n  knitr::kable(caption = my_caption)\n\n\n\nLand Cover Class for Refined Area\n\n\nland_cover_class\narea_ha\npercent_area\n\n\n\n\nTree cover\n283\n66.6\n\n\nGrassland\n89\n20.9\n\n\nBuilt-up\n19\n4.5\n\n\nPermanent water bodies\n19\n4.5\n\n\nCropland\n10\n2.4\n\n\nBare / sparse vegetation\n4\n0.9\n\n\nMoss and lichen\n1\n0.2\n\n\n\n\n\n\n\nCode\nplot_title &lt;- \"Land Cover Class for Refined Area\"\n\n# \nland_cover_summary |&gt; \n  # convert land_cover_class to factor and arrange based on the area_ha\n  dplyr::mutate(land_cover_class = forcats::fct_reorder(land_cover_class, area_ha)) |&gt;\n  ggplot2::ggplot(ggplot2::aes(x = land_cover_class, y = area_ha)) +\n  ggplot2::geom_col() +\n  ggplot2::coord_flip() +\n  ggplot2::labs(title= plot_title,\n                x = \"Land Cover Class\",\n                y = \"Area (ha)\") +\n  ggplot2::theme_minimal()"
  },
  {
    "objectID": "posts/logos-equipment/index.html",
    "href": "posts/logos-equipment/index.html",
    "title": "Logos and Equipment List Somewhere Accessible",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, dpi=60, out.width = \"100%\")\noptions(scipen=999)\noptions(knitr.kable.NA = '--') #'--'\noptions(knitr.kable.NAN = '--')\n\n\nThis here is a post showing where we are now keeping the company equipment details as well as the company logos. We need these things to be accessible to all team members from all repos so we have put them here since this is a public repo.\nHere is the location of the equipment lists that we use in safety/field planning:\nhttps://raw.githubusercontent.com/NewGraphEnvironment/new_graphiti/main/assets/data/equipment.csv\n\n\nCode\nreadr::read_csv(\n  url(\n    glue::glue(\"https://raw.githubusercontent.com/{params$repo_owner}/{params$repo_name}/main/assets/data/equipment.csv\")\n  )\n)|&gt; \n  fpr::fpr_kable(font = 12)\n\n\n\n\n\n\n\neq_item\neq_pers_standard\neq_truck\neq_safety\neq_task1\neq_task2\neq_task3\neq_task4\neq_task5\neq_task6\neq_task7\nmateo_winterschedt\n\n\n\n\nclinometer\nx\n--\n--\nall\nelectrofishing\nfish passage\n--\n--\n--\n--\nx\n\n\nfield vest\nx\n--\n--\nall\n--\nfish passage\n--\n--\n--\n--\nx\n\n\nnote book\nx\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nGPS\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\nx\n\n\nSuncreen\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nBugspray\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nPolarized glasses\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nBear Spray\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\nx\n\n\nphone/camera\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nbattery pack booster for phone\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nHat\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nfirst aid kit personal\nx\n--\nx\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nWaders\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\nx\n\n\nBoots\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\nx\n\n\nExtra clothes\nx\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nrain gear\nx\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nSki poles\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nwater\nx\n--\nx\nall\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nfood\nx\n--\nx\nall\n--\nfish passage\n--\n--\n--\n--\n--\n\n\ngloves work\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nglasses safety\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nheadlamp\nx\n--\nx\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nOakton Multimeter\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\nTurbidity Meter LaMotte 2020e\n--\n--\n--\n--\n--\n--\nenvironmental monitoring\nwq\n--\n--\n--\n\n\nHand saw\n--\nx\nx\nall\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\nBackpack Electrofisher\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\nstop nets x 4\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\nsalt blocks\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\nloose salt\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\ndip nets x 2\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\nLinesman Gloves x 3\n--\n--\nx\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\ntape measure hand\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\ntape measure eslon\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\npilon x 2\n--\nx\n--\nall\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nMeasuring board\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nScale\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nPermits\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nBackroads Mapbook\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nLocational maps\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nFish ID book\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nBackground Documents\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nradio handheld\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nradio truck\n--\nx\n--\n--\n--\n--\n--\n--\n--\n--\nx\n\n\nradio chargers x 3\n--\n--\nx\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nSatelite communicator\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nField Safety Plan\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nfirst aid kit level 1\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nFirst Aid binder stocked\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nSite Cards / Field Guide\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nMinnow Traps\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nCatfood\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nFlagging\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nLaptop w/basecamp\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nGPS cable\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nLazer level\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nAssessment cards fish passage\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nUAV radio\n--\n--\nx\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nUAV\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nUAV landing pad\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nUAV GC tape\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nUAV safety plan (when required)\n--\n--\nx\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nUAV registration\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nUAV license\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nUAV radio license\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nFlow meter\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nATV\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nThrow bags\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\npolaski\n--\nx\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nshovel\n--\nx\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nfire extinguisher backpack\n--\nx\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nfire extinguisher pressurized\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nbucket rigid x 2\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nbucket foldable\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nclove oil kit w/ instructions\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\ngloves leather\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nhard hat\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nsteel toed boots\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nsharpies\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nhand lens\nx\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nATV gas\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nATV lock\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nUAV battery charger\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nwader disinfectant kit\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\ntruck tow rope\n--\nx\nx\nall\n--\n--\n--\n--\n--\n--\n--\n\n\ntruck jack\n--\nx\nx\nall\n--\n--\n--\n--\n--\n--\n--\n\n\nGPS batteries\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\nATV helmets\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nBattery booster\n--\nx\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nCompressor 12V\n--\nx\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nRubber boots (no-slip soles)\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nSmall BT Speaker (for bears)\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nGPS Case waterproof\n--\n--\n--\nsnorkling\n--\n--\n--\n--\n--\n--\n--\n\n\nNotebook waterproof\n--\n--\n--\nsnorkling\n--\n--\n--\n--\n--\n--\n--\n\n\nDrysuits\n--\n--\n--\nsnorkling\n--\n--\n--\n--\n--\n--\n--\n\n\nSnorkels\n--\n--\n--\nsnorkling\n--\n--\n--\n--\n--\n--\n--\n\n\ndrysuit gloves\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nGoggles\n--\n--\n--\nsnorkling\n--\n--\n--\n--\n--\n--\n--\n\n\nFanny pack\n--\n--\n--\nsnorkling\n--\n--\n--\n--\n--\n--\n--\n\n\nlarge backpack\n--\n--\n--\n--\nelectrofishing\n--\n--\n--\n--\n--\n--\n\n\nTow strap\n--\nx\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nrange finder\nx\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\n\n\n\n\n\n\n\nLogos:\nhttps://raw.githubusercontent.com/NewGraphEnvironment/new_graphiti/main/assets/logos\nThere are many ‚Äî so here is a list of their names and locations online:\n\n\nCode\nfile_names &lt;- fs::dir_ls(\n  glue::glue(here::here(\"assets/logos\")),\n  glob = c(\"*.png\", \"*.jpg\", \"*.jpeg\"),\n  recurse = TRUE\n) \n\n\ntibble::tibble(path = file_names) |&gt; \n    dplyr::mutate(path = stringr::str_replace_all(path, \"/Users/airvine/Projects/repo/new_graphiti\", \"https://github.com/NewGraphEnvironment/new_graphiti/tree/main\")) |&gt; \n  fpr::fpr_kable(font = 12)\n\n\n\n\n\n\n\npath\n\n\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/BLACK/PNG/nge-full_black.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/BLACK/PNG/nge-icon_black.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/BLACK/PNG/nge-icon_name_big_black.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/BLACK/PNG/nge-icon_name_black.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/BLACK/PNG/nge-name_black.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/BLACK/PNG/nge-name_research_consulting_black.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/BLACK/PNG/nge-research_consulting_black.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/COLOR/PNG/nge-full_color.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/COLOR/PNG/nge-icon.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/COLOR/PNG/nge-icon_name.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/COLOR/PNG/nge-icon_name_big_color.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/COLOR/PNG/nge-name_color.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/COLOR/PNG/nge-name_research_consulting_color.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/COLOR/PNG/nge-research_consulting_color.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/GREYSCALE/PNG/nge-full_greyscale.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/GREYSCALE/PNG/nge-icon_gradient_grey.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/GREYSCALE/PNG/nge-icon_name_big_greyscale.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/GREYSCALE/PNG/nge-icon_name_greyscale.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/GREYSCALE/PNG/nge-name_greyscale.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/GREYSCALE/PNG/nge-name_research_consulting_greyscale.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/GREYSCALE/PNG/nge-research_consulting_greyscale.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/WHITE/PNG/nge-full_white.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/WHITE/PNG/nge-icon_name_big_white.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/WHITE/PNG/nge-icon_name_white.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/WHITE/PNG/nge-icon_white.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/WHITE/PNG/nge-name_research_consulting_white.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/WHITE/PNG/nge-name_white.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/WHITE/PNG/nge-research_consulting_white.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/pixel.png"
  },
  {
    "objectID": "posts/2024-09-21-aws-cors-cog-leaflet/2024-09-21-aws-cors-cog-leaflet.html",
    "href": "posts/2024-09-21-aws-cors-cog-leaflet/2024-09-21-aws-cors-cog-leaflet.html",
    "title": "CORS for serving of COGs of UAV imagery on AWS with R",
    "section": "",
    "text": "Whoa Billy. Time to host our UAV imagery on AWS and serve it out through leaflet and do dank moves like put before after images on slippy maps next to each other for world peace. Ha. Well that is a bit dramatic but hey. Still pretty cool.\nFirst thing is to convert the image to a cog and sync it up to a bucket. Not doing that here. Will do soon though. What we do here is - after we are sure there are public permissions allowed to the bucket but we also need to deal with big bad CORS. We can set a viewing Cross-origin resource sharing (CORS). ‚ÄúCORS defines a way for client web applications that are loaded in one domain to interact with resources in a different domain‚Äù. This is done with the following JSON.\n[\n    {\n        \"AllowedHeaders\": [\n            \"*\"\n        ],\n        \"AllowedMethods\": [\n            \"GET\"\n        ],\n        \"AllowedOrigins\": [\n            \"*\"\n        ],\n        \"ExposeHeaders\": [\n            \"x-amz-server-side-encryption\",\n            \"x-amz-request-id\",\n            \"x-amz-id-2\"\n        ],\n        \"MaxAgeSeconds\": 3000\n    }\n]\nWe went the daft way here and just copied and pasted this into the CORS section of our bucket in the console but we should be able to use the paws package and the s3_put_bucket_cors function (I would think). We are going to leave that for another day.\n\n\nCode\nlibrary(paws)\n\n\nWarning: package 'paws' was built under R version 4.4.1\n\n\nCode\nlibrary(s3fs)\nlibrary(leaflet)\nlibrary(leafem)\n\n\nList your buckets kid.\n\n\nCode\ns3 &lt;- paws::s3()\nbuckets &lt;- s3$list_buckets()\npurrr::map_chr(buckets$Buckets, \"Name\")\n\n\n[1] \"23cog\"        \"new-graphiti\"\n\n\nNow list em with s3fs and show the file structure kid.\n\n\nCode\ns3fs::s3_dir_ls(refresh = TRUE) \n\n\n[1] \"s3://23cog\"        \"s3://new-graphiti\"\n\n\nCode\nbucket_path &lt;- s3fs::s3_path(\"23cog\")\ns3fs::s3_dir_tree(bucket_path)\n\n\ns3://23cog\n‚îú‚îÄ‚îÄ .DS_Store\n‚îú‚îÄ‚îÄ 20210906lampreymoricetribv220230317-DEM.tif\n‚îú‚îÄ‚îÄ 20210906lampreymoricetribv220230317.las\n‚îú‚îÄ‚îÄ 20210906lampreymoricetribv220230317.tif\n‚îú‚îÄ‚îÄ FHAP_template.csv\n‚îú‚îÄ‚îÄ bc_093l026_xli1m_utm09_2018.tif\n‚îú‚îÄ‚îÄ bc_093l048_xli1m_utm09_2019.tif\n‚îú‚îÄ‚îÄ bc_093l056_xli1m_utm09_2019_cog.tif\n‚îú‚îÄ‚îÄ bc_093l058_xli1m_utm09_2019.tif\n‚îú‚îÄ‚îÄ bc_093m032_xli1m_utm09_2019_cog2.tif\n‚îú‚îÄ‚îÄ ept.json\n‚îú‚îÄ‚îÄ glen_valle_lidar_2019_cog.tif\n‚îú‚îÄ‚îÄ xli1m_utm09_2019_093L04811.tif\n‚îú‚îÄ‚îÄ xli1m_utm09_2019_093L04812.tif\n‚îú‚îÄ‚îÄ xli1m_utm09_2019_093L04813.tif\n‚îú‚îÄ‚îÄ xli1m_utm09_2019_093L04814.tif\n‚îú‚îÄ‚îÄ xli1m_utm09_2019_093L04821.tif\n‚îú‚îÄ‚îÄ xli1m_utm09_2019_093L04822.tif\n‚îú‚îÄ‚îÄ xli1m_utm09_2019_093L04823.tif\n‚îú‚îÄ‚îÄ xli1m_utm09_2019_093L04824.tif\n‚îú‚îÄ‚îÄ xli1m_utm09_2019_093L04831.tif\n‚îú‚îÄ‚îÄ xli1m_utm09_2019_093L04832.tif\n‚îú‚îÄ‚îÄ xli1m_utm09_2019_093L04833.tif\n‚îú‚îÄ‚îÄ xli1m_utm09_2019_093L04834.tif\n‚îú‚îÄ‚îÄ xli1m_utm09_2019_093L04841.tif\n‚îú‚îÄ‚îÄ xli1m_utm09_2019_093L04842.tif\n‚îú‚îÄ‚îÄ xli1m_utm09_2019_093L04843.tif\n‚îú‚îÄ‚îÄ xli1m_utm09_2019_093L04844.tif\n‚îî‚îÄ‚îÄ private\n    ‚îî‚îÄ‚îÄ FHAP_template.xlsx\n\n\nList all the darned full paths please\n\n\nCode\ns3_dir_ls(bucket_path)\n\n\n [1] \"s3://23cog/.DS_Store\"                                  \n [2] \"s3://23cog/20210906lampreymoricetribv220230317-DEM.tif\"\n [3] \"s3://23cog/20210906lampreymoricetribv220230317.las\"    \n [4] \"s3://23cog/20210906lampreymoricetribv220230317.tif\"    \n [5] \"s3://23cog/FHAP_template.csv\"                          \n [6] \"s3://23cog/bc_093l026_xli1m_utm09_2018.tif\"            \n [7] \"s3://23cog/bc_093l048_xli1m_utm09_2019.tif\"            \n [8] \"s3://23cog/bc_093l056_xli1m_utm09_2019_cog.tif\"        \n [9] \"s3://23cog/bc_093l058_xli1m_utm09_2019.tif\"            \n[10] \"s3://23cog/bc_093m032_xli1m_utm09_2019_cog2.tif\"       \n[11] \"s3://23cog/ept.json\"                                   \n[12] \"s3://23cog/glen_valle_lidar_2019_cog.tif\"              \n[13] \"s3://23cog/xli1m_utm09_2019_093L04811.tif\"             \n[14] \"s3://23cog/xli1m_utm09_2019_093L04812.tif\"             \n[15] \"s3://23cog/xli1m_utm09_2019_093L04813.tif\"             \n[16] \"s3://23cog/xli1m_utm09_2019_093L04814.tif\"             \n[17] \"s3://23cog/xli1m_utm09_2019_093L04821.tif\"             \n[18] \"s3://23cog/xli1m_utm09_2019_093L04822.tif\"             \n[19] \"s3://23cog/xli1m_utm09_2019_093L04823.tif\"             \n[20] \"s3://23cog/xli1m_utm09_2019_093L04824.tif\"             \n[21] \"s3://23cog/xli1m_utm09_2019_093L04831.tif\"             \n[22] \"s3://23cog/xli1m_utm09_2019_093L04832.tif\"             \n[23] \"s3://23cog/xli1m_utm09_2019_093L04833.tif\"             \n[24] \"s3://23cog/xli1m_utm09_2019_093L04834.tif\"             \n[25] \"s3://23cog/xli1m_utm09_2019_093L04841.tif\"             \n[26] \"s3://23cog/xli1m_utm09_2019_093L04842.tif\"             \n[27] \"s3://23cog/xli1m_utm09_2019_093L04843.tif\"             \n[28] \"s3://23cog/xli1m_utm09_2019_093L04844.tif\"             \n[29] \"s3://23cog/private\"                                    \n\n\nBiuld a functshi to give us the actual https url. Sure there is a function somewhere already to do this but couldn‚Äôt find it.\n\n\nCode\n# Define your S3 path\ns3_path &lt;-  \"s3://23cog/20210906lampreymoricetribv220230317.tif\"\n\ns3_path_to_https &lt;- function(s3_path) {\n  # Remove the 's3://' prefix\n  path_without_prefix &lt;- sub(\"^s3://\", \"\", s3_path)\n  \n  # Split the path into bucket and key\n  parts &lt;- strsplit(path_without_prefix, \"/\", fixed = TRUE)[[1]]\n  bucket_name &lt;- parts[1]\n  object_key &lt;- paste(parts[-1], collapse = \"/\")\n  \n  # Construct the HTTPS URL\n  https_url &lt;- sprintf(\"https://%s.s3.amazonaws.com/%s\", bucket_name, object_key)\n  return(https_url)\n}\n\nurl &lt;- s3_path_to_https(s3_path)\nprint(url)\n\n\n[1] \"https://23cog.s3.amazonaws.com/20210906lampreymoricetribv220230317.tif\"\n\n\nSince we already have some valid COGs up on AWS we will link to one to be sure it works.\n\n\nCode\n  leaflet::leaflet() |&gt;\n    leaflet::addTiles() |&gt;\n    leafem:::addCOG(\n      url = url\n      , group = \"COG\"\n      , resolution = 512\n      , autozoom = TRUE\n    )\n\n\n\n\n\n\nDope.\nWhen we work with the paws package - when we want to get help we use ?s3 and navigate in from there. This next section doesn‚Äôt work yet so we turn eval = F and get on with our lives. Would like to activate this policy from\n\n\nCode\nmy_bucket_name = \"23cog\" \n\n# Define the CORS policy as a list\nmy_policy &lt;- list(\n    list(\n        AllowedHeaders = list(\"*\"),  # Must be a list\n        AllowedMethods = list(\"GET\"),  # Must be a list\n        AllowedOrigins = list(\"*\"),  # Must be a list\n        ExposeHeaders = list(\n            \"x-amz-server-side-encryption\",\n            \"x-amz-request-id\",\n            \"x-amz-id-2\"\n        ),  # Must be a list\n        MaxAgeSeconds = 3000  # Must be a number\n    )\n)\n\n\n# Convert the policy list to a pretty JSON format\nmy_policy_json &lt;- jsonlite::toJSON(my_policy, pretty = TRUE, auto_unbox = TRUE)\n\n\n# Set the CORS configuration directly using the list\npaws::s3()$put_bucket_cors(\n    Bucket = my_bucket_name,\n    CORSConfiguration = my_policy_json  # Pass the list of rules directly\n)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "new graphiti",
    "section": "",
    "text": "CORS for serving of COGs of UAV imagery on AWS with R\n\n\n\n\n\n\naws\n\n\ns3\n\n\nr\n\n\npaws\n\n\ns3sf\n\n\nleaflet\n\n\nleafem\n\n\nCOG\n\n\nCORS\n\n\n\n\n\n\n\n\n\nSep 21, 2024\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nMapping Land Cover with R\n\n\n\n\n\n\nland cover\n\n\nR\n\n\nplanetary computer\n\n\nremote sensing\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nMapping and Plotting Precipitation with R\n\n\n\n\n\n\nprecipitation\n\n\nR\n\n\ndrought\n\n\nrayshader\n\n\n\n\n\n\n\n\n\nJun 19, 2024\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning up bib files flexibly with Zotero and R\n\n\n\n\n\n\nnews\n\n\nbibtex\n\n\nR\n\n\ncitations\n\n\n\n\n\n\n\n\n\nMay 27, 2024\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nSetting aws bucket permissions with R\n\n\n\n\n\n\naws\n\n\ns3\n\n\nr\n\n\npaws\n\n\n\n\n\n\n\n\n\nMay 24, 2024\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nSyncing files to aws with R\n\n\n\n\n\n\naws\n\n\ns3\n\n\nr\n\n\npaws\n\n\nprocessx\n\n\n\n\n\n\n\n\n\nMay 23, 2024\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nLogos and Equipment List Somewhere Accessible\n\n\n\n\n\n\nassets\n\n\n\n\n\n\n\n\n\nMay 7, 2024\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nLower snake_case vs Everything_Else\n\n\n\n\n\n\nnames\n\n\n\n\n\n\n\n\n\nMay 7, 2024\n\n\nal\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "rws",
    "section": "",
    "text": "raNdom workish stufF"
  },
  {
    "objectID": "posts/aws-storage-processx/index.html",
    "href": "posts/aws-storage-processx/index.html",
    "title": "Syncing files to aws with R",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, dpi=60, out.width = \"100%\")\noptions(scipen=999)\noptions(knitr.kable.NA = '--') #'--'\noptions(knitr.kable.NAN = '--')\n\n\nInspired by https://blog.djnavarro.net/posts/2022-03-17_using-aws-s3-in-r/ by Danielle Navarro.\nNote to self - /Users/airvine/Projects/repo/new_graphiti/_freeze/posts/aws-storage-processx/index/execute-results/html.json is created when I render this document. This seems to be what is published to website after 1. the github_actions workflow is run to generate the gh-pages branch (on github runner) 2. the site is published with gitpages from github.\n‚ÄúQuick‚Äù post to document where I got to with syncing files to aws with R. Didn‚Äôt love the aws.s3::sync function because from what I could tell I could not tell it to delete files if they were not present locally or in a bucket (I could be wrong).\nThen climbed into s3fs which mirrors the fs package and seems a bit more user friendly than the aws.s3 package for managing files. It is created by Dyfan Jones who also is the top contributor to paws!! He seems like perhaps as much of a beast as one of the contributors to s3fs who is Scott Chamberlain.\nFor the sync issue figured why not just call the aws command line tool from R. processx is an insane package that might be the mother of all packages. It allows you to run command line tools from R with flexibility for some things like setting the directory where the command is called from in the processx called function (big deal as far as I can tell).\nWe need to set up our aws account online. The blog above from Danielle Navarro covers that I believe (I struggled through it a long time ago). I should use a ~/.aws/credentials file but don‚Äôt yet. I have my credentials in my ~/.Renviron file as well as in my ~/.bash_profile (probably a ridiculous setup). They are:\nAWS_ACCESS_KEY_ID='my_key'\nAWS_DEFAULT_REGION='my_region'\nAWS_SECRET_ACCESS_KEY='my_secret_key'\n\n\nCode\n# library(aws.s3)\nlibrary(processx)\n# library(paws) #this is the mom.  Couple examples of us hashed out here\nlibrary(s3fs)\n# library(aws.iam) #not useing - set permissions\nlibrary(here) #helps us with working directory issues related to the `environment` we operate in when rendering\n\n\n\nSee buckets using the s3fs package.\n\nCurrent buckets are:\n\n\nCode\ns3fs::s3_dir_ls(refresh = TRUE) \n\n\n[1] \"s3://23cog\"\n\n\n\n\nCode\n# First we set up our AWS s3 file system. I am actually not sure this is necessary but I did it.  Will turn the chunk off\n# to not repeat.\n# s3fs::s3_file_system(profile_name = \"s3fs_example\")\n\n\n\n\nCreate a Bucket\nLet‚Äôs generate the name of the bucket based on the name of the repo but due to aws bucket naming rules we need to swap out our underscores for hyphens! Maybe a good enough reason to change our naming conventions for our repos on github!!\n\n\nCode\nbucket_name &lt;- basename(here::here()) |&gt; \n  stringr::str_replace_all(\"_\", \"-\") \n\nbucket_path &lt;- s3fs::s3_path(bucket_name)\n\ns3fs::s3_bucket_create( bucket_path)  \n\n\n[1] \"s3://new-graphiti\"\n\n\n\n\nSync Files to Bucket\nWe build a little wrapper function to help us debug issues when running system commands with processx.\n\n\nCode\nsys_call &lt;- function(){\n  result &lt;- tryCatch({\n    processx::run(\n      command,\n      args = args,\n      echo = TRUE,            # Print the command output live\n      wd = working_directory, # Set the working directory\n      spinner = TRUE,         # Show a spinner\n      timeout = 60            # Timeout after 60 seconds\n    )\n  }, error = function(e) {\n    # Handle errors: e.g., print a custom error message\n    cat(\"An error occurred: \", e$message, \"\\n\")\n    NULL  # Return NULL or another appropriate value\n  })\n  \n  # Check if the command was successful\n  if (!is.null(result)) {\n    cat(\"Exit status:\", result$status, \"\\n\")\n    cat(\"Output:\\n\", result$stdout)\n  } else {\n    cat(\"Failed to execute the command properly.\\n\")\n  }\n}\n\n\n\nThen we specify our command and arguments. To achieve the desired behavior of including only files in the assets/* directory, you need to combine the order of --exclude and --include flags appropriately (exclude everything first thenn include what we want):\n\n\nCode\ncommand &lt;- \"aws\"\nargs &lt;- c('s3', 'sync', '.', bucket_path, '--delete', '--exclude', '*', '--include', 'posts/*')\n\nworking_directory = here::here() #we could just remove from funciton to get the current wd but its nice to have so we leave\n\n\nNow lets put a tester file in our directory and sync it to our bucket. We will remove it later to test if it is removed on sync.\n\n\nCode\nfile.create(here::here('posts/test.txt'))\n\n\n[1] TRUE\n\n\nRun our little function to sync the files to the bucket.\n\n\nCode\nsys_call()\n\n\nCompleted 237 Bytes/511.7 KiB (3.0 KiB/s) with 12 file(s) remaining\nupload: posts/_metadata.yml to s3://new-graphiti/posts/_metadata.yml\nCompleted 237 Bytes/511.7 KiB (3.0 KiB/s) with 11 file(s) remaining\nCompleted 2.0 KiB/511.7 KiB (11.9 KiB/s) with 11 file(s) remaining \nupload: posts/logos-equipment/index.qmd to s3://new-graphiti/posts/logos-equipment/index.qmd\nCompleted 2.0 KiB/511.7 KiB (11.9 KiB/s) with 10 file(s) remaining\nCompleted 3.6 KiB/511.7 KiB (20.8 KiB/s) with 10 file(s) remaining\nupload: posts/snakecase/index.qmd to s3://new-graphiti/posts/snakecase/index.qmd\nCompleted 3.6 KiB/511.7 KiB (20.8 KiB/s) with 9 file(s) remaining\nCompleted 8.6 KiB/511.7 KiB (48.5 KiB/s) with 9 file(s) remaining\nupload: posts/snakecase/thumbnail.jpg to s3://new-graphiti/posts/snakecase/thumbnail.jpg\nCompleted 8.6 KiB/511.7 KiB (48.5 KiB/s) with 8 file(s) remaining\nCompleted 13.9 KiB/511.7 KiB (74.3 KiB/s) with 8 file(s) remaining\nupload: posts/aws-storage-permissions/index.qmd to s3://new-graphiti/posts/aws-storage-permissions/index.qmd\nCompleted 13.9 KiB/511.7 KiB (74.3 KiB/s) with 7 file(s) remaining\nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 7 file(s) remaining\nupload: posts/aws-storage-processx/image.jpg to s3://new-graphiti/posts/aws-storage-processx/image.jpg\nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 6 file(s) remaining\nupload: posts/test.txt to s3://new-graphiti/posts/test.txt        \nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 5 file(s) remaining\nCompleted 26.0 KiB/511.7 KiB (119.1 KiB/s) with 5 file(s) remaining\nupload: posts/aws-storage-processx/index.rmarkdown to s3://new-graphiti/posts/aws-storage-processx/index.rmarkdown\nCompleted 26.0 KiB/511.7 KiB (119.1 KiB/s) with 4 file(s) remaining\nCompleted 34.0 KiB/511.7 KiB (152.0 KiB/s) with 4 file(s) remaining\nupload: posts/aws-storage-permissions/image.jpg to s3://new-graphiti/posts/aws-storage-permissions/image.jpg\nCompleted 34.0 KiB/511.7 KiB (152.0 KiB/s) with 3 file(s) remaining\nCompleted 42.2 KiB/511.7 KiB (180.5 KiB/s) with 3 file(s) remaining\nupload: posts/aws-storage-processx/index.qmd to s3://new-graphiti/posts/aws-storage-processx/index.qmd\nCompleted 42.2 KiB/511.7 KiB (180.5 KiB/s) with 2 file(s) remaining\nCompleted 83.0 KiB/511.7 KiB (256.0 KiB/s) with 2 file(s) remaining\nupload: posts/logos-equipment/image.jpg to s3://new-graphiti/posts/logos-equipment/image.jpg\nCompleted 83.0 KiB/511.7 KiB (256.0 KiB/s) with 1 file(s) remaining\nCompleted 339.0 KiB/511.7 KiB (830.9 KiB/s) with 1 file(s) remaining\nCompleted 511.7 KiB/511.7 KiB (654.4 KiB/s) with 1 file(s) remaining\nupload: posts/snakecase/all.jpeg to s3://new-graphiti/posts/snakecase/all.jpeg\nExit status: 0 \nOutput:\n Completed 237 Bytes/511.7 KiB (3.0 KiB/s) with 12 file(s) remaining\nupload: posts/_metadata.yml to s3://new-graphiti/posts/_metadata.yml\nCompleted 237 Bytes/511.7 KiB (3.0 KiB/s) with 11 file(s) remaining\nCompleted 2.0 KiB/511.7 KiB (11.9 KiB/s) with 11 file(s) remaining \nupload: posts/logos-equipment/index.qmd to s3://new-graphiti/posts/logos-equipment/index.qmd\nCompleted 2.0 KiB/511.7 KiB (11.9 KiB/s) with 10 file(s) remaining\nCompleted 3.6 KiB/511.7 KiB (20.8 KiB/s) with 10 file(s) remaining\nupload: posts/snakecase/index.qmd to s3://new-graphiti/posts/snakecase/index.qmd\nCompleted 3.6 KiB/511.7 KiB (20.8 KiB/s) with 9 file(s) remaining\nCompleted 8.6 KiB/511.7 KiB (48.5 KiB/s) with 9 file(s) remaining\nupload: posts/snakecase/thumbnail.jpg to s3://new-graphiti/posts/snakecase/thumbnail.jpg\nCompleted 8.6 KiB/511.7 KiB (48.5 KiB/s) with 8 file(s) remaining\nCompleted 13.9 KiB/511.7 KiB (74.3 KiB/s) with 8 file(s) remaining\nupload: posts/aws-storage-permissions/index.qmd to s3://new-graphiti/posts/aws-storage-permissions/index.qmd\nCompleted 13.9 KiB/511.7 KiB (74.3 KiB/s) with 7 file(s) remaining\nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 7 file(s) remaining\nupload: posts/aws-storage-processx/image.jpg to s3://new-graphiti/posts/aws-storage-processx/image.jpg\nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 6 file(s) remaining\nupload: posts/test.txt to s3://new-graphiti/posts/test.txt        \nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 5 file(s) remaining\nCompleted 26.0 KiB/511.7 KiB (119.1 KiB/s) with 5 file(s) remaining\nupload: posts/aws-storage-processx/index.rmarkdown to s3://new-graphiti/posts/aws-storage-processx/index.rmarkdown\nCompleted 26.0 KiB/511.7 KiB (119.1 KiB/s) with 4 file(s) remaining\nCompleted 34.0 KiB/511.7 KiB (152.0 KiB/s) with 4 file(s) remaining\nupload: posts/aws-storage-permissions/image.jpg to s3://new-graphiti/posts/aws-storage-permissions/image.jpg\nCompleted 34.0 KiB/511.7 KiB (152.0 KiB/s) with 3 file(s) remaining\nCompleted 42.2 KiB/511.7 KiB (180.5 KiB/s) with 3 file(s) remaining\nupload: posts/aws-storage-processx/index.qmd to s3://new-graphiti/posts/aws-storage-processx/index.qmd\nCompleted 42.2 KiB/511.7 KiB (180.5 KiB/s) with 2 file(s) remaining\nCompleted 83.0 KiB/511.7 KiB (256.0 KiB/s) with 2 file(s) remaining\nupload: posts/logos-equipment/image.jpg to s3://new-graphiti/posts/logos-equipment/image.jpg\nCompleted 83.0 KiB/511.7 KiB (256.0 KiB/s) with 1 file(s) remaining\nCompleted 339.0 KiB/511.7 KiB (830.9 KiB/s) with 1 file(s) remaining\nCompleted 511.7 KiB/511.7 KiB (654.4 KiB/s) with 1 file(s) remaining\nupload: posts/snakecase/all.jpeg to s3://new-graphiti/posts/snakecase/all.jpeg\n\n\nThen we can see our bucket contents - as well as list our bucket contents and capture them.\n\n\nCode\ns3fs::s3_dir_tree(bucket_path)\n\n\ns3://new-graphiti\n‚îî‚îÄ‚îÄ posts\n    ‚îú‚îÄ‚îÄ _metadata.yml\n    ‚îú‚îÄ‚îÄ test.txt\n    ‚îú‚îÄ‚îÄ aws-storage-permissions\n    ‚îÇ   ‚îú‚îÄ‚îÄ image.jpg\n    ‚îÇ   ‚îî‚îÄ‚îÄ index.qmd\n    ‚îú‚îÄ‚îÄ aws-storage-processx\n    ‚îÇ   ‚îú‚îÄ‚îÄ image.jpg\n    ‚îÇ   ‚îú‚îÄ‚îÄ index.qmd\n    ‚îÇ   ‚îî‚îÄ‚îÄ index.rmarkdown\n    ‚îú‚îÄ‚îÄ logos-equipment\n    ‚îÇ   ‚îú‚îÄ‚îÄ image.jpg\n    ‚îÇ   ‚îî‚îÄ‚îÄ index.qmd\n    ‚îî‚îÄ‚îÄ snakecase\n        ‚îú‚îÄ‚îÄ all.jpeg\n        ‚îú‚îÄ‚îÄ index.qmd\n        ‚îî‚îÄ‚îÄ thumbnail.jpg\n\n\nCode\nt &lt;- s3fs::s3_dir_info(bucket_path, recurse = TRUE)\n\n\nNow we will remove test.txt\n\n\nCode\nfile.remove(here::here('posts/test.txt'))\n\n\n[1] TRUE\n\n\nNow we sync again.\n\n\nCode\nsys_call()\n\n\ndelete: s3://new-graphiti/posts/test.txt\nExit status: 0 \nOutput:\n delete: s3://new-graphiti/posts/test.txt\n\n\nList our bucket contents and capture them again\n\n\nCode\ns3_dir_tree(bucket_path)\n\n\ns3://new-graphiti\n‚îî‚îÄ‚îÄ posts\n    ‚îú‚îÄ‚îÄ _metadata.yml\n    ‚îú‚îÄ‚îÄ aws-storage-permissions\n    ‚îÇ   ‚îú‚îÄ‚îÄ image.jpg\n    ‚îÇ   ‚îî‚îÄ‚îÄ index.qmd\n    ‚îú‚îÄ‚îÄ aws-storage-processx\n    ‚îÇ   ‚îú‚îÄ‚îÄ image.jpg\n    ‚îÇ   ‚îú‚îÄ‚îÄ index.qmd\n    ‚îÇ   ‚îî‚îÄ‚îÄ index.rmarkdown\n    ‚îú‚îÄ‚îÄ logos-equipment\n    ‚îÇ   ‚îú‚îÄ‚îÄ image.jpg\n    ‚îÇ   ‚îî‚îÄ‚îÄ index.qmd\n    ‚îî‚îÄ‚îÄ snakecase\n        ‚îú‚îÄ‚îÄ all.jpeg\n        ‚îú‚îÄ‚îÄ index.qmd\n        ‚îî‚îÄ‚îÄ thumbnail.jpg\n\n\nCode\nt2 &lt;- s3fs::s3_dir_info(bucket_path, recurse = TRUE)\n\n\nCompare the file structure before and after our sync.\n\n\nCode\nwaldo::compare(t$key, t2$key)\n\n\n     old                             | new                                 \n [9] \"posts/snakecase/all.jpeg\"      | \"posts/snakecase/all.jpeg\"      [9] \n[10] \"posts/snakecase/index.qmd\"     | \"posts/snakecase/index.qmd\"     [10]\n[11] \"posts/snakecase/thumbnail.jpg\" | \"posts/snakecase/thumbnail.jpg\" [11]\n[12] \"posts/test.txt\"                -                                     \n\n\nSuccess!!\n\n\nTo Do\nWe need to build the call to sync the other way (cloud to local) in a way that perhaps nukes local files if they are not on the cloud. This is because we need to collaborate within our team so we do things like one person will change the name of images so when the other person syncs they will have only the newly named image in their local directory.\n\nThis all deserved consideration as it could get really messy from a few different angles (ie. one person adds files they don‚Äôt want nuked and then they get nukes. There are lots of different options for doing things so we will get there.)\n\n\nDelete Bucket\nLets delete the bucket.\n\n\nCode\n#\nHere is the command line approach that we will turn off in favor of the s3fs approach.\nargs &lt;- c('s3', 'rb', bucket_path, '--force')\nsys_call()\n\n\n\n\nCode\n# Here is the `s3fs` way to \"delete\" all the versions.  \n# list all the files in the bucket\nfl &lt;- s3fs::s3_dir_ls(bucket_path, recurse = TRUE, refresh = TRUE)\n\n# list all the version info for all the files\nvi &lt;- fl |&gt; \n  purrr::map_df(s3fs::s3_file_version_info)\n\ns3fs::s3_file_delete(path = vi$uri)\n\n\n\n\nCode\ns3fs::s3_bucket_delete(bucket_path)\n\n\n[1] \"s3://new-graphiti\"\n\n\nAs we have tried this before we know that if we tell it we want to delete a bucket with versioned files in it we need to empty the bucket first including delete_markers. That is easy in the aws console with th UI but seems tricky. There is a bunch of discussion on options to this here https://stackoverflow.com/questions/29809105/how-do-i-delete-a-versioned-bucket-in-aws-s3-using-the-cli . Thinking a good way around it (and a topic for another post) would be to apply a lifecycle-configuration to the bucket that deletes all versions of files after a day - allowing you to delete bucket after they expire (as per the above post). Really we may want to have a lifecycle-configuration on all our versioned buckets to keep costs down anyway but deserves more thought and perhaps another post.\n\n\nCode\n# old notes\n# We are going to test creating a bucket with versioning on.  This has large implications for billing with some details\n# of how it works [here](https://aws.amazon.com/blogs/aws/amazon-s3-enhancement-versioning/) with example of costs [here](https://aws.amazon.com/s3/faqs/?nc1=h_ls).  Thinking we may want versioned buckets for things like `sqlite`\n# \"snapshot\" databases but definitely not for things like images."
  },
  {
    "objectID": "posts/2024-05-27-references-bib-succinct/index.html",
    "href": "posts/2024-05-27-references-bib-succinct/index.html",
    "title": "Cleaning up bib files flexibly with Zotero and R",
    "section": "",
    "text": "Here we will clean up a bib file exported from Zotero using R to contain only the entries found in a Rmarkdown file.\nCode\nlibrary(here)\n\n\nhere() starts at /Users/airvine/Projects/repo/new_graphiti\n\n\nCode\nlibrary(stringr)\nlibrary(knitr)\n\n# get the name of this post directory\npost_dir &lt;- paste0(here::here(), \"/posts/\", params$post_dir_name)\npost_dir_fig &lt;- paste0(post_dir, \"/fig/\")\nLet‚Äôs define our .bib files\nCode\nrmd_file &lt;- \"~/Projects/repo/fish_passage_peace_2023_reporting/fish_passage_peace_2023_reporting.Rmd\"\nbib_file &lt;- paste0(here::here(), \"/assets/NewGraphEnvironment.bib\")\noutput_file &lt;- \"~/Projects/repo/fish_passage_peace_2023_reporting/references.bib\"\nHere are the functions:\nCode\n# Function to extract citations from an RMarkdown file\nbib_extract_citations &lt;- function(rmd_file, additional_citations = c()) {\n  # Read the entire RMarkdown file\n  lines &lt;- readLines(rmd_file)\n  \n  # Concatenate all lines into a single string\n  text &lt;- paste(lines, collapse = \" \")\n  \n  # Regular expression to find citations in the form of @this_citation or [@that_citation; @another_citation]\n  citation_pattern &lt;- \"@[a-zA-Z0-9_:-]+\"\n  \n  # Extract all citations\n  citations &lt;- str_extract_all(text, citation_pattern)[[1]]\n  \n  # Remove the leading '@' from the citations\n  citations &lt;- unique(sub(\"^@\", \"\", citations))\n  \n  # Combine with additional citations\n  all_citations &lt;- unique(c(citations, additional_citations))\n  \n  return(all_citations)\n}\n\n# Function to clean a BibTeX file to keep only cited entries\nbib_clean &lt;- function(bib_file, citations, output_file) {\n  file.create(output_file)\n  # Read the entire BibTeX file\n  lines &lt;- readLines(bib_file)\n  \n  # Initialize variables\n  keep_entry &lt;- FALSE\n  cleaned_lines &lt;- c()\n  \n  for (line in lines) {\n    # Check if the line starts a new citation entry\n    if (grepl(\"^@\", line)) {\n      # Extract the citation key\n      citation_key &lt;- sub(\"^@.*\\\\{([^,]+),.*\", \"\\\\1\", line)\n      \n      # Determine if the entry should be kept\n      keep_entry &lt;- citation_key %in% citations\n    }\n    \n    # Add the line to cleaned_lines if the entry is to be kept\n    if (keep_entry) {\n      cleaned_lines &lt;- c(cleaned_lines, line)\n    }\n  }\n  \n  # Write the cleaned lines to the output file\n  writeLines(cleaned_lines, output_file)\n  \n  cat(\"Cleaned BibTeX file created:\", output_file, \"\\n\")\n}\nAs a big part of the motivation to do this is to reduce the bloat in our repositories we will add the default name of the bib file to the .gitignore of this repo.\nCode\nknitr::include_graphics(paste0(post_dir_fig, \"Screen Shot 2024-05-27 at 1.40.44 PM.png\"))\n\n\n\n\n\n\n\n\n\nCode\nknitr::include_graphics(paste0(post_dir_fig, \"Screen Shot 2024-05-27 at 1.40.55 PM.png\"))"
  },
  {
    "objectID": "posts/2024-05-27-references-bib-succinct/index.html#extract-citations-from-the-rmarkdown-file",
    "href": "posts/2024-05-27-references-bib-succinct/index.html#extract-citations-from-the-rmarkdown-file",
    "title": "Cleaning up bib files flexibly with Zotero and R",
    "section": "Extract citations from the RMarkdown file",
    "text": "Extract citations from the RMarkdown file\n\n\nCode\n# Extract citations from the RMarkdown file\ncitations &lt;- bib_extract_citations(rmd_file, additional_citations = nocites)"
  },
  {
    "objectID": "posts/2024-05-27-references-bib-succinct/index.html#clean-the-bibtex-file-to-keep-only-cited-entries",
    "href": "posts/2024-05-27-references-bib-succinct/index.html#clean-the-bibtex-file-to-keep-only-cited-entries",
    "title": "Cleaning up bib files flexibly with Zotero and R",
    "section": "Clean the BibTeX file to keep only cited entries",
    "text": "Clean the BibTeX file to keep only cited entries\n\n\nCode\nbib_clean(bib_file, citations, output_file)\n\n\nCleaned BibTeX file created: ~/Projects/repo/fish_passage_peace_2023_reporting/references.bib"
  },
  {
    "objectID": "posts/snakecase/index.html",
    "href": "posts/snakecase/index.html",
    "title": "snake_case vs Everything_Else",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, dpi=60, out.width = \"100%\")\noptions(scipen=999)\noptions(knitr.kable.NA = '--') #'--'\noptions(knitr.kable.NAN = '--')\n\n\nBeen preferring snakecase naming convention for all files and columns and variables for a long time for the following reasons:\n\nIt‚Äôs much easier to type. Reaching for the shift button is a pain.\nArguably easier to read. I find it easier to read snake_case than CamelCase.\nCan make it easy to name things in a fashion that allows you to dissect, what, something, is by the way it is named and allows automatic sorting to group similar things together. This presentation by Jenny Bryan is a good read - https://speakerdeck.com/jennybc/how-to-name-files-the-sequel.\n\n\n\nCode\nknitr::include_graphics(\"thumbnail.jpg\")\n\n\n\n\n\n\n\n\nFigure¬†1\n\n\n\n\n\nWorth noting that sometimes-rules-need-to-be-broken sometimes such as when you are naming chunks in Rmarkdown. It breaks our cross-references.\n\nAs we see here though - it doesn‚Äôt matter in Quarto (ex. see Figure¬†1) vs Figure¬†2).\n\n\nCode\nknitr::include_graphics(\"all.jpeg\")\n\n\n\n\n\n\n\n\nFigure¬†2\n\n\n\n\n\n\n‚Äúno names have an anonymous function‚Äù.\n¬†¬†¬†¬†-Michael Sumner"
  },
  {
    "objectID": "posts/2024-06-19-precipitation/index.html",
    "href": "posts/2024-06-19-precipitation/index.html",
    "title": "Mapping and Plotting Precipitation with R",
    "section": "",
    "text": "Really interested in quantifying and visualizing weather data for specific areas that we are working‚Ä¶. Here is a first start.\n\nWARNING - this work is stolen!! I have adapted this from a repository on GitHub from the wonderfully talented Milos Popovic. All credit to Milos! What a boss - really awesome stuff.\n\nAlso of note is the image used for the blog. That is Cotey Gallagher‚Ä¶ I hope she doesn‚Äôt sue me. https://www.linkedin.com/pulse/how-crazy-would-could-really-rain-cats-dogs-cotey-gallagher/\n\nFirst thing we will do is load our packages. If you do not have the packages installed yet you can change the update_pkgs param in the yml of this file to TRUE. Using pak is great because it allows you to update your packages when you want to.\n\n\nCode\npkgs_cran &lt;- c(\n  \"here\",\n  \"fs\",\n  \"pRecipe\",\n  \"giscoR\",\n  \"terra\",\n  \"tidyverse\",\n  \"rayshader\",\n  \"sf\",\n  \"classInt\",\n  \"rgl\")\n\npkgs_gh &lt;- c(\n  \"poissonconsulting/pgfeatureserv\",\n  \"poissonconsulting/fwapgr\",\n  \"NewGraphEnvironment/rfp\"\n  # we will turn this off since the function it uses won't run for folks without db credentials\n  # \"NewGraphEnvironment/fpr\"\n )\n\npkgs &lt;- c(pkgs_cran, pkgs_gh)\n\nif(params$update_pkgs){\n  # install the pkgs\n  lapply(pkgs,\n         pak::pkg_install,\n         ask = FALSE)\n}\n\n# load the pkgs\npkgs_ld &lt;- c(pkgs_cran,\n             basename(pkgs_gh))\ninvisible(\n  lapply(pkgs_ld,\n       require,\n       character.only = TRUE)\n)\n\n\n\nDefine our Area of Interest\nHere we diverge a bit from Milos version as we are going to load a custom area of interest. We will be connecting to our remote database using Poisson Consulting‚Äôs fwapgr::fwa_watershed_at_measure function which leverages the in database FWA_WatershedAtMeasure function from Simon Norris‚Äô wonderful fwapg package.\n\nWe use a blue line key and a downstream route measure to define our area of interest which is the Neexdzii Kwa (a.k.a Upper Bulkley River) near Houston, British Columbia.\n\nAs per the Freshwater Atlas of BC - the blue line key:\n\nUniquely identifies a single flow line such that a main channel and a secondary channel with the same watershed code would have different blue line keys (the Fraser River and all side channels have different blue line keys).\n\n\nA downstream route measure is:\n\nThe distance, in meters, along the route from the mouth of the route to the feature. This distance is measured from the mouth of the containing route to the downstream end of the feature.\n\n\n\nCode\n# lets build a custom watersehed just for upstream of the confluence of Neexdzii Kwa and Wetzin Kwa\n# blueline key\nblk &lt;- 360873822\n# downstream route measure\ndrm &lt;- 166030.4\n\naoi &lt;- fwapgr::fwa_watershed_at_measure(blue_line_key = blk, \n                                        downstream_route_measure = drm) |&gt; \n  sf::st_transform(4326)\n\n\n\n\nRetrieve the Precipitation Data\nFor this example we will retrieve our precipitation data from Multi-Source Weighted-Ensemble Precipitation using the pRecipe package.\n\n\nCode\n# let's create our data directory\ndir_data &lt;- here::here('posts', params$post_dir_name, \"data\")\n\nfs::dir_create(dir_data)\n\n\nTo actually download the data we are going to put a chunk option that allows us to just execute the code once and update it with the update_gis param in our yml. We will use usethis::use_git_ignore to add the data to our .gitignore file so that we do not commit that insano enourmouse file to our git repository.\n\n\nCode\npRecipe::download_data(\n    dataset = \"mswep\",\n    path = dir_data,\n    domain = \"raw\",\n    timestep = \"yearly\"\n)\n\nusethis::use_git_ignore(paste0('posts/', params$post_dir_name, \"/data/*\"))\n\n\nNow we read in our freshly downloaded .nc file and clip to our area of interest.\n\n\nCode\n# get the name of the file with a .nc at the end\nnc_file &lt;- fs::dir_ls(dir_data, glob = \"*.nc\")\n\nmswep_data &lt;- terra::rast(\n  nc_file\n) |&gt;\nterra::crop(\n    aoi\n)\n\n\nNext we extract the years of the data from the filename of the .nc file and then transform the data into a dataframe. We need to remove the data from 2023 because it is only for January as per the filename:\nmswep_tp_mm_global_197902_202301_025_yearly.nc\n\n\nCode\n# the names of the datasets are arbitrary (precipitation_1:precipitation_45) \n# we will rename the datasets to the years.  \n# here we extract 2023 from the nc_file name of the file using regex\nyear_end &lt;- as.numeric(stringr::str_extract(basename(nc_file), \"(?&lt;=_\\\\d{6}_)\\\\d{4}\"))\nyear_start &lt;- as.numeric(stringr::str_extract(basename(nc_file), \"(?&lt;=_)[0-9]{4}(?=[0-9]{2}_[0-9]{6}_)\"))\n\n# assign the names to replace \nnames(mswep_data) &lt;- year_start:year_end\n\n\nmswep_df &lt;- mswep_data |&gt;\n    as.data.frame(xy = TRUE) |&gt;\n    tidyr::pivot_longer(\n        !c(\"x\", \"y\"),\n        names_to = \"year\",\n        values_to = \"precipitation\"\n    ) |&gt; \n  # 2023 is not complete so we remove it\n    dplyr::filter(year != 2023)\n\n\n\n\nGet Additional Data\nWe could use some data for context such as major streams, highways and the railway. We get the streams and railway from data distribution bc api using the bcdata package. Our rfp package calls just allow some extra sanity checks on the bcdata::bcdc_query_geodata function. It‚Äôs not really necessary but can be helpful when errors occur (ex. the name of the column to filter on is input incorrectly).\n\n\n\nCode\n# grab all the railways\nl_rail &lt;- rfp::rfp_bcd_get_data(\n    bcdata_record_id = stringr::str_to_upper(\"whse_basemapping.gba_railway_tracks_sp\")\n) |&gt; \n  sf::st_transform(4326) |&gt; \n  janitor::clean_names() \n\n# streams in the bulkley and then filter to just keep the big ones\nl_streams &lt;- rfp::rfp_bcd_get_data(\n  bcdata_record_id = stringr::str_to_upper(\"whse_basemapping.fwa_stream_networks_sp\"),\n  col_filter = stringr::str_to_upper(\"watershed_group_code\"),\n  col_filter_value = \"BULK\",\n  # grab a smaller object by including less columns\n  col_extract = stringr::str_to_upper(c(\"linear_feature_id\", \"stream_order\"))\n) |&gt; \n  sf::st_transform(4326) |&gt; \n  janitor::clean_names() |&gt; \n  dplyr::filter(stream_order &gt; 4)\n\n\nBecause the highways we use in our mapping are not available for direct download from the Data Distribution BC api (some other versions are here we will query them from our remote database. The function used (fpr::fpr_db_query) is a wrapper around the DBI::dbGetQuery function that allows us to query our remote database by calling our environmental variables and making a connection. This will not work without the proper credentials so if you were trying to reproduce this and don‚Äôt have the credentials you won‚Äôt be able to retrieve the roads. To get around this we have stored the trimmed roads data in the data directory of this post so we can read it in from there.\n\n\nCode\n# highways\n# define the type of roads we want to include using the transport_line_type_code. We will include RA1 and RH1 (Road arerial/highway major)\nrd_codes &lt;- c(\"RA1\", \"RH1\")\nl_rds &lt;- fpr::fpr_db_query(\n  query = glue::glue(\"SELECT transport_line_id, structured_name_1, transport_line_type_code, geom FROM whse_basemapping.transport_line WHERE transport_line_type_code IN ({glue::glue_collapse(glue::single_quote(rd_codes), sep = ', ')})\")\n  )|&gt; \n  sf::st_transform(4326) \n\n\nsf::st_intersection(l_rds, \n                    # we will remove all the aoi columns except the geometry so we don't get all the aoi columns appended\n                    aoi |&gt; dplyr::select(geometry)) |&gt; \n  sf::st_write(here::here('posts', params$post_dir_name, \"data\", \"l_rds.gpkg\"), delete_dsn = TRUE)\n\n\nNow we trim up all those layers. We have some functions to validate and repair geometries and then we clip them to our area of interest.\n\n\nCode\n# we don't actually need to trim the rds since we already did that but for simplicity we will do it again\n  l_rds &lt;- sf::st_read(here::here('posts', params$post_dir_name, \"data\", \"l_rds.gpkg\"), quiet = TRUE) \n\n\nlayers_to_trim &lt;- tibble::lst(l_rail, l_streams, l_rds)\n\n# Function to validate and repair geometries\nvalidate_geometries &lt;- function(layer) {\n  layer &lt;- sf::st_make_valid(layer)\n  layer &lt;- layer[sf::st_is_valid(layer), ]\n  return(layer)\n}\n\n# Apply validation to the AOI and layers\naoi &lt;- validate_geometries(aoi)\nlayers_to_trim &lt;- purrr::map(layers_to_trim, validate_geometries)\n\n# clip them  with purrr and sf\nlayers_trimmed &lt;- purrr::map(\n  layers_to_trim,\n  ~ sf::st_intersection(.x, aoi)\n) \n\n\n\n\nPlot the Precipitation Data by Year\nFirst thing we do here is highjack the plot function from Milos.\n\n\nCode\ntheme_for_the_win &lt;- function(){\n    theme_minimal() +\n    theme(\n        axis.line = element_blank(),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        legend.position = \"right\",\n        legend.title = element_text(\n            size = 11, color = \"grey10\"\n        ),\n        legend.text = element_text(\n            size = 10, color = \"grey10\"\n        ),\n        panel.grid.major = element_line(\n            color = NA\n        ),\n        panel.grid.minor = element_line(\n            color = NA\n        ),\n        plot.background = element_rect(\n            fill = NA, color = NA\n        ),\n        legend.background = element_rect(\n            fill = \"white\", color = NA\n        ),\n        panel.border = element_rect(\n            fill = NA, color = NA\n        ),\n        plot.margin = unit(\n            c(\n                t = 0, r = 0,\n                b = 0, l = 0\n            ), \"lines\"\n        )\n    )\n}\n\nbreaks &lt;- classInt::classIntervals(\n    mswep_df$precipitation,\n    n = 5,\n    style = \"equal\"\n)$brks\n\ncolors &lt;- hcl.colors(\n    n = length(breaks),\n    palette = \"Temps\",\n    rev = TRUE\n)\n\n\nNow we plot the data by year.\n\n\nCode\nmap1 &lt;- ggplot(\n    data = mswep_df\n) +\ngeom_raster(\n    aes(\n        x = x,\n        y = y,\n        fill = precipitation\n    )\n) +\ngeom_contour(\n    aes(\n       x = x,\n       y = y,\n       z = precipitation\n    ), color = \"white\" # add this line\n) +\ngeom_sf(\n    data = aoi,\n    fill = \"transparent\",\n    color = \"grey10\",\n    size = .5\n) +\nscale_fill_gradientn(\n    name = \"mm\",\n    colors = colors,\n    breaks = breaks,\n    labels = round(breaks, 0), # use round(breaks, 0)\n    limits = c(\n        min(mswep_df$precipitation),\n        max(mswep_df$precipitation)\n    )\n) +\nfacet_wrap(~year) +\nguides(\n    fill = guide_colourbar(\n        direction = \"vertical\",\n        barheight = unit(50, \"mm\"),\n        barwidth = unit(5, \"mm\"),\n        title.position = \"top\",\n        label.position = \"right\",\n        title.hjust = .5,\n        label.hjust = .5,\n        ncol = 1,\n        byrow = FALSE\n    )\n) +\ntheme_for_the_win()\n\nmap1\n\n\n\n\n\n\n\n\n\nPretty cool. Interesting to see really wet and dry years in the last 20 years or so such as the wet ones in 2004, 2007, 2011 and 2020 and the dry ones in 2000, 2010, 2014, 2021 and 2022. The contours on the maps are really interesting as they show the gradients which generally run west to east - but occasionally run south to north.\n\n\nAverage Precipitation\nNow we will average all the years together to get an average precipitation map. We will add our additional layers for context too. Roads are black, railways are yellow and streams are blue.\n\n\nCode\nmswep_average_df &lt;- mswep_df |&gt;\n  dplyr::group_by(\n    x, y, .drop = FALSE\n  ) |&gt;\n  dplyr::summarise(\n    precipitation = mean(precipitation)\n  ) |&gt; \n  dplyr::mutate(year = \"average\")\n\n\nbreaks &lt;- classInt::classIntervals(\n  mswep_average_df$precipitation,\n  n = 5,\n  style = \"equal\"\n)$brks\n\n\ncolors &lt;- hcl.colors(\n  n = length(breaks),\n  palette = \"Temps\",\n  rev = TRUE\n)\n\nmap_average &lt;- ggplot(\n  data = mswep_average_df\n) +\n  geom_raster(\n    aes(\n      x = x,\n      y = y,\n      fill = precipitation\n    )\n  ) +\n  geom_contour(\n    aes(\n      x = x,\n      y = y,\n      z = precipitation\n    ), color = \"white\" # add this line\n  ) +\n  geom_sf(\n    data = aoi,\n    fill = \"transparent\",\n    color = \"grey10\",\n    size = .5\n  ) +\n  scale_fill_gradientn(\n    name = \"mm\",\n    colors = colors,\n    breaks = breaks,\n    labels = round(breaks, 0), # use round(breaks, 0)\n    limits = c(\n      min(mswep_average_df$precipitation),\n      max(mswep_average_df$precipitation)\n    )\n  ) +\n  guides(\n    fill = guide_colourbar(\n      direction = \"vertical\",\n      barheight = unit(50, \"mm\"),\n      barwidth = unit(5, \"mm\"),\n      title.position = \"top\",\n      label.position = \"right\",\n      title.hjust = .5,\n      label.hjust = .5,\n      ncol = 1,\n      byrow = FALSE\n    )\n  ) +\n  geom_sf(\n    data = layers_trimmed$l_rds,\n    color = \"black\",\n    size = .8\n  ) +\n  geom_sf(\n    data = layers_trimmed$l_streams,\n    color = \"blue\",\n    size = .8\n  ) +\n  geom_sf(\n    data = layers_trimmed$l_rail,\n    color = \"yellow\",\n    size = .8\n  ) +\n  theme_for_the_win()\n\nmap_average\n\n\n\n\n\n\n\n\n\n\n\nCompare the Average Precipitation to a Specific Year\nWe often talk about a ‚Äúdry‚Äù year or a ‚Äúwet‚Äù year. Let‚Äôs compare the average precipitation to a specific year. We will build a little function to do this so that we can easily compare any year to the average.\n\n\nCode\nmap_vs_average &lt;- function(year_compare){\n  \n  mswep_df_2022 &lt;- mswep_df |&gt;\n    dplyr::filter(year == year_compare) |&gt; \n    dplyr::mutate(year = as.character(year))\n  \n  mswep_df_compare &lt;- bind_rows(mswep_average_df, mswep_df_2022)\n  \n  breaks &lt;- classInt::classIntervals(\n    mswep_df_compare$precipitation,\n    n = 5,\n    style = \"equal\"\n  )$brks\n  \n  colors &lt;- hcl.colors(\n    n = length(breaks),\n    palette = \"Temps\",\n    rev = TRUE\n  )\n  \n  ggplot(\n    data = mswep_df_compare\n  ) +\n    facet_wrap(~year) +\n    geom_raster(\n      aes(\n        x = x,\n        y = y,\n        fill = precipitation\n      )\n    ) +\n    geom_contour(\n      aes(\n        x = x,\n        y = y,\n        z = precipitation\n      ), color = \"white\" # add this line\n    ) +\n    geom_sf(\n      data = aoi,\n      fill = \"transparent\",\n      color = \"grey10\",\n      size = .5\n    ) +\n    scale_fill_gradientn(\n      name = \"mm\",\n      colors = colors,\n      breaks = breaks,\n      labels = round(breaks, 0), # use round(breaks, 0)\n      limits = c(\n        min(mswep_df_compare$precipitation),\n        max(mswep_df_compare$precipitation)\n      )\n    ) +\n    guides(\n      fill = guide_colourbar(\n        direction = \"vertical\",\n        barheight = unit(50, \"mm\"),\n        barwidth = unit(5, \"mm\"),\n        title.position = \"top\",\n        label.position = \"right\",\n        title.hjust = .5,\n        label.hjust = .5,\n        ncol = 1,\n        byrow = FALSE\n      )\n    ) +\n    geom_sf(\n      data = layers_trimmed$l_rds,\n      color = \"black\",\n      size = .8\n    ) +\n    geom_sf(\n      data = layers_trimmed$l_streams,\n      color = \"blue\",\n      size = .8\n    ) +\n    geom_sf(\n      data = layers_trimmed$l_rail,\n      color = \"yellow\",\n      size = .8\n    ) +\n    theme_for_the_win()\n}\n\n\nFirst let‚Äôs check out 2022 as it seemed pretty dry.\n\n\nCode\nmap_vs_average(2022)\n\n\n\n\n\n\n\n\n\nNow let‚Äôs look at 2020 as that seemed wet with lots of streams flowing really nicely.\n\n\nCode\nmap_vs_average(2020)\n\n\n\n\n\n\n\n\n\nDefinitely wetter than average.\n\n\n3D Contour Map\nLet‚Äôs make an interactive 3D contour map of the average precipitation data. Use the mouse to rotate the map and zoom in and out!\n\n\nCode\n{\n  \n  rayshader::plot_gg(\n    ggobj = map_average,\n    width = 7,\n    height = 7,\n    scale = 250,\n    solid = FALSE,\n    shadow = TRUE,\n    shadowcolor = \"white\",\n    shadowwidth = 0,\n    shadow_intensity = 1,\n    # window.size = c(600, 600),\n    zoom = .7,\n    phi = 30,\n    theta = 337.5\n    # fov =30\n  )\n  rgl::rglwidget(width = 982, height = 1025, reuse = FALSE)\n}\n\n\n\n\n\n\nThanks Miles!"
  }
]