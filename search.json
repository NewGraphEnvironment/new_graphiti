[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "rws",
    "section": "",
    "text": "raNdom workish stufF"
  },
  {
    "objectID": "posts/2024-11-15-bcdata-ortho-historic/index.html",
    "href": "posts/2024-11-15-bcdata-ortho-historic/index.html",
    "title": "Getting details of historic orthophoto imagery with R",
    "section": "",
    "text": "We would like to obtain historic ortho photo imagery so that we can compare historic watershed conditions compared to current (ex. floodplain vegetation clearing, channel morphology, etc.). For our use case — restoration baseline condition assessment and impact evaluation of land cover change — our goal is to reconstruct historical conditions for entire sub-basins, as far back as possible (e.g., 1930 or 1960), and programmatically compare these to recent remotely sensed land cover analysis.\nCode\nsuppressMessages(library(tidyverse))\nlibrary(ggplot2)\nlibrary(bcdata)\nlibrary(fwapgr)\nlibrary(knitr)\nsuppressMessages(library(sf))\nlibrary(crosstalk)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(DT)\nlibrary(htmltools)\nCode\npath_post &lt;- fs::path(\n  here::here(),\n  \"posts\",\n  params$post_name\n)\nCode\nstaticimports::import(\n  dir = fs::path(\n    path_post,\n    \"scripts\"\n  ),\n  outfile = fs::path(\n    path_post,\n    \"scripts\",\n    \"staticimports\",\n    ext = \"R\"\n  )\n)\nCode\nsource(\n  fs::path(\n    path_post,\n    \"scripts\",\n    \"staticimports\",\n    ext = \"R\"\n  )\n)\n\n\nlfile_name &lt;- function(dat_name = NULL, ext = \"geojson\") {\n  fs::path(\n    path_post,\n    \"data\",\n    paste0(dat_name, \".\", ext)\n  )\n}\n\nlburn_sf &lt;- function(dat = NULL, dat_name = NULL) {\n  if (is.null(dat_name)) {\n    cli::cli_abort(\"You must provide a name for the GeoJSON file using `dat_name`.\")\n  }\n  \n  dat |&gt;\n    sf::st_write(\n      lfile_name(dat_name),\n      delete_dsn = TRUE\n      # append = FALSE\n    )\n}\n\n# Function to validate and repair geometries\nlngs_geom_validate &lt;- function(layer) {\n  layer &lt;- sf::st_make_valid(layer)\n  layer &lt;- layer[sf::st_is_valid(layer), ]\n  return(layer)\n}\nCode\n# definet he buffer in m\nbuf &lt;- 1500"
  },
  {
    "objectID": "posts/2024-11-15-bcdata-ortho-historic/index.html#generate-an-area-of-interest",
    "href": "posts/2024-11-15-bcdata-ortho-historic/index.html#generate-an-area-of-interest",
    "title": "Getting details of historic orthophoto imagery with R",
    "section": "Generate an Area of Interest",
    "text": "Generate an Area of Interest\nHere we download our area of interest which is the Neexdzii Kwah River (a.k.a Upper Bulkley River) which is located between Houston, BC (just south of Smithers) and Topley, BC which is east of Houston and north of Burns Lake, BC. We hit up our remote database managed by Simon Norris with a package built by Poisson Consulting specifically for the task. We use the downstream_route_measure of the Bulkley River (identified through a unique blue_line_key) to query the watershed area upstream of the point where the Neexdzii Kwah River enters the Wedzin Kwah River (a.k.a Morice River). Since photopoint centres that fall just outside of the watershed can provide imagery of the edge areas of the watershed we buffer this area to an amount that we approximate is half the width or hieght of the ground distance captured by each image (1500m).\n\n\nCode\n# lets build a custom watersehed just for upstream of the confluence of Neexdzii Kwa and Wetzin Kwa\n# blueline key\nblk &lt;- 360873822\n# downstream route measure\ndrm &lt;- 166030.4\n\n\n\naoi_raw &lt;- fwapgr::fwa_watershed_at_measure(blue_line_key = blk, \n                                        downstream_route_measure = drm) |&gt; \n  # we put it in utm zone 9 so we can easily buffer using meters\n  sf::st_transform(32609) |&gt; \n  dplyr::select(geometry)\n\naoi &lt;- sf::st_buffer(\n  aoi_raw,\n  dist = buf\n) |&gt; \n  sf::st_transform(4326)\n\n\n#get the bounding box of our aoi\n# aoi_bb &lt;- sf::st_bbox(aoi)\n\n#lets burn this so we don't need to download each time\naoi_raw &lt;- lngs_geom_validate(aoi_raw)\naoi &lt;- lngs_geom_validate(aoi)\n\n\n\n# now lets buffer our aoi by 1000m\n\nlburn_sf(\n  aoi,\n  deparse(substitute(aoi)))\n\nlburn_sf(\n  aoi_raw,\n  deparse(substitute(aoi_raw)))\n\n# map_aoi &lt;- ggplot() +\n#   geom_sf(\n#       data = aoi_raw,\n#       fill = \"transparent\",\n#       color = \"black\",\n#       linewidth = .5\n#   ) +\n#   geom_sf(\n#       data = aoi,\n#       fill = \"transparent\",\n#       color = \"red\",\n#       linewidth = .5\n#   ) \n#   \n# map_aoi"
  },
  {
    "objectID": "posts/2024-11-15-bcdata-ortho-historic/index.html#download-spatial-data-layers-from-bc-data-catalogue",
    "href": "posts/2024-11-15-bcdata-ortho-historic/index.html#download-spatial-data-layers-from-bc-data-catalogue",
    "title": "Getting details of historic orthophoto imagery with R",
    "section": "Download Spatial Data Layers From BC DAta Catalogue",
    "text": "Download Spatial Data Layers From BC DAta Catalogue\nNext we grab a few key layers from the BC Data Catalogue API using convenience function from our rfp package (“Reproducable Field Products”) which wrap the provincially maintained bcdata package. We grab:\n\nRailways\nStreams in the Bulkley Watershed group that are 4th order or greater.\nOrthophoto Tile Polygons\nHistoric Imagery Points\nHistoric Imagery Polygons\nNTS 1:50,000 Grid (we will see why in a second)\nAir Photo Centroids\n\n\n\nCode\n# grab all the railways\nl_rail &lt;- rfp::rfp_bcd_get_data(\n    bcdata_record_id = \"whse_basemapping.gba_railway_tracks_sp\"\n) |&gt; \n  sf::st_transform(4326) |&gt; \n  janitor::clean_names() \n\n\n# streams in the bulkley and then filter to just keep the big ones\nl_streams &lt;- rfp::rfp_bcd_get_data(\n  bcdata_record_id = \"whse_basemapping.fwa_stream_networks_sp\",\n  col_filter = \"watershed_group_code\",\n  col_filter_value = \"BULK\",\n  # grab a smaller object by including less columns\n  col_extract = c(\"linear_feature_id\", \"stream_order\", \"gnis_name\", \"downstream_route_measure\", \"blue_line_key\", \"length_metre\")\n) |&gt; \n  sf::st_transform(4326) |&gt; \n  janitor::clean_names() |&gt; \n  dplyr::filter(stream_order &gt;= 4)\n\n# historic orthophotos\n# WHSE_IMAGERY_AND_BASE_MAPS.AIMG_HIST_INDEX_MAPS_POLY\n#https://catalogue.data.gov.bc.ca/dataset/airborne-imagery-historical-index-map-points\nl_imagery_tiles &lt;- rfp::rfp_bcd_get_data(\n  # https://catalogue.data.gov.bc.ca/dataset/orthophoto-tile-polygons/resource/f46aaf7b-58be-4a25-a678-79635d6eb986\n  bcdata_record_id = \"WHSE_IMAGERY_AND_BASE_MAPS.AIMG_ORTHOPHOTO_TILES_POLY\") |&gt; \n  sf::st_transform(4326) \n\nl_imagery_hist_pnts &lt;- rfp::rfp_bcd_get_data(\n  bcdata_record_id = \"WHSE_IMAGERY_AND_BASE_MAPS.AIMG_HIST_INDEX_MAPS_POINT\") |&gt; \n  sf::st_transform(4326) \n\nl_imagery_hist_poly &lt;- rfp::rfp_bcd_get_data(\n  bcdata_record_id = \"WHSE_IMAGERY_AND_BASE_MAPS.AIMG_HIST_INDEX_MAPS_POLY\") |&gt; \n  sf::st_transform(4326) \n\nl_imagery_grid &lt;- rfp::rfp_bcd_get_data(\n  bcdata_record_id = \"WHSE_BASEMAPPING.NTS_50K_GRID\") |&gt; \n  sf::st_transform(4326) \n\n\nFollowing download we run some clean up to ensure the geometry of our spatial files is “valid”, trim to our area of interest and burn locally so that every time we rerun iterations of this memo we don’t need to wait for the download process which takes a little longer than we want to wait.\n\n\nCode\n# get a list of the objects in our env that start with l_\nls &lt;- ls()[stringr::str_starts(ls(), \"l_\")] \n\nlayers_all &lt;- tibble::lst(\n  !!!mget(ls)\n)\n\n# Apply validation to the AOI and layers\nlayers_all &lt;- purrr::map(\n  layers_all, \n  lngs_geom_validate\n  )\n\n# clip them  with purrr and sf\nlayers_trimmed &lt;- purrr::map(\n  layers_all,\n  ~ sf::st_intersection(.x, aoi)\n) \n\n# Burn each `sf` object to GeoJSON\npurrr::walk2(\n  layers_trimmed,\n  names(layers_trimmed),\n  lburn_sf\n)\n\n\n\n\nCode\n# lets use the nts mapsheet to query the photo centroids to avoid a massive file download\ncol_value &lt;- layers_trimmed$l_imagery_grid |&gt; \n  dplyr::pull(map_tile) \n\nl_photo_centroids &lt;- rfp::rfp_bcd_get_data(\n  bcdata_record_id = \"WHSE_IMAGERY_AND_BASE_MAPS.AIMG_PHOTO_CENTROIDS_SP\",\n  col_filter = \"nts_tile\",\n  col_filter_value = col_value) |&gt; \n  sf::st_transform(4326) \n\n# Apply validation to the AOI and layers\nl_photo_centroids &lt;-lngs_geom_validate(l_photo_centroids)\n\n# clip to aoi - can use  layers_trimmed$aoi \nl_photo_centroids &lt;- sf::st_intersection(l_photo_centroids, aoi)\n\n\nlburn_sf(l_photo_centroids, \"l_photo_centroids\")\n\n\nNext - we read the layers back in. The download step is skipped now unless we turn it on again by changing the update_gis param in our memo yaml header to TRUE.\n\n\nCode\n# now we read in all the sf layers that are local so it is really quick\nlayers_to_load &lt;- fs::dir_ls(\n  fs::path(\n    path_post,\n    \"data\"),\n  glob = \"*.geojson\"\n)\n\nlayers_trimmed &lt;- layers_to_load |&gt;\n  purrr::map(\n    ~ sf::st_read(\n      .x, quiet = TRUE)\n  ) |&gt; \n  purrr::set_names(\n    nm =tools::file_path_sans_ext(\n      basename(\n        names(\n          layers_to_load\n        )\n      )\n    )\n  )"
  },
  {
    "objectID": "posts/2024-11-15-bcdata-ortho-historic/index.html#map-the-area-of-interest",
    "href": "posts/2024-11-15-bcdata-ortho-historic/index.html#map-the-area-of-interest",
    "title": "Getting details of historic orthophoto imagery with R",
    "section": "Map the Area of Interest",
    "text": "Map the Area of Interest\nArea of interest is mapped in Figure 1.\n\n\nCode\nmap &lt;- ggplot2::ggplot() +\n  ggplot2::geom_sf(\n      data = layers_trimmed$aoi_raw,\n      fill = \"transparent\",\n      color = \"black\",\n      linewidth = .5\n  ) +\n  ggplot2::geom_sf(\n      data = layers_trimmed$aoi,\n      fill = \"transparent\",\n      color = \"yellow\",\n      linewidth = .5\n  ) +\n  ggplot2::geom_sf(\n    data = layers_trimmed$l_streams,\n    color = \"blue\",\n    size = 1\n  ) +\n  ggplot2::geom_sf(\n    data = layers_trimmed$l_rail,\n    color = \"black\",\n    size = 1\n  ) +\n  ggplot2::geom_sf(\n    data = layers_trimmed$l_imagery_hist_pnts,\n    color = \"red\",\n    size = 2\n  ) +\n  # ggplot2::geom_sf(\n  #   data = layers_trimmed$l_imagery_hist_poly,\n  #   color = \"red\",\n  #   size = 10\n  # ) +\n  ggplot2::geom_sf(\n    data = layers_trimmed$l_imagery_grid,\n    alpha = 0.25,\n  ) +\n  ggplot2::geom_sf_text(\n    data = layers_trimmed$l_imagery_grid,\n    ggplot2::aes(label = map_tile),\n    size = 3  # Adjust size of the text labels as needed\n  )\n\nmap +\n  ggplot2::geom_sf_text(\n    data = layers_trimmed$l_streams |&gt; dplyr::distinct(gnis_name, .keep_all = TRUE),\n    ggplot2::aes(\n      label = gnis_name\n    ),\n    size = 2  # Adjust size of the text labels as needed\n  ) \n\n\n\n\n\n\n\n\nFigure 1: Area of interest. The buffered watershed used for historic airphoto analysis is shown in yellow."
  },
  {
    "objectID": "posts/2024-11-15-bcdata-ortho-historic/index.html#explore-the-bc-data-catalouge-imagery-layer-options",
    "href": "posts/2024-11-15-bcdata-ortho-historic/index.html#explore-the-bc-data-catalouge-imagery-layer-options",
    "title": "Getting details of historic orthophoto imagery with R",
    "section": "Explore the BC Data Catalouge Imagery Layer Options",
    "text": "Explore the BC Data Catalouge Imagery Layer Options\n\nOrthophoto Tile Polygons\nFor the Orthophoto Tile Polygons Historic Imagery Polygons layer the range of year_operational is 1996, 2013. This is not as far back as we would prefer to be looking.\n\n\nHistoric Imagery Points\nOK, seems we cannot get machine readable historical air photo information from the downloaded from the BC data catalogue Historic Imagery Points layer perhaps because the majority of the photos are not georeferenced? What we see in the map and table below (red dot on map) is one point which contains 8 records including links to pdfs and kmls which are basically a georeferenced drawing of where the imagery overlaps (Table 1 and Figure 2). From as far as I can tell - if we wanted to try to use the kmls or pdfs linked in the attribute tables of the “Historic Imagery Points” layer to select orthoimagery we would need to eyeball where the photo polygons overlap where we want to see imagery for and manually write down identifiers for photo by hand. Maybe I am missing something but it sure seems that way.\n\n\nCode\nmy_caption &lt;- \"The 'airborne-imagery-historical-index-map-points' datset for the area of interest\"\n#This what the information in the [Historic Imagery Points](https://catalogue.data.gov.bc.ca/dataset/airborne-imagery-historical-index-map-points) layer looks like.\n\nlayers_trimmed$l_imagery_hist_pnts |&gt; \n  dplyr::mutate(\n    kml_url = ngr::ngr_str_link_url(\n      url_base = \"https://openmaps.gov.bc.ca/flight_indices/kml/large_scale\", \n      url_resource = \n        fs::path_rel(\n          kml_url, start = \"https://openmaps.gov.bc.ca/flight_indices/kml/large_scale\"\n        )\n    ),\n    pdf_url = ngr::ngr_str_link_url(\n      url_base = \"https://openmaps.gov.bc.ca/flight_indices/pdf\", \n      url_resource = \n        fs::path_rel(pdf_url, start = \"https://openmaps.gov.bc.ca/flight_indices/pdf\")\n    )\n  )|&gt; \n  dplyr::select(-id) |&gt; \n  sf::st_drop_geometry() |&gt; \n  knitr::kable(\n    escape = FALSE,\n    caption = my_caption\n  )\n\n\n\n\nTable 1: The ‘airborne-imagery-historical-index-map-points’ datset for the area of interest\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhistorical_index_map_id\nscale_category\ngeoextent_mapsheet\nmap_tag\nstart_year\nend_year\nkml_url\npdf_url\nobjectid\nse_anno_cad_data\n\n\n\n\n557\nlarge\n093l_e\n093l_e_1\n1950\n1963\nurl_link\nurl_link\n1860\nNA\n\n\n558\nlarge\n093l_e\n093l_e_2\n1971\n1974\nurl_link\nurl_link\n1861\nNA\n\n\n559\nlarge\n093l_e\n093l_e_3\n1975\n1975\nurl_link\nurl_link\n1862\nNA\n\n\n560\nlarge\n093l_e\n093l_e_4\n1980\n1980\nurl_link\nurl_link\n1863\nNA\n\n\n561\nlarge\n093l_e\n093l_e_5\n1980\n1980\nurl_link\nurl_link\n1864\nNA\n\n\n562\nlarge\n093l_e\n093l_e_6\n1981\n1983\nurl_link\nurl_link\n1865\nNA\n\n\n563\nlarge\n093l_e\n093l_e_7\n1989\n1989\nurl_link\nurl_link\n1866\nNA\n\n\n564\nlarge\n093l_e\n093l_e_8\n1990\n1990\nurl_link\nurl_link\n1867\nNA\n\n\n\n\n\n\n\n\n\n\n\nCode\nmy_caption &lt;- \"Screenshot of kml downloaded from link provided in Historic Imagery Points.\"\nknitr::include_graphics(fs::path(\n  path_post,\n  \"fig\",\n  \"Screenshot1\",\n  ext = \"png\"\n  )\n)\n\n\n\n\n\n\n\n\nFigure 2: Screenshot of kml downloaded from link provided in Historic Imagery Points.\n\n\n\n\n\n\n\nHistoric Imagery Polygons\nIt appears we have the same sort of kml/pdf product as we saw in the Historic Imagery Points is being served through the Historic Imagery Polygons layer (Table 2).\n\n\nCode\nmy_caption &lt;- \"The 'airborne-imagery-historical-index-map-points' datset for the area of interest\"\n#This what the information in the [Historic Imagery Points](https://catalogue.data.gov.bc.ca/dataset/airborne-imagery-historical-index-map-points) layer looks like.\n\nlayers_trimmed$l_imagery_hist_poly |&gt; \n  dplyr::mutate(\n    kml_url = ngr::ngr_str_link_url(\n      url_base = \"https://openmaps.gov.bc.ca/flight_indices/kml/large_scale\", \n      url_resource = fs::path(basename(fs::path_dir(kml_url)), basename(kml_url))\n    ),\n    pdf_url = ngr::ngr_str_link_url(\n      url_base = \"https://openmaps.gov.bc.ca/flight_indices/pdf\", \n      url_resource = fs::path(\n        ngr::ngr_str_dir_from_path(pdf_url, levels = 3), \n        ngr::ngr_str_dir_from_path(pdf_url, levels = 2),\n        ngr::ngr_str_dir_from_path(pdf_url, levels = 1),\n        basename(pdf_url))\n    )\n  )|&gt; \n  dplyr::select(-id) |&gt; \n  sf::st_drop_geometry() |&gt; \n  dplyr::arrange(start_year) |&gt; \n  knitr::kable(\n    escape = FALSE,\n  caption = my_caption\n  )\n\n\n\n\nTable 2: The ‘airborne-imagery-historical-index-map-points’ datset for the area of interest\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhistorical_index_map_id\nscale_category\ngeoextent_mapsheet\nmap_tag\nstart_year\nend_year\nkml_url\npdf_url\nfeature_area_sqm\nfeature_length_m\nobjectid\nse_anno_cad_data\n\n\n\n\n866\nmedium\n093l\n093l_1\n1938\n1956\nurl_link\nurl_link\n14421427749\n481668.3\n2183\nNA\n\n\n862\nmedium\n093k\n093k_2\n1946\n1956\nurl_link\nurl_link\n14421427749\n481668.3\n2179\nNA\n\n\n557\nlarge\n093l_e\n093l_e_1\n1950\n1963\nurl_link\nurl_link\n7211433354\n352456.5\n1923\nNA\n\n\n549\nlarge\n093k_w\n093k_w_1\n1950\n1963\nurl_link\nurl_link\n7211433354\n352456.5\n1915\nNA\n\n\n863\nmedium\n093k\n093k_3\n1961\n1966\nurl_link\nurl_link\n14421427749\n481668.3\n2180\nNA\n\n\n867\nmedium\n093l\n093l_2\n1966\n1972\nurl_link\nurl_link\n14421427749\n481668.3\n2184\nNA\n\n\n550\nlarge\n093k_w\n093k_w_2\n1969\n1973\nurl_link\nurl_link\n7211433354\n352456.5\n1916\nNA\n\n\n1047\nsmall\n093nw\n093nw_1\n1970\n1975\nurl_link\nurl_link\n56959075420\n956978.8\n2312\nNA\n\n\n558\nlarge\n093l_e\n093l_e_2\n1971\n1974\nurl_link\nurl_link\n7211433354\n352456.5\n1924\nNA\n\n\n864\nmedium\n093k\n093k_4\n1971\n1971\nurl_link\nurl_link\n14421427749\n481668.3\n2181\nNA\n\n\n559\nlarge\n093l_e\n093l_e_3\n1975\n1975\nurl_link\nurl_link\n7211433354\n352456.5\n1925\nNA\n\n\n551\nlarge\n093k_w\n093k_w_3\n1975\n1980\nurl_link\nurl_link\n7211433354\n352456.5\n1917\nNA\n\n\n865\nmedium\n093k\n093k_5\n1977\n1985\nurl_link\nurl_link\n14421427749\n481668.3\n2182\nNA\n\n\n552\nlarge\n093k_w\n093k_w_4\n1978\n1985\nurl_link\nurl_link\n7211433354\n352456.5\n1918\nNA\n\n\n553\nlarge\n093k_w\n093k_w_5\n1978\n1988\nurl_link\nurl_link\n7211433354\n352456.5\n1919\nNA\n\n\n1048\nsmall\n093nw\n093nw_2\n1979\n1979\nurl_link\nurl_link\n56959075420\n956978.8\n2313\nNA\n\n\n560\nlarge\n093l_e\n093l_e_4\n1980\n1980\nurl_link\nurl_link\n7211433354\n352456.5\n1926\nNA\n\n\n561\nlarge\n093l_e\n093l_e_5\n1980\n1980\nurl_link\nurl_link\n7211433354\n352456.5\n1927\nNA\n\n\n562\nlarge\n093l_e\n093l_e_6\n1981\n1983\nurl_link\nurl_link\n7211433354\n352456.5\n1928\nNA\n\n\n868\nmedium\n093l\n093l_3\n1981\n1984\nurl_link\nurl_link\n14421427749\n481668.3\n2185\nNA\n\n\n1049\nsmall\n093nw\n093nw_3\n1982\n1984\nurl_link\nurl_link\n56959075420\n956978.8\n2314\nNA\n\n\n554\nlarge\n093k_w\n093k_w_6\n1985\n1988\nurl_link\nurl_link\n7211433354\n352456.5\n1920\nNA\n\n\n1050\nsmall\n093nw\n093nw_trim\n1986\n1988\nurl_link\nurl_link\n64069901258\n1020771.3\n2315\nNA\n\n\n563\nlarge\n093l_e\n093l_e_7\n1989\n1989\nurl_link\nurl_link\n7211433354\n352456.5\n1929\nNA\n\n\n555\nlarge\n093k_w\n093k_w_7\n1989\n1989\nurl_link\nurl_link\n7211433354\n352456.5\n1921\nNA\n\n\n556\nlarge\n093k_w\n093k_w_8\n1990\n1990\nurl_link\nurl_link\n7211433354\n352456.5\n1922\nNA\n\n\n564\nlarge\n093l_e\n093l_e_8\n1990\n1990\nurl_link\nurl_link\n7211433354\n352456.5\n1930\nNA\n\n\n\n\n\n\n\n\n\n\nAir Photo Centroids\nEach of the Air Photo Centroids are georeferenced with a date range of:\n\n\nCode\nrange(layers_trimmed$l_photo_centroids$photo_date)\n\n\n[1] \"1963-08-07\" \"2019-09-18\"\n\n\n\nWe visualize column metadata in Table 3 and map the centroids in our study area with Figure 3.\n\n\nCode\n# At this point we have downloaded two csvs (one for each NTS 1:50,000 mapsheet of course) with information about the airphotos including UTM coordinates that we will assume for now are the photo centres. In our next steps we read in what we have, turn into spatial object, trim to overall study area and plot.\n# list csvs\nls &lt;- fs::dir_ls(\n  fs::path(\n    path_post,\n    \"data\"),\n  glob = \"*.csv\"\n)\n\nphotos_raw &lt;- ls |&gt; \n  purrr::map_df(\n    readr::read_csv\n  ) |&gt; \n  sf::st_as_sf(\n    coords = c(\"Longitude\", \"Latitude\"), crs = 4326\n  ) |&gt; \n  janitor::clean_names() |&gt; \n  dplyr::mutate(photo_date = lubridate::mdy(photo_date)) \n\n\nphotos_aoi &lt;- sf::st_intersection(\n  photos_raw, \n  layers_trimmed$aoi |&gt; st_make_valid()\n  )\n\n\n\n\nCode\nbcdata::bcdc_describe_feature(\"WHSE_IMAGERY_AND_BASE_MAPS.AIMG_PHOTO_CENTROIDS_SP\") |&gt; \n  knitr::kable() |&gt; \n  kableExtra::scroll_box(width = \"100%\", height = \"500px\")\n\n\n\n\nTable 3: Metadata for the 'airphoto-centroids' layer in the BC Data Catalouge\n\n\n\n\n\n\n\ncol_name\nsticky\nremote_col_type\nlocal_col_type\ncolumn_comments\n\n\n\n\nid\nTRUE\nxsd:string\ncharacter\nNA\n\n\nAIRP_ID\nTRUE\nxsd:decimal\nnumeric\nAIRP_ID is the unique photo frame identifier, generated by the source APS system.\n\n\nFLYING_HEIGHT\nFALSE\nxsd:decimal\nnumeric\nFLYING_HEIGHT is the flying height above mean sea level in metres.\n\n\nPHOTO_YEAR\nTRUE\nxsd:decimal\nnumeric\nPHOTO_YEAR is the operational year to which this photograph is assigned.\n\n\nPHOTO_DATE\nFALSE\nxsd:date\ndate\nPHOTO_DATE is the date (year, month, and day) of exposure of the photograph.\n\n\nPHOTO_TIME\nFALSE\nxsd:string\ncharacter\nPHOTO_TIME is the time of exposure (hours, minutes, seconds), expressed in Pacific Standard Time, e.g., 9:43:09 AM.\n\n\nLATITUDE\nFALSE\nxsd:decimal\nnumeric\nLATITUDE is the geographic coordinate, in decimal degrees (dd.dddddd), of the location of the feature as measured from the equator, e.g., 55.323653.\n\n\nLONGITUDE\nFALSE\nxsd:decimal\nnumeric\nLONGITUDE is the geographic coordinate, in decimal degrees (dd.dddddd), of the location of the feature as measured from the prime meridian, e.g., -123.093544.\n\n\nFILM_ROLL\nFALSE\nxsd:string\ncharacter\nFILM_ROLL is a BC Government film roll identifier, e.g., bc5624.\n\n\nFRAME_NUMBER\nFALSE\nxsd:decimal\nnumeric\nFRAME_NUMBER is the sequential frame number of this photograph within a film roll.\n\n\nGEOREF_METADATA_IND\nFALSE\nxsd:string\ncharacter\nGEOREF_METADATA_IND indicates if georeferencing metadata exists for this photograph, i.e., Y, N.\n\n\nPUBLISHED_IND\nFALSE\nxsd:string\ncharacter\nPUBLISHED_IND indicates if this photograph's geometry and metadata should be exposed for viewing, i.e., Y,N.\n\n\nMEDIA\nFALSE\nxsd:string\ncharacter\nMEDIA describes the photographic medium on which this photograph was recorded, e.g. Film - BW.\n\n\nPHOTO_TAG\nFALSE\nxsd:string\ncharacter\nPHOTO_TAG is a combination of film roll identifier and frame number that uniquely identifies an air photo, e.g., bcc09012_035.\n\n\nBCGS_TILE\nFALSE\nxsd:string\ncharacter\nBCGS_TILE identifies the BCGS mapsheet within which the centre of this photograph is contained. The BCGW mapsheet could be 1:20,000, 1:10,000 or 1:5,000, e.g., 104a01414.\n\n\nNTS_TILE\nFALSE\nxsd:string\ncharacter\nNTS_TILE identifies the NTS 1:50,000 mapsheet tile within which the centre of this photograph is contained, e.g., 104A03.\n\n\nSCALE\nFALSE\nxsd:string\ncharacter\nSCALE of the photo with respect to ground based on a 9-inch square hardcopy print, e.g., 1:18,000.\n\n\nGROUND_SAMPLE_DISTANCE\nFALSE\nxsd:decimal\nnumeric\nGROUND_SAMPLE_DISTANCE indicates the distance on the ground in centimetres represented by a single pixel in the scanned or original digital version of this photograph.\n\n\nOPERATION_TAG\nFALSE\nxsd:string\ncharacter\nOPERATION_TAG is an alpha numeric shorthand operation identifier representing photographic medium, operation number, requesting agency and operational year of photography, e.g., D003FI15.\n\n\nFOCAL_LENGTH\nFALSE\nxsd:decimal\nnumeric\nFOCAL_LENGTH is the focal length of the lens, in millimetres, used to capture this photograph.\n\n\nTHUMBNAIL_IMAGE_URL\nFALSE\nxsd:string\ncharacter\nTHUMBNAIL_IMAGE_URL is a hyperlink to the 1/16th resolution thumbnail version of this image.\n\n\nFLIGHT_LOG_URL\nFALSE\nxsd:string\ncharacter\nFLIGHT_LOG_URL is a hyperlink to the scanned version of the original handwritten flight log page (film record) for this image.\n\n\nCAMERA_CALIBRATION_URL\nFALSE\nxsd:string\ncharacter\nCAMERA_CALIBRATION_URL is a hyperlink to the camera calibration report file for this image.\n\n\nPATB_GEOREF_URL\nFALSE\nxsd:string\ncharacter\nPATB_GEOREF_URL is a hyperlink to the PatB geo-referencing file for this image.\n\n\nFLIGHT_LINE_SEGMENT_ID\nFALSE\nxsd:decimal\nnumeric\nFLIGHT_LINE_SEGMENT_ID identifies the section of flight line to which this photograph belongs.\n\n\nOPERATION_ID\nFALSE\nxsd:decimal\nnumeric\nOPERATION_ID is a unique identifier for the operation to which this photograph belongs. It may be used by the data custodian to diagnose positional errors.\n\n\nFILM_RECORD_ID\nFALSE\nxsd:decimal\nnumeric\nFILM_RECORD_ID is a unique identifier for the film record to which this photograph belongs. It may be used by the data custodian to diagnose positional errors.\n\n\nSHAPE\nFALSE\ngml:PointPropertyType\nsfc geometry\nSHAPE is the column used to reference the spatial coordinates defining the feature.\n\n\nOBJECTID\nTRUE\nxsd:decimal\nnumeric\nOBJECTID is a column required by spatial layers that interact with ESRI ArcSDE. It is populated with unique values automatically by SDE.\n\n\nSE_ANNO_CAD_DATA\nFALSE\nxsd:hexBinary\nnumeric\nSE_ANNO_CAD_DATA is a binary column used by spatial tools to store annotation, curve features and CAD data when using the SDO_GEOMETRY storage data type.\n\n\n\n\n\n\n\n\n\n\n\nCode\nmap +\n  geom_sf(\n    data = layers_trimmed$l_photo_centroids,\n    alpha = 0.25\n  ) \n\n\n\n\n\n\n\n\nFigure 3: ‘airphoto-centroids’ dataset for the area of interest.\n\n\n\n\n\nThat is a lot of photos! 9388 photos to be exact!!!"
  },
  {
    "objectID": "posts/2024-11-15-bcdata-ortho-historic/index.html#refine-airphoto-centres-by-clipping-with-buffered-streams",
    "href": "posts/2024-11-15-bcdata-ortho-historic/index.html#refine-airphoto-centres-by-clipping-with-buffered-streams",
    "title": "Getting details of historic orthophoto imagery with R",
    "section": "Refine Airphoto Centres by Clipping with Buffered Streams",
    "text": "Refine Airphoto Centres by Clipping with Buffered Streams\nAlthough we are likely moveing on to a different strategy - this section details how we can obtain imagery IDs for photo centres that fall within a pre-determined distance from streams of interest in our study area.\n\n\nCode\n# amount to buffer all stream segments\nq_buffer &lt;- 1500\n# q_drm_main &lt;- 263795\n\n# length of streams other than selected explicity to buffer\nq_drm_other &lt;- 3000\n\n\nHere are our query parameters to narrow down the area within our study are watershed in which we want to find photos for:\n\nBuffer: 1500m - size of buffer used on either side of stream lines selected\nStream segments:\n\nBulkley River (gnis_name in the stream layer)\nMaxan Creek\nBuck Creek\nfor each remaining stream - segments of that stream which begin before 3000m from the downstream system (i.e. the first 3km) of stream.\n\n\n\n\nCode\n# We use the `downstream_route_measure` of the stream layer to exclude areas upstream of Bulkley Lake (also known as Taman Creek).  We find it in QGIS by highlighting the stream layer and clicking on our segment of interest while we have the information tool selected - the resulting pop-up looks like this in QGIS.\nknitr::include_graphics(fs::path(\n  path_post,\n  \"fig\",\n  \"Screenshot2\",\n  ext = \"png\"\n  )\n)\n\n\n\n\nCode\nr_streams &lt;- c(\"Maxan Creek\", \"Buck Creek\")\n\naoi_refined_raw &lt;- layers_trimmed$l_streams |&gt; \n  # removed  & downstream_route_measure &lt; q_drm_main for bulkley as doestn't cahnge 1960s query and increases beyond just by 5 photos\n  dplyr::filter(gnis_name == \"Bulkley River\"|\n                  gnis_name != \"Bulkley River\" & downstream_route_measure &lt; q_drm_other |\n                  gnis_name %in% r_streams) |&gt; \n  # dplyr::arrange(downstream_route_measure) |&gt;\n  # calculate when we get to length_m by adding up the length_metre field and filtering out everything up to length_m\n  # dplyr::filter(cumsum(length_metre) &lt;= length_m) |&gt;\n  sf::st_union() |&gt; \n  # we need to run st_sf or we get a sp object in a list...\n  sf::st_sf()\n  \naoi_refined_buffered &lt;- sf::st_buffer(\n  aoi_refined_raw,\n  q_buffer, endCapStyle = \"FLAT\"\n) \n\nphotos_aoi_refined &lt;- sf::st_intersection(\n  layers_trimmed$l_photo_centroids, \n  aoi_refined_buffered\n  )\n\n\nLet’s plot again and include our buffered areas around the first 3000m of streams (area in red) along with the location of the photo points that land within that area. Looks like this give us 2770 photos.\n\n\nCode\nmap +\n  geom_sf(\n    data = aoi_refined_buffered,\n    color = \"red\",\n    alpha= 0\n  ) +\n  geom_sf(\n    data = photos_aoi_refined,\n    alpha = 0.25,\n  ) +\n  geom_sf_text(\n    data = layers_trimmed$l_streams |&gt; dplyr::distinct(gnis_name, .keep_all = TRUE),\n    aes(\n      label = gnis_name\n    ),\n    size = 2  # Adjust size of the text labels as needed\n  ) \n\n\n\n\n\n\n\n\n\nThat is not as many photos - but still quite a few (2770).\n\n\nCode\n# @fig-dt1 below can be used to filter these photos from any time and/or mapsheet and export the result to csv or excel file.  \n#| label: fig-dt1\n#| tbl-cap: \"All photo centroids located with watershed study area.\"\nphotos_aoi_refined |&gt; \n  dplyr::select(-id) |&gt; \n  my_dt_table(cols_freeze_left = 0)"
  },
  {
    "objectID": "posts/2024-11-15-bcdata-ortho-historic/index.html#filter-photos-by-date",
    "href": "posts/2024-11-15-bcdata-ortho-historic/index.html#filter-photos-by-date",
    "title": "Getting details of historic orthophoto imagery with R",
    "section": "Filter Photos by Date",
    "text": "Filter Photos by Date\nNow lets map by year to see what our options are including the earliest photos possible. Here is our range to choose from:\n\n\nCode\nrange(photos_aoi_refined$photo_date)\n\n\n[1] \"1963-08-07\" \"2019-09-18\"\n\n\n`\n\n\nCode\nmap +\ngeom_sf(\n  data = photos_aoi_refined |&gt; dplyr::filter(photo_year &lt;= \"1975\")\n  ) +\n  facet_wrap(~ photo_year)\n\n\n\n\n\n\n\n\n\nWell - looks like we get really good coverage of the Bulkley River mainstem in 1968 then much better coverage of the Buck Creek drainage and Maxan Creek in 1971. For 1975 - the coverage of the Bulkley mainstem and Maxan Creek is pretty good…\n\nIf we just wanted the areas near the river and we don’t mind mixing years - we grab the photos from:\n\n1968 all\n1971 for the Buck Creek and Maxan Creek areas only\n1975 Maxan Creek only\n\n\n\n\nCode\n# spatially represent just Buck and Maxan, buffer and clip the 1971 photos\n# \"r_\" is for \"refine\"\nr_year1 &lt;- \"1968\"\nr_year2 &lt;- \"1971\"\nr_year3 &lt;- \"1975\"\n\nr_streams2 &lt;- c(\"Maxan Creek\")\n\nl_streams_refined1 &lt;- layers_trimmed$l_streams |&gt; \n  # we defined r_streams in chunk way above \n  dplyr::filter(gnis_name %in% r_streams) |&gt; \n  sf::st_union() |&gt; \n  # we need to run st_sf or we get a sp object in a list...\n  sf::st_sf()\n  \naoi_refined_buffered2 &lt;- sf::st_buffer(\n  l_streams_refined1,\n  q_buffer, endCapStyle = \"FLAT\"\n) \n\nl_streams_refined2 &lt;- layers_trimmed$l_streams |&gt; \n  # we defined r_streams in chunk way above \n  dplyr::filter(gnis_name %in% r_streams2) |&gt; \n  sf::st_union() |&gt; \n  # we need to run st_sf or we get a sp object in a list...\n  sf::st_sf()\n  \naoi_refined_buffered3 &lt;- sf::st_buffer(\n  l_streams_refined2,\n  q_buffer, endCapStyle = \"FLAT\"\n) \n\n# filter first year\nphotos1 &lt;- photos_aoi_refined |&gt; \n  dplyr::filter(\n      photo_year == r_year1\n  )\n\n# filter second year using just the streams we want to include\nphotos2 &lt;- sf::st_intersection(\n  layers_trimmed$l_photo_centroids |&gt; dplyr::filter(photo_year == r_year2), \n  aoi_refined_buffered2\n  )\n\n# filter second year using just the streams we want to include\nphotos3 &lt;- sf::st_intersection(\n  layers_trimmed$l_photo_centroids |&gt; dplyr::filter(photo_year == r_year3), \n  aoi_refined_buffered3\n  )\n\nphotos_all &lt;- dplyr::bind_rows(photos1, photos2, photos3)\n\n\nNow let’s have a look at the individual year components (Figure 4) as well as the whole dataset (Figure 5). We are privileged to potentially have the assistance of Mike Price to help us obtain this imagery from the UBC archives. If there are too many photos to grab as is - the table below can be filtered by photo_year to reduce the number of photos. The resulting filtered dataset can then be downloaded by pressing the CSV or Excel buttons at the bottom of the table….\n\n\nCode\nmy_caption &lt;- \"Amalgamated photo points presented by year.\"\nmap +\n  geom_sf(\n  data = photos_all\n  ) +\n  facet_wrap(~ photo_year)\n\n\n\n\n\n\n\n\nFigure 4: Amalgamated photo points presented by year.\n\n\n\n\n\n\n\nCode\nmy_caption &lt;- \"Amalgamated photo points\"\nmap +\n  geom_sf(\n  data = photos_all\n  ) +\n  geom_sf_text(\n    data = layers_trimmed$l_streams |&gt; dplyr::distinct(gnis_name, .keep_all = TRUE),\n    aes(\n      label = gnis_name\n    ),\n    size = 2  # Adjust size of the text labels as needed\n  ) \n\n\n\n\n\n\n\n\nFigure 5: Amalgamated photo points\n\n\n\n\n\n\nExport csv with Photo Information for Areas Adjacent to Streams\nLet’s burn out a csv that can be used to find the imagery for the 253 photos above.\n\n\nCode\nlfile_name_photos &lt;- function(dat = NULL){\n  fs::path(\n      path_post,\n      \"exports\",\n      paste(\n        \"airphotos\",\n        paste(range(dat$photo_date), collapse = \"_\"),\n        sep = \"_\"\n      ),\n      ext = \"csv\"\n    )\n}\n\nphotos_all |&gt; \n  readr::write_csv(\n    lfile_name_photos(photos_all), na =\"\"\n  )\n\n\nlpath_link &lt;- function(dat = NULL){\n  paste0(\n    \"https://github.com/NewGraphEnvironment/new_graphiti/tree/main/posts/2024-11-15-bcdata-ortho-historic/exports/\",\n    basename(\n      lfile_name_photos(dat)\n    )\n  )\n}\n\n\nWe can view and download exported csv files here but really we are perhaps better off using the widget below to get the csv file we need.\n\n\nCode\nphotos_all |&gt; \n  dplyr::select(-id) |&gt; \n  my_dt_table(cols_freeze_left = 0)"
  },
  {
    "objectID": "posts/2024-11-15-bcdata-ortho-historic/index.html#estimate-polygon-size-based-on-the-scale-and-a-9-x-9-negative",
    "href": "posts/2024-11-15-bcdata-ortho-historic/index.html#estimate-polygon-size-based-on-the-scale-and-a-9-x-9-negative",
    "title": "Getting details of historic orthophoto imagery with R",
    "section": "Estimate Polygon Size based on the Scale and a 9” x 9” Negative",
    "text": "Estimate Polygon Size based on the Scale and a 9” x 9” Negative\nHere we take a shot at deriving the image footprint using the scale and a negative size of 9” X 9” which seems to be what is recorded in the flight logs (haven’t checked every single one yet).\n\n\nCode\n# not accurate!!!!!!!!!!!! this is for the equator!!!\n# Add geometry\nphotos_poly_prep &lt;- layers_trimmed$l_photo_centroids |&gt; \n  dplyr::select(-id) |&gt; \n  sf::st_drop_geometry() |&gt; \n  # Parse scale\n  dplyr::mutate(\n    scale_parsed = as.numeric(stringr::str_remove(scale, \"1:\")),\n    width_m = 9 * scale_parsed * 0.0254,  # Width in meters\n    height_m = 9 * scale_parsed * 0.0254, # Height in meters\n  ) |&gt;\n  # Create geometry\n  dplyr::rowwise() |&gt; \n  dplyr::mutate(\n    geometry = list({\n      # Create polygon corners\n      center &lt;- c(longitude, latitude)\n      width_deg = (width_m / 2) / 111320  # Convert width to degrees (~111.32 km per degree latitude)\n      height_deg = (height_m / 2) / 111320 # Approximate for longitude; accurate near equator\n      \n      # Define corners\n      corners &lt;- matrix(\n        c(\n          center[1] - width_deg, center[2] - height_deg, # Bottom-left\n          center[1] + width_deg, center[2] - height_deg, # Bottom-right\n          center[1] + width_deg, center[2] + height_deg, # Top-right\n          center[1] - width_deg, center[2] + height_deg, # Top-left\n          center[1] - width_deg, center[2] - height_deg  # Close the polygon\n        ),\n        ncol = 2,\n        byrow = TRUE\n      )\n      \n      # Create polygon geometry\n      sf::st_polygon(list(corners))\n    })\n  ) |&gt; \n  dplyr::ungroup() |&gt;\n  # Convert to sf object\n  sf::st_as_sf(sf_column_name = \"geometry\", crs = 4326) \n\n\n\n\nCode\nphotos_poly_prep &lt;- layers_trimmed$l_photo_centroids |&gt; \n  dplyr::select(-id) |&gt; \n  sf::st_transform(crs = 32609) |&gt;  # Transform to UTM Zone 9 for accurate metric calculations\n  # Parse scale and calculate dimensions in meters\n  dplyr::mutate(\n    scale_parsed = as.numeric(stringr::str_remove(scale, \"1:\")),\n    width_m = 9 * scale_parsed * 0.0254,  # Width in meters\n    height_m = 9 * scale_parsed * 0.0254  # Height in meters\n  ) |&gt; \n  # Create geometry using UTM coordinates\n  dplyr::rowwise() |&gt; \n  dplyr::mutate(\n    geometry = list({\n      # Create polygon corners in UTM (meters)\n      center &lt;- sf::st_coordinates(geometry)  # Extract UTM coordinates\n      width_half = width_m / 2\n      height_half = height_m / 2\n      \n      # Define corners in meters\n      corners &lt;- matrix(\n        c(\n          center[1] - width_half, center[2] - height_half, # Bottom-left\n          center[1] + width_half, center[2] - height_half, # Bottom-right\n          center[1] + width_half, center[2] + height_half, # Top-right\n          center[1] - width_half, center[2] + height_half, # Top-left\n          center[1] - width_half, center[2] - height_half  # Close the polygon\n        ),\n        ncol = 2,\n        byrow = TRUE\n      )\n      \n      # Create polygon geometry\n      sf::st_polygon(list(corners))\n    })\n  ) |&gt; \n  dplyr::ungroup() |&gt; \n  # Convert to sf object with UTM Zone 9 CRS\n  sf::st_as_sf(sf_column_name = \"geometry\") |&gt; \n  sf::st_set_crs(32609) |&gt;  # Assign UTM Zone 9 CRS\n  # Transform back to WGS84 (if needed)\n  sf::st_transform(crs = 4326)\n\n\n\n\nCode\n# Assuming `photos_poly` has a `photo_year` column with years of interest\nphotos_poly &lt;- photos_poly_prep |&gt; \n  dplyr::filter(\n    photo_year %in% c(1968, 1971, 1975)\n    )\n\nl_photo_centroids_fltered &lt;- layers_trimmed$l_photo_centroids |&gt; \n      dplyr::filter(\n        photo_year %in% c(1968, 1971, 1975)\n    )\n\nyears &lt;- unique(photos_poly$photo_year)\n\nyears_centroids &lt;- unique(l_photo_centroids_fltered$photo_year)\n\nmap &lt;- my_leaflet()\n\n# Loop through each year and add polygons with the year as a group\nfor (year in years) {\n  map &lt;- map |&gt; \n    leaflet::addPolygons(\n      data = photos_poly |&gt; \n        dplyr::filter(\n          photo_year == year\n          ), \n      color = \"black\", \n      weight = 1, \n      smoothFactor = 0.5,\n      opacity = 1.0, \n      fillOpacity = 0,\n      group = paste0(\"Polygons - \", year)\n    )\n}\n\n# Add centroid layers for each year\nfor (year in years_centroids) {\n  map &lt;- map |&gt; \n    leaflet::addCircleMarkers(\n      data = l_photo_centroids_fltered |&gt; dplyr::filter(photo_year == year),\n      radius = 1,\n      color = \"black\",\n      fillOpacity = 0.7,\n      opacity = 1.0,\n      group = paste0(\"Centroids - \", year)\n    )\n}\n\nall_groups &lt;- c(paste0(\"Polygons - \", years), paste0(\"Centroids - \", years_centroids))\n\n# Add layer control to toggle year groups\nmap &lt;- map |&gt; \n  leaflet::addPolygons(\n    data = layers_trimmed$aoi, \n              color = \"black\", \n              weight = 1, \n              smoothFactor = 0.5,\n              opacity = 1.0, \n              fillOpacity = 0\n              ) |&gt; \n  # leaflet::addPolygons(\n  #   data = layers_trimmed$aoi_raw, \n  #             color = \"yellow\", \n  #             weight = 1, \n  #             smoothFactor = 0.5,\n  #             opacity = 1.0, \n  #             fillOpacity = 0\n  #             ) |&gt; \n  leaflet::addLayersControl(\n    baseGroups = c(\n      \"Esri.DeLorme\",\n      \"ESRI Aerial\"),\n    overlayGroups = all_groups,\n    options = leaflet::layersControlOptions(collapsed = FALSE)\n  ) |&gt; \n  leaflet.extras::addFullscreenControl()\n\nmap\n\n\n\n\n\n\n\n\nFigure 6"
  },
  {
    "objectID": "posts/2024-11-15-bcdata-ortho-historic/index.html#build-a-interactive-dashboard-that-allows-us-to-visualize-and-download-by-a-specific-year",
    "href": "posts/2024-11-15-bcdata-ortho-historic/index.html#build-a-interactive-dashboard-that-allows-us-to-visualize-and-download-by-a-specific-year",
    "title": "Getting details of historic orthophoto imagery with R",
    "section": "Build a Interactive Dashboard that allows us to Visualize and Download by a Specific Year",
    "text": "Build a Interactive Dashboard that allows us to Visualize and Download by a Specific Year\nUse the filter and slider to see the coverage for photos we have for an individual year then export to excel or csv file with the buttons at the bottom of the table below.\n\n\nCode\n# Wrap data frame in SharedData\nsd &lt;- crosstalk::SharedData$new(\n  layers_trimmed$l_photo_centroids |&gt; \n    dplyr::mutate(\n      thumbnail_image_url = ngr::ngr_str_link_url(\n        url_base = \"https://openmaps.gov.bc.ca/thumbs\", \n        url_resource = \n          fs::path_rel(\n            thumbnail_image_url, start = \"https://openmaps.gov.bc.ca/thumbs\"\n          )\n      ),\n      flight_log_url = ngr::ngr_str_link_url(\n        url_base = \"https://openmaps.gov.bc.ca/thumbs/logbooks/\", \n        url_resource = \n          fs::path_rel(flight_log_url, start = \"https://openmaps.gov.bc.ca/thumbs/logbooks/\")\n      )\n    )|&gt; \n    \n    dplyr::select(-id)\n)\n\n\n# Use SharedData like a dataframe with Crosstalk-enabled widgets\nmap3 &lt;- sd |&gt;\n  leaflet::leaflet(height = 500) |&gt; #height=500, width=780\n  leaflet::addProviderTiles(\"Esri.WorldTopoMap\", group = \"Topo\") |&gt;\n  leaflet::addProviderTiles(\"Esri.WorldImagery\", group = \"ESRI Aerial\") |&gt;\n  leaflet::addCircleMarkers(\n    radius = 3,\n    fillColor = \"black\",\n    color= \"#ffffff\",\n    stroke = TRUE,\n    fillOpacity = 1.0,\n    weight = 2,\n    opacity = 1.0\n  ) |&gt;\n  leaflet::addPolylines(data=layers_trimmed$l_streams,\n               opacity=0.75, \n               color = 'blue',\n               fillOpacity = 0.75, \n               weight=2) |&gt; \n    leaflet::addPolygons(\n    data = layers_trimmed$aoi, \n              color = \"black\", \n              weight = 1, \n              smoothFactor = 0.5,\n              opacity = 1.0, \n              fillOpacity = 0\n              ) |&gt; \n  leaflet::addLayersControl(\n    baseGroups = c(\n      \"Esri.DeLorme\",\n      \"ESRI Aerial\"),\n    options = leaflet::layersControlOptions(collapsed = F)) |&gt; \n  leaflet.extras::addFullscreenControl(position = \"bottomright\") |&gt; \n  leaflet::addScaleBar(position = \"bottomleft\")\n\n\nwidgets &lt;- crosstalk::bscols(\n  widths = c(3, 9),\n  crosstalk::filter_checkbox(\n    id = \"label\",\n    label =  \"Media Type\",\n    sharedData = sd,\n    group =  ~media\n  ),\n  crosstalk::filter_slider(\n    id = \"year_slider\",\n    label = \"Year\",\n    sharedData = sd,\n    column = ~photo_year,\n    round = 0\n  )\n)\n\nhtmltools::browsable(\n  htmltools::tagList(\n    widgets,\n    map3,\n    sd |&gt; my_dt_table(page_length = 5, escape = FALSE)\n  )\n)\n\n\n\n\n\n\nMedia Type\n\n\n\n\nDigital - Colour\n\n\n\n\n\nFilm - BW\n\n\n\n\n\nFilm - Colour\n\n\n\n\n\n\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\nHow many different flight log records are there?\n\n156\n\nWhat are the unique values of scale reported?\n\n1:10000, 1:12000, 1:15000, 1:15840, 1:16000, 1:18000, 1:20000, 1:30000, 1:31680, 1:40000, 1:50000, 1:60000, 1:70000, 1:8000, 9600"
  },
  {
    "objectID": "posts/2024-11-15-bcdata-ortho-historic/index.html#thinkgs-to-do",
    "href": "posts/2024-11-15-bcdata-ortho-historic/index.html#thinkgs-to-do",
    "title": "Getting details of historic orthophoto imagery with R",
    "section": "Thinkgs to do",
    "text": "Thinkgs to do\nAccording to Figure 2 some images have been georeferenced. However, using a kml with basically a picture drawn on it showing what appears to be the airp_id seems like a very difficult task. Guessing there is a decent chance that there is a digital file somewhere within the gov that details which airp_ids are on which kml/pdf and we could use that to remove photos from our chosen year from ones we wish to aquire and georeference."
  },
  {
    "objectID": "posts/2026-01-08-stac-ortho-mosaics/index.html",
    "href": "posts/2026-01-08-stac-ortho-mosaics/index.html",
    "title": "Sentinel-2 Ortho Mosaics with STAC and gdalcubes",
    "section": "",
    "text": "Tracking how land cover and stream morphology changes over time helps us understand ecosystem health and guides conservation and restoration work. These changes are particularly relevant for aquatic system health in floodplain and riparian areas. Satellite imagery makes this tracking possible at scales that would be impossible to survey on foot. Although aerial imagery from airplanes is higher resolution and better for detailed work, satellite imagery is free and often easily available. Sentinel-2 data only goes back to 2015, and before that most available products are lower resolution (30m rather than 10m).\nHere we generate annual orthomosaic composites from Sentinel-2 imagery using STAC (Simoes et al. 2021) and gdalcubes (Appel and Pebesma 2019). We query Microsoft’s Planetary Computer for cloud-free summer imagery from 2016 to 2025, create median composites for each year, and visualize the results in an interactive leaflet map.\nCode\nlibrary(gdalcubes)\nlibrary(rstac)\nlibrary(terra)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(fs)\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(here)\nlibrary(magick)\nCode\npath_post &lt;- fs::path(\n here::here(),\n \"posts\",\n params$post_name\n)\n\ndir_data &lt;- fs::path(path_post, \"data\")\n\n# Ensure data directory exists\nfs::dir_create(dir_data)"
  },
  {
    "objectID": "posts/2026-01-08-stac-ortho-mosaics/index.html#define-area-of-interest",
    "href": "posts/2026-01-08-stac-ortho-mosaics/index.html#define-area-of-interest",
    "title": "Sentinel-2 Ortho Mosaics with STAC and gdalcubes",
    "section": "Define Area of Interest",
    "text": "Define Area of Interest\nWe define a bounding box for our area of interest.\n\n\nCode\nbbox &lt;- c(\n xmin = -126.17545256019142,\n ymin =  54.36161045287439,\n xmax = -126.12615394008702,\n ymax =  54.38908432381547\n)\n\n# Transform bbox to UTM for the cube view\nbbox_sf &lt;- sf::st_bbox(bbox, crs = 4326) |&gt;\n sf::st_as_sfc() |&gt;\n sf::st_transform(32609) |&gt;\n sf::st_bbox()"
  },
  {
    "objectID": "posts/2026-01-08-stac-ortho-mosaics/index.html#generate-annual-mosaics",
    "href": "posts/2026-01-08-stac-ortho-mosaics/index.html#generate-annual-mosaics",
    "title": "Sentinel-2 Ortho Mosaics with STAC and gdalcubes",
    "section": "Generate Annual Mosaics",
    "text": "Generate Annual Mosaics\nWe query Sentinel-2 L2A imagery from Microsoft’s Planetary Computer for each year from 2016 to 2025. Summer months (June-July) are selected with cloud cover &lt;= 20%. The gdalcubes package creates median composites which are saved as NetCDF files.\n\n\nCode\nyears &lt;- 2016:2025\n\n# Function to generate mosaic for a single year\ngenerate_annual_mosaic &lt;- function(year, bbox, bbox_sf, dir_data) {\n\n message(paste(\"Processing year:\", year))\n\n date_start &lt;- paste0(year, \"-06-01\")\n date_end &lt;- paste0(year, \"-07-31\")\n datetime_range &lt;- paste(date_start, date_end, sep = \"/\")\n\n # Query STAC\n items &lt;- tryCatch({\n   rstac::stac(\"https://planetarycomputer.microsoft.com/api/stac/v1\") |&gt;\n     rstac::stac_search(\n       collections = \"sentinel-2-l2a\",\n       bbox = bbox,\n       datetime = datetime_range\n     ) |&gt;\n     rstac::ext_filter(`eo:cloud_cover` &lt;= 20) |&gt;\n     rstac::post_request() |&gt;\n     rstac::items_sign(rstac::sign_planetary_computer())\n }, error = function(e) {\n   message(paste(\"Error querying STAC for year\", year, \":\", e$message))\n   return(NULL)\n })\n\n # Check if we got any items\n if (is.null(items) || length(items$features) == 0) {\n   message(paste(\"No imagery found for year:\", year))\n   return(NULL)\n }\n\n message(paste(\"Found\", length(items$features), \"scenes for year\", year))\n\n # Create image collection\n col &lt;- gdalcubes::stac_image_collection(\n   items$features,\n   asset_names = c(\"B04\", \"B03\", \"B02\")\n )\n\n # Define cube view\n v &lt;- gdalcubes::cube_view(\n   srs = \"EPSG:32609\",\n   extent = list(\n     left = bbox_sf[\"xmin\"],\n     right = bbox_sf[\"xmax\"],\n     bottom = bbox_sf[\"ymin\"],\n     top = bbox_sf[\"ymax\"],\n     t0 = date_start,\n     t1 = date_end\n   ),\n   dx = 10, dy = 10,\n   dt = \"P2M\",\n   aggregation = \"median\",\n   resampling = \"near\"\n )\n\n # Create cube and write to NetCDF\n cube &lt;- gdalcubes::raster_cube(col, v)\n\n nc_file &lt;- fs::path(dir_data, paste0(\"s2_\", year, \".nc\"))\n gdalcubes::write_ncdf(cube, nc_file)\n\n message(paste(\"Saved:\", nc_file))\n return(nc_file)\n}\n\n# Generate mosaics for all years\nmosaic_files &lt;- purrr::map(\n years,\n ~generate_annual_mosaic(.x, bbox, bbox_sf, dir_data)\n)\n\nnames(mosaic_files) &lt;- years\n\n\n\n\nCode\n# Create thumbnail image from 2016 mosaic\nr_2016 &lt;- terra::rast(fs::path(dir_data, \"s2_2016.nc\"))\n\n# Save as JPG for post thumbnail\njpeg(fs::path(path_post, \"image.jpg\"), width = 800, height = 600, quality = 90)\nterra::plotRGB(r_2016, r = 3, g = 2, b = 1, stretch = \"hist\", axes = FALSE, mar = 0)\ndev.off()\n\n\nquartz_off_screen \n                2"
  },
  {
    "objectID": "posts/2026-01-08-stac-ortho-mosaics/index.html#load-existing-mosaics",
    "href": "posts/2026-01-08-stac-ortho-mosaics/index.html#load-existing-mosaics",
    "title": "Sentinel-2 Ortho Mosaics with STAC and gdalcubes",
    "section": "Load Existing Mosaics",
    "text": "Load Existing Mosaics\nWhen update_gis is FALSE, we load the previously generated NetCDF files.\n\n\nCode\n# List all NetCDF files in data directory\nnc_files &lt;- fs::dir_ls(dir_data, glob = \"*.nc\")\n\n# Extract years from filenames and sort\nnc_years &lt;- gsub(\".*s2_(\\\\d{4})\\\\.nc\", \"\\\\1\", nc_files) |&gt;\n as.numeric() |&gt;\n sort()\n\n# Load rasters\nrasters &lt;- purrr::map(nc_files, terra::rast) |&gt;\n purrr::set_names(nc_years)\n\nmessage(paste(\"Loaded\", length(rasters), \"annual mosaics\"))"
  },
  {
    "objectID": "posts/2026-01-08-stac-ortho-mosaics/index.html#initial-rgb-plot",
    "href": "posts/2026-01-08-stac-ortho-mosaics/index.html#initial-rgb-plot",
    "title": "Sentinel-2 Ortho Mosaics with STAC and gdalcubes",
    "section": "Initial RGB Plot",
    "text": "Initial RGB Plot\nHere we display the most recent year’s composite as a static RGB image.\n\n\nCode\n# Get the most recent year available\nlatest_year &lt;- max(nc_years)\nr_latest &lt;- rasters[[as.character(latest_year)]]\n\n# Check structure\nmessage(paste(\"Latest year:\", latest_year))\nmessage(paste(\"Number of layers:\", terra::nlyr(r_latest)))\n\n# Plot RGB with histogram stretch\nterra::plotRGB(r_latest, r = 3, g = 2, b = 1, stretch = \"lin\",\n              main = paste(\"Sentinel-2 RGB Composite -\", latest_year))\n\n\n\n\n\n\n\n\nFigure 1: Most recent Sentinel-2 RGB composite (median of summer months)."
  },
  {
    "objectID": "posts/2026-01-08-stac-ortho-mosaics/index.html#interactive-leaflet-map",
    "href": "posts/2026-01-08-stac-ortho-mosaics/index.html#interactive-leaflet-map",
    "title": "Sentinel-2 Ortho Mosaics with STAC and gdalcubes",
    "section": "Interactive Leaflet Map",
    "text": "Interactive Leaflet Map\nDisplay all annual mosaics as selectable layers in a leaflet map.\n\n\nCode\n# Initialize leaflet map\nmap &lt;- leaflet::leaflet() |&gt;\n leaflet::addProviderTiles(\"Esri.WorldTopoMap\", group = \"Topo\") |&gt;\n leaflet::addProviderTiles(\"Esri.WorldImagery\", group = \"ESRI Aerial\") |&gt;\n leaflet::setView(\n   lng = mean(bbox[c(\"xmin\", \"xmax\")]),\n   lat = mean(bbox[c(\"ymin\", \"ymax\")]),\n   zoom = 14\n )\n\n# Add each year's raster as a layer\nfor (year in nc_years) {\n r &lt;- rasters[[as.character(year)]]\n\n # Add RGB raster layer using leafem\n map &lt;- map |&gt;\n   leafem::addRasterRGB(\n     r,\n     r = 3, g = 2, b = 1,\n     quantiles = c(0.02, 0.98),\n     group = as.character(year)\n   )\n}\n\n# Add layer control\nmap &lt;- map |&gt;\n leaflet::addLayersControl(\n   baseGroups = c(\"Topo\", \"ESRI Aerial\"),\n   overlayGroups = as.character(nc_years),\n   options = leaflet::layersControlOptions(collapsed = FALSE)\n ) |&gt;\n # Hide all years except the most recent by default\n leaflet::hideGroup(as.character(nc_years[nc_years != latest_year])) |&gt;\n leaflet.extras::addFullscreenControl()\n\nmap\n\n\n\n\n\n\n\n\nFigure 2: Interactive map with annual Sentinel-2 composites from 2016-2025."
  },
  {
    "objectID": "posts/2026-01-08-stac-ortho-mosaics/index.html#time-series-animation",
    "href": "posts/2026-01-08-stac-ortho-mosaics/index.html#time-series-animation",
    "title": "Sentinel-2 Ortho Mosaics with STAC and gdalcubes",
    "section": "Time Series Animation",
    "text": "Time Series Animation\nCycling through each year shows how the landscape has changed over almost a decade.\n\n\nCode\n# Create temporary PNG for each year\ntmp_dir &lt;- tempdir()\n\nimg_files &lt;- purrr::map_chr(nc_years, function(year) {\n  r &lt;- rasters[[as.character(year)]]\n  tmp_file &lt;- fs::path(tmp_dir, paste0(\"frame_\", year, \".png\"))\n\n  png(tmp_file, width = 800, height = 600)\n  terra::plotRGB(r, r = 3, g = 2, b = 1, stretch = \"lin\", axes = FALSE, mar = 0)\n  dev.off()\n\n  tmp_file\n})\n\n# Read images and add year label to each frame\nframes &lt;- purrr::map(seq_along(img_files), function(i) {\n  magick::image_read(img_files[i]) |&gt;\n    magick::image_annotate(\n      nc_years[i],\n      size = 40,\n      color = \"white\",\n      strokecolor = \"black\",\n      gravity = \"south\",\n      location = \"+0+20\"\n    )\n}) |&gt;\n  magick::image_join()\n\nanimation &lt;- magick::image_animate(frames, fps = 1, dispose = \"previous\")\n\n# Save gif\ngif_path &lt;- fs::path(path_post, \"time_series.gif\")\nmagick::image_write(animation, gif_path)\n\n# Display\nanimation\n\n\n\n\n\n\n\n\nFigure 3: Animated time series of Sentinel-2 composites from 2016-2025."
  },
  {
    "objectID": "posts/2026-01-08-stac-ortho-mosaics/index.html#summary",
    "href": "posts/2026-01-08-stac-ortho-mosaics/index.html#summary",
    "title": "Sentinel-2 Ortho Mosaics with STAC and gdalcubes",
    "section": "Summary",
    "text": "Summary\nThis workflow demonstrates how to:\n\nQuery Sentinel-2 imagery from Microsoft’s Planetary Computer using STAC\nCreate cloud-free median composites using gdalcubes\nVisualize multi-temporal imagery in an interactive leaflet map\nGenerate an animated time series GIF to show change over time\n\nTo regenerate the mosaics (e.g., to add new years or change parameters), set update_gis: TRUE in the YAML header and re-render the document."
  },
  {
    "objectID": "posts/2024-06-19-precipitation/index.html",
    "href": "posts/2024-06-19-precipitation/index.html",
    "title": "Mapping and Plotting Precipitation with R",
    "section": "",
    "text": "Really interested in quantifying and visualizing weather data for specific areas that we are working…. Here is a first start.\n\nWARNING - this work is stolen!! I have adapted this from a repository on GitHub from the wonderfully talented Milos Popovic. All credit to Milos! What a boss - really awesome stuff.\n\nAlso of note is the image used for the blog. That is Cotey Gallagher… I hope she doesn’t sue me. https://www.linkedin.com/pulse/how-crazy-would-could-really-rain-cats-dogs-cotey-gallagher/\n\nFirst thing we will do is load our packages. If you do not have the packages installed yet you can change the update_pkgs param in the yml of this file to TRUE. Using pak is great because it allows you to update your packages when you want to.\n\n\nCode\npkgs_cran &lt;- c(\n  \"here\",\n  \"fs\",\n  \"pRecipe\",\n  \"giscoR\",\n  \"terra\",\n  \"tidyverse\",\n  \"rayshader\",\n  \"sf\",\n  \"classInt\",\n  \"rgl\")\n\npkgs_gh &lt;- c(\n  \"poissonconsulting/pgfeatureserv\",\n  \"poissonconsulting/fwapgr\",\n  \"NewGraphEnvironment/rfp\"\n  # we will turn this off since the function it uses won't run for folks without db credentials\n  # \"NewGraphEnvironment/fpr\"\n )\n\npkgs &lt;- c(pkgs_cran, pkgs_gh)\n\nif(params$update_pkgs){\n  # install the pkgs\n  lapply(pkgs,\n         pak::pkg_install,\n         ask = FALSE)\n}\n\n# load the pkgs\npkgs_ld &lt;- c(pkgs_cran,\n             basename(pkgs_gh))\ninvisible(\n  lapply(pkgs_ld,\n       require,\n       character.only = TRUE)\n)\n\n\n\nDefine our Area of Interest\nHere we diverge a bit from Milos version as we are going to load a custom area of interest. We will be connecting to our remote database using Poisson Consulting’s fwapgr::fwa_watershed_at_measure function which leverages the in database FWA_WatershedAtMeasure function from Simon Norris’ wonderful fwapg package.\n\nWe use a blue line key and a downstream route measure to define our area of interest which is the Neexdzii Kwa (a.k.a Upper Bulkley River) near Houston, British Columbia.\n\nAs per the Freshwater Atlas of BC - the blue line key:\n\nUniquely identifies a single flow line such that a main channel and a secondary channel with the same watershed code would have different blue line keys (the Fraser River and all side channels have different blue line keys).\n\n\nA downstream route measure is:\n\nThe distance, in meters, along the route from the mouth of the route to the feature. This distance is measured from the mouth of the containing route to the downstream end of the feature.\n\n\n\nCode\n# lets build a custom watersehed just for upstream of the confluence of Neexdzii Kwa and Wetzin Kwa\n# blueline key\nblk &lt;- 360873822\n# downstream route measure\ndrm &lt;- 166030.4\n\naoi &lt;- fwapgr::fwa_watershed_at_measure(blue_line_key = blk, \n                                        downstream_route_measure = drm) |&gt; \n  sf::st_transform(4326)\n\n\n\n\nRetrieve the Precipitation Data\nFor this example we will retrieve our precipitation data from Multi-Source Weighted-Ensemble Precipitation using the pRecipe package.\n\n\nCode\n# let's create our data directory\ndir_data &lt;- here::here('posts', params$post_dir_name, \"data\")\n\nfs::dir_create(dir_data)\n\n\nTo actually download the data we are going to put a chunk option that allows us to just execute the code once and update it with the update_gis param in our yml. We will use usethis::use_git_ignore to add the data to our .gitignore file so that we do not commit that insano enourmouse file to our git repository.\n\n\nCode\npRecipe::download_data(\n    dataset = \"mswep\",\n    path = dir_data,\n    domain = \"raw\",\n    timestep = \"yearly\"\n)\n\nusethis::use_git_ignore(paste0('posts/', params$post_dir_name, \"/data/*\"))\n\n\nNow we read in our freshly downloaded .nc file and clip to our area of interest.\n\n\nCode\n# get the name of the file with a .nc at the end\nnc_file &lt;- fs::dir_ls(dir_data, glob = \"*.nc\")\n\nmswep_data &lt;- terra::rast(\n  nc_file\n) |&gt;\nterra::crop(\n    aoi\n)\n\n\nNext we extract the years of the data from the filename of the .nc file and then transform the data into a dataframe. We need to remove the data from 2023 because it is only for January as per the filename:\nmswep_tp_mm_global_197902_202301_025_yearly.nc\n\n\nCode\n# the names of the datasets are arbitrary (precipitation_1:precipitation_45) \n# we will rename the datasets to the years.  \n# here we extract 2023 from the nc_file name of the file using regex\nyear_end &lt;- as.numeric(stringr::str_extract(basename(nc_file), \"(?&lt;=_\\\\d{6}_)\\\\d{4}\"))\nyear_start &lt;- as.numeric(stringr::str_extract(basename(nc_file), \"(?&lt;=_)[0-9]{4}(?=[0-9]{2}_[0-9]{6}_)\"))\n\n# assign the names to replace \nnames(mswep_data) &lt;- year_start:year_end\n\n\nmswep_df &lt;- mswep_data |&gt;\n    as.data.frame(xy = TRUE) |&gt;\n    tidyr::pivot_longer(\n        !c(\"x\", \"y\"),\n        names_to = \"year\",\n        values_to = \"precipitation\"\n    ) |&gt; \n  # 2023 is not complete so we remove it\n    dplyr::filter(year != 2023)\n\n\n\n\nGet Additional Data\nWe could use some data for context such as major streams, highways and the railway. We get the streams and railway from data distribution bc api using the bcdata package. Our rfp package calls just allow some extra sanity checks on the bcdata::bcdc_query_geodata function. It’s not really necessary but can be helpful when errors occur (ex. the name of the column to filter on is input incorrectly).\n\n\n\nCode\n# grab all the railways\nl_rail &lt;- rfp::rfp_bcd_get_data(\n    bcdata_record_id = stringr::str_to_upper(\"whse_basemapping.gba_railway_tracks_sp\")\n) |&gt; \n  sf::st_transform(4326) |&gt; \n  janitor::clean_names() \n\n# streams in the bulkley and then filter to just keep the big ones\nl_streams &lt;- rfp::rfp_bcd_get_data(\n  bcdata_record_id = stringr::str_to_upper(\"whse_basemapping.fwa_stream_networks_sp\"),\n  col_filter = stringr::str_to_upper(\"watershed_group_code\"),\n  col_filter_value = \"BULK\",\n  # grab a smaller object by including less columns\n  col_extract = stringr::str_to_upper(c(\"linear_feature_id\", \"stream_order\"))\n) |&gt; \n  sf::st_transform(4326) |&gt; \n  janitor::clean_names() |&gt; \n  dplyr::filter(stream_order &gt; 4)\n\n\nBecause the highways we use in our mapping are not available for direct download from the Data Distribution BC api (some other versions are here we will query them from our remote database. The function used (fpr::fpr_db_query) is a wrapper around the DBI::dbGetQuery function that allows us to query our remote database by calling our environmental variables and making a connection. This will not work without the proper credentials so if you were trying to reproduce this and don’t have the credentials you won’t be able to retrieve the roads. To get around this we have stored the trimmed roads data in the data directory of this post so we can read it in from there.\n\n\nCode\n# highways\n# define the type of roads we want to include using the transport_line_type_code. We will include RA1 and RH1 (Road arerial/highway major)\nrd_codes &lt;- c(\"RA1\", \"RH1\")\nl_rds &lt;- fpr::fpr_db_query(\n  query = glue::glue(\"SELECT transport_line_id, structured_name_1, transport_line_type_code, geom FROM whse_basemapping.transport_line WHERE transport_line_type_code IN ({glue::glue_collapse(glue::single_quote(rd_codes), sep = ', ')})\")\n  )|&gt; \n  sf::st_transform(4326) \n\n\nsf::st_intersection(l_rds, \n                    # we will remove all the aoi columns except the geometry so we don't get all the aoi columns appended\n                    aoi |&gt; dplyr::select(geometry)) |&gt; \n  sf::st_write(here::here('posts', params$post_dir_name, \"data\", \"l_rds.gpkg\"), delete_dsn = TRUE)\n\n\nNow we trim up all those layers. We have some functions to validate and repair geometries and then we clip them to our area of interest.\n\n\nCode\n# we don't actually need to trim the rds since we already did that but for simplicity we will do it again\n  l_rds &lt;- sf::st_read(here::here('posts', params$post_dir_name, \"data\", \"l_rds.gpkg\"), quiet = TRUE) \n\n\nlayers_to_trim &lt;- tibble::lst(l_rail, l_streams, l_rds)\n\n# Function to validate and repair geometries\nvalidate_geometries &lt;- function(layer) {\n  layer &lt;- sf::st_make_valid(layer)\n  layer &lt;- layer[sf::st_is_valid(layer), ]\n  return(layer)\n}\n\n# Apply validation to the AOI and layers\naoi &lt;- validate_geometries(aoi)\nlayers_to_trim &lt;- purrr::map(layers_to_trim, validate_geometries)\n\n# clip them  with purrr and sf\nlayers_trimmed &lt;- purrr::map(\n  layers_to_trim,\n  ~ sf::st_intersection(.x, aoi)\n) \n\n\n\n\nPlot the Precipitation Data by Year\nFirst thing we do here is highjack the plot function from Milos.\n\n\nCode\ntheme_for_the_win &lt;- function(){\n    theme_minimal() +\n    theme(\n        axis.line = element_blank(),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        legend.position = \"right\",\n        legend.title = element_text(\n            size = 11, color = \"grey10\"\n        ),\n        legend.text = element_text(\n            size = 10, color = \"grey10\"\n        ),\n        panel.grid.major = element_line(\n            color = NA\n        ),\n        panel.grid.minor = element_line(\n            color = NA\n        ),\n        plot.background = element_rect(\n            fill = NA, color = NA\n        ),\n        legend.background = element_rect(\n            fill = \"white\", color = NA\n        ),\n        panel.border = element_rect(\n            fill = NA, color = NA\n        ),\n        plot.margin = unit(\n            c(\n                t = 0, r = 0,\n                b = 0, l = 0\n            ), \"lines\"\n        )\n    )\n}\n\nbreaks &lt;- classInt::classIntervals(\n    mswep_df$precipitation,\n    n = 5,\n    style = \"equal\"\n)$brks\n\ncolors &lt;- hcl.colors(\n    n = length(breaks),\n    palette = \"Temps\",\n    rev = TRUE\n)\n\n\nNow we plot the data by year.\n\n\nCode\nmap1 &lt;- ggplot(\n    data = mswep_df\n) +\ngeom_raster(\n    aes(\n        x = x,\n        y = y,\n        fill = precipitation\n    )\n) +\ngeom_contour(\n    aes(\n       x = x,\n       y = y,\n       z = precipitation\n    ), color = \"white\" # add this line\n) +\ngeom_sf(\n    data = aoi,\n    fill = \"transparent\",\n    color = \"grey10\",\n    size = .5\n) +\nscale_fill_gradientn(\n    name = \"mm\",\n    colors = colors,\n    breaks = breaks,\n    labels = round(breaks, 0), # use round(breaks, 0)\n    limits = c(\n        min(mswep_df$precipitation),\n        max(mswep_df$precipitation)\n    )\n) +\nfacet_wrap(~year) +\nguides(\n    fill = guide_colourbar(\n        direction = \"vertical\",\n        barheight = unit(50, \"mm\"),\n        barwidth = unit(5, \"mm\"),\n        title.position = \"top\",\n        label.position = \"right\",\n        title.hjust = .5,\n        label.hjust = .5,\n        ncol = 1,\n        byrow = FALSE\n    )\n) +\ntheme_for_the_win()\n\nmap1\n\n\n\n\n\n\n\n\n\nPretty cool. Interesting to see really wet and dry years in the last 20 years or so such as the wet ones in 2004, 2007, 2011 and 2020 and the dry ones in 2000, 2010, 2014, 2021 and 2022. The contours on the maps are really interesting as they show the gradients which generally run west to east - but occasionally run south to north.\n\n\nAverage Precipitation\nNow we will average all the years together to get an average precipitation map. We will add our additional layers for context too. Roads are black, railways are yellow and streams are blue.\n\n\nCode\nmswep_average_df &lt;- mswep_df |&gt;\n  dplyr::group_by(\n    x, y, .drop = FALSE\n  ) |&gt;\n  dplyr::summarise(\n    precipitation = mean(precipitation)\n  ) |&gt; \n  dplyr::mutate(year = \"average\")\n\n\nbreaks &lt;- classInt::classIntervals(\n  mswep_average_df$precipitation,\n  n = 5,\n  style = \"equal\"\n)$brks\n\n\ncolors &lt;- hcl.colors(\n  n = length(breaks),\n  palette = \"Temps\",\n  rev = TRUE\n)\n\nmap_average &lt;- ggplot(\n  data = mswep_average_df\n) +\n  geom_raster(\n    aes(\n      x = x,\n      y = y,\n      fill = precipitation\n    )\n  ) +\n  geom_contour(\n    aes(\n      x = x,\n      y = y,\n      z = precipitation\n    ), color = \"white\" # add this line\n  ) +\n  geom_sf(\n    data = aoi,\n    fill = \"transparent\",\n    color = \"grey10\",\n    size = .5\n  ) +\n  scale_fill_gradientn(\n    name = \"mm\",\n    colors = colors,\n    breaks = breaks,\n    labels = round(breaks, 0), # use round(breaks, 0)\n    limits = c(\n      min(mswep_average_df$precipitation),\n      max(mswep_average_df$precipitation)\n    )\n  ) +\n  guides(\n    fill = guide_colourbar(\n      direction = \"vertical\",\n      barheight = unit(50, \"mm\"),\n      barwidth = unit(5, \"mm\"),\n      title.position = \"top\",\n      label.position = \"right\",\n      title.hjust = .5,\n      label.hjust = .5,\n      ncol = 1,\n      byrow = FALSE\n    )\n  ) +\n  geom_sf(\n    data = layers_trimmed$l_rds,\n    color = \"black\",\n    size = .8\n  ) +\n  geom_sf(\n    data = layers_trimmed$l_streams,\n    color = \"blue\",\n    size = .8\n  ) +\n  geom_sf(\n    data = layers_trimmed$l_rail,\n    color = \"yellow\",\n    size = .8\n  ) +\n  theme_for_the_win()\n\nmap_average\n\n\n\n\n\n\n\n\n\n\n\nCompare the Average Precipitation to a Specific Year\nWe often talk about a “dry” year or a “wet” year. Let’s compare the average precipitation to a specific year. We will build a little function to do this so that we can easily compare any year to the average.\n\n\nCode\nmap_vs_average &lt;- function(year_compare){\n  \n  mswep_df_2022 &lt;- mswep_df |&gt;\n    dplyr::filter(year == year_compare) |&gt; \n    dplyr::mutate(year = as.character(year))\n  \n  mswep_df_compare &lt;- bind_rows(mswep_average_df, mswep_df_2022)\n  \n  breaks &lt;- classInt::classIntervals(\n    mswep_df_compare$precipitation,\n    n = 5,\n    style = \"equal\"\n  )$brks\n  \n  colors &lt;- hcl.colors(\n    n = length(breaks),\n    palette = \"Temps\",\n    rev = TRUE\n  )\n  \n  ggplot(\n    data = mswep_df_compare\n  ) +\n    facet_wrap(~year) +\n    geom_raster(\n      aes(\n        x = x,\n        y = y,\n        fill = precipitation\n      )\n    ) +\n    geom_contour(\n      aes(\n        x = x,\n        y = y,\n        z = precipitation\n      ), color = \"white\" # add this line\n    ) +\n    geom_sf(\n      data = aoi,\n      fill = \"transparent\",\n      color = \"grey10\",\n      size = .5\n    ) +\n    scale_fill_gradientn(\n      name = \"mm\",\n      colors = colors,\n      breaks = breaks,\n      labels = round(breaks, 0), # use round(breaks, 0)\n      limits = c(\n        min(mswep_df_compare$precipitation),\n        max(mswep_df_compare$precipitation)\n      )\n    ) +\n    guides(\n      fill = guide_colourbar(\n        direction = \"vertical\",\n        barheight = unit(50, \"mm\"),\n        barwidth = unit(5, \"mm\"),\n        title.position = \"top\",\n        label.position = \"right\",\n        title.hjust = .5,\n        label.hjust = .5,\n        ncol = 1,\n        byrow = FALSE\n      )\n    ) +\n    geom_sf(\n      data = layers_trimmed$l_rds,\n      color = \"black\",\n      size = .8\n    ) +\n    geom_sf(\n      data = layers_trimmed$l_streams,\n      color = \"blue\",\n      size = .8\n    ) +\n    geom_sf(\n      data = layers_trimmed$l_rail,\n      color = \"yellow\",\n      size = .8\n    ) +\n    theme_for_the_win()\n}\n\n\nFirst let’s check out 2022 as it seemed pretty dry.\n\n\nCode\nmap_vs_average(2022)\n\n\n\n\n\n\n\n\n\nNow let’s look at 2020 as that seemed wet with lots of streams flowing really nicely.\n\n\nCode\nmap_vs_average(2020)\n\n\n\n\n\n\n\n\n\nDefinitely wetter than average.\n\n\n3D Contour Map\nLet’s make an interactive 3D contour map of the average precipitation data. Use the mouse to rotate the map and zoom in and out!\n\n\nCode\n{\n  \n  rayshader::plot_gg(\n    ggobj = map_average,\n    width = 7,\n    height = 7,\n    scale = 250,\n    solid = FALSE,\n    shadow = TRUE,\n    shadowcolor = \"white\",\n    shadowwidth = 0,\n    shadow_intensity = 1,\n    # window.size = c(600, 600),\n    zoom = .7,\n    phi = 30,\n    theta = 337.5\n    # fov =30\n  )\n  rgl::rglwidget(width = 982, height = 1025, reuse = FALSE)\n}\n\n\n\n\n\n\nThanks Miles!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "new graphiti",
    "section": "",
    "text": "Sentinel-2 Ortho Mosaics with STAC and gdalcubes\n\n\n\nr\n\nstac\n\ngdalcubes\n\nsentinel-2\n\nleaflet\n\nremote sensing\n\n\n\n\n\n\n\n\n\nJan 8, 2026\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nGetting details of historic orthophoto imagery with R\n\n\n\nfwapg\n\nr\n\nbcdata\n\nimagery\n\napi\n\n\n\n\n\n\n\n\n\nNov 15, 2024\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nMapping Land Cover with R\n\n\n\nland cover\n\nR\n\nplanetary computer\n\nremote sensing\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nMapping and Plotting Precipitation with R\n\n\n\nprecipitation\n\nR\n\ndrought\n\nrayshader\n\n\n\n\n\n\n\n\n\nJun 19, 2024\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nSetting aws bucket permissions with R\n\n\n\naws\n\ns3\n\nr\n\npaws\n\n\n\n\n\n\n\n\n\nMay 24, 2024\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nSyncing files to aws with R\n\n\n\naws\n\ns3\n\nr\n\npaws\n\nprocessx\n\n\n\n\n\n\n\n\n\nMay 23, 2024\n\n\nal\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/aws-storage-permissions/index.html",
    "href": "posts/aws-storage-permissions/index.html",
    "title": "Setting aws bucket permissions with R",
    "section": "",
    "text": "Here we will set up an s3 bucket with a policy that allows the public to read from the bucket, but not from a specific directory, and allows a particular aws_account_id root user as well as a defined user to write to the bucket.\nAlthough we are stoked on the s3fs package for working with s3 buckets, we will use the paws package more than perhaps necessary here - only to learn about how it all works. Seems like s3fs is the way to go for common moves but paws is the “mom” providing the structure and guidance to that package.\n\n\nCode\nlibrary(paws)\nlibrary(here)\n\n\nhere() starts at /Users/airvine/Projects/repo/new_graphiti\n\n\nCode\nlibrary(jsonlite)\nlibrary(stringr)\nlibrary(s3fs)\n\n\nList our current buckets\n\n\nCode\ns3 &lt;- paws::s3()\ns3$list_buckets()\n\n\n$Buckets\n$Buckets[[1]]\n$Buckets[[1]]$Name\n[1] \"23cog\"\n\n$Buckets[[1]]$CreationDate\n[1] \"2023-03-17 00:07:12 GMT\"\n\n\n\n$Owner\n$Owner$DisplayName\n[1] \"al\"\n\n$Owner$ID\n[1] \"f5267b02e31758d1efea79b4eaef5d0423efb3e6a54ab869dc860bcc68ebae2d\"\n\n\n\nCreate Bucket\nLet’s create a bucket called the same name as this repository.\n\n\nCode\nmy_bucket_name &lt;- basename(here::here()) |&gt; \n  stringr::str_replace_all(\"_\", \"-\") \n\nbucket_path &lt;- s3fs::s3_path(my_bucket_name)\n\n\n\n\nCode\ns3$create_bucket(Bucket = my_bucket_name,\n  CreateBucketConfiguration = list(\n    LocationConstraint = Sys.getenv(\"AWS_DEFAULT_REGION\")\n  ))\n\n\n$Location\n[1] \"http://new-graphiti.s3.amazonaws.com/\"\n\n\n\n\nAdd the policy to the bucket.\n\nImportant - First we need to allow “new public policies” to be added to the bucket. We do this by deleteing the public access block. This is a security feature that prevents public access to the bucket. We will remove it so we can add our own policy. Took a while to catch this.\n\n\n\nCode\ns3$delete_public_access_block(\n  Bucket = my_bucket_name\n)\n\n\nlist()\n\n\n\nWrite the policy for the bucket Here is a function to make a generic policy for an s3 bucket that allows public to read from the bucket, but not from a specific directory, and allows a particular aws_account_id to write to the bucket. Plus + it allows you to provide Presigned URLs so we can provide temporary access to private objects without having to change the overall bucket or object permissions.\n\nKey thing here is that if we want a user to override the policy placed on a directory or file after we Deny access we need to add a condition to the policy that exempts the user (paste0(\"arn:aws:iam::\", aws_account_id, \":user/\", user_access_permission)) from the Deny.\n\n\nCode\n# https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example1.html\n#https://chatgpt.com/share/16106509-a34d-4f69-bf95-cd5eb2649707\naws_policy_write &lt;- function(bucket_name, \n                             bucket_dir_private, \n                             aws_account_id, \n                             user_access_permission, \n                             write_json = FALSE, \n                             dir_output = \"policy\", \n                             file_name = \"policy.json\") {\n  policy &lt;- list(\n    Version = \"2012-10-17\",\n    Statement = list(\n      list(\n        Effect = \"Allow\",\n        Principal = \"*\",\n        Action = \"s3:GetObject\",\n        Resource = paste0(\"arn:aws:s3:::\", bucket_name, \"/*\")\n      ),\n      list(\n        Effect = \"Deny\",\n        Principal = \"*\",\n        Action = \"s3:GetObject\",\n        Resource = paste0(\"arn:aws:s3:::\", bucket_name, \"/\", bucket_dir_private, \"/*\"),\n        # IMPORTANT - Denies everyone from getting objects from the private directory except for user_access_permission\n        Condition = list(\n          StringNotEquals = list(\n            \"aws:PrincipalArn\" = paste0(\"arn:aws:iam::\", aws_account_id, \":user/\", user_access_permission)\n          )\n        )\n      ),\n      list(\n        Effect = \"Allow\",\n        Principal = list(AWS = paste0(\"arn:aws:iam::\", aws_account_id, \":root\")),\n        Action = c(\"s3:DeleteObject\", \"s3:PutObject\"),\n        Resource = paste0(\"arn:aws:s3:::\", bucket_name, \"/*\")\n      )\n      #going to leave this here for now\n      # list(\n      #   Effect = \"Allow\",\n      #   Principal = list(AWS = paste0(\"arn:aws:iam::\", aws_account_id, \":user/\", user_access_permission)),\n      #   Action = c(\"s3:GetBucketLocation\", \"s3:ListBucket\"),\n      #   Resource = paste0(\"arn:aws:s3:::\", bucket_name)\n      # ),\n      # list(\n      #   Effect = \"Allow\",\n      #   Principal = list(AWS = paste0(\"arn:aws:iam::\", aws_account_id, \":user/\", user_access_permission)),\n      #   Action = \"s3:GetObject\",\n      #   Resource = paste0(\"arn:aws:s3:::\", bucket_name, \"/*\")\n      # )\n    )\n  )\n  \n  json_policy &lt;- jsonlite::toJSON(policy, pretty = TRUE, auto_unbox = TRUE)\n  \n  if (write_json) {\n    dir.create(dir_output, showWarnings = FALSE)\n    output_path &lt;- file.path(dir_output, file_name)\n    write(json_policy, file = output_path)\n    message(\"Policy written to \", output_path)\n  } else {\n    return(json_policy)\n  }\n}\n\n\nNow we can write the policy to the bucket.\n\n\nCode\n# run the function to build the json policy statement\nmy_policy &lt;- aws_policy_write(bucket_name = my_bucket_name, \n                         bucket_dir_private = \"private\",\n                         aws_account_id = Sys.getenv(\"AWS_ACCOUNT_ID\"),\n                         user_access_permission = \"airvine\",\n                         write_json = FALSE\n                         )\n\n# push the policy to the bucket\ns3$put_bucket_policy(\n  Bucket = my_bucket_name,\n  Policy = my_policy,\n  ExpectedBucketOwner = Sys.getenv(\"AWS_ACCOUNT_ID\")\n)\n\n\nlist()\n\n\n\n\nCode\n# this is cool - Check the policy was added correctly.\ns3$get_bucket_policy(my_bucket_name)\n\n\n\n\nAdd some files to the bucket\nFirst we add a photo to the main bucket. Going to use s3fs for this since I haven’t actually done just one file yet… We are using the here package to get the path to the image due to rendering complexities.\n\n\nCode\ns3fs::s3_file_copy(\n  path = paste0(here::here(), \"/posts/\", params$post_dir_name, \"/image.jpg\"),\n  bucket_path\n)\n\n\n[1] \"s3://new-graphiti/image.jpg\"\n\n\nThen we add one to the private directory.\n\n\nCode\ns3fs::s3_dir_create(\n  path = paste0(bucket_path, \"/private\")\n)\n\n\n[1] \"s3://new-graphiti/private\"\n\n\nCode\ns3fs::s3_file_copy(\n  path = paste0(here::here(), \"/posts/\", params$post_dir_name, \"/image.jpg\"),\n  paste0(bucket_path, \"/private\")\n)\n\n\n[1] \"s3://new-graphiti/private/image.jpg\"\n\n\n\n\nAccess the bucket\nLet’s see if we can add the images to this post.\nCreate the paths to the images.\n\n\nCode\n# s3fs::s3_dir_info(bucket_path, recurse = TRUE)\nimage_path &lt;- paste0(\"https://\", my_bucket_name, \".s3.amazonaws.com/image.jpg\")\nimage_path_private &lt;- paste0(\"https://\", my_bucket_name, \".s3.amazonaws.com/private/image.jpg\")\n\n\nAccess the public image.\n\n\nCode\nknitr::include_graphics(image_path)\n\n\n\n\n\n\n\n\n\nGood to go.\nAnd now access the private image.\n\n\nCode\nknitr::include_graphics(image_path_private)\n\n\n\n\n\n\n\n\n\n💣 Jackpot! We have the image in the “private” bucket so can’t access them from the post without permission.\n\n\nProvide temporary access to an object\nBecause we granted ourselves access to the private directory and our IAM roles have the correct privileges, we can create a Presigned URL to provide temporary access to the private image. We will set the maximum of 7 days for the URL to be valid. That means that at 2024-06-03 14:48 the URL will no longer work and the image below will no longer render in this post!\n\n\nCode\nknitr::include_graphics(\n  s3fs::s3_file_url(\n    s3_dir_ls(paste0(bucket_path, \"/private\")),\n    604800,\n    \"get_object\"\n  )\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# this is the cmd line way\nurl_file_share &lt;- s3_dir_ls(paste0(bucket_path, \"/private\"))\ncommand &lt;- \"aws\"\nargs &lt;- c('s3', 'presign', url_file_share, '--expires-in', '604800')\n\nworking_directory = here::here() #we could just remove from funciton to get the current wd but its nice to have so we leave\n\n# loaded this function from the other file. should put in functions file or package\nsys_call()\n\n\nIn order to rerun our post we need to delete the bucket. When we do rerun - we use the s3fs package to do it\n\n\nCode\n# Dont delete the bucket or the post wont render! ha\n# Burn down the bucket 🔥.  If we try to use `s3$delete_bucket(Bucket = my_bucket_name)` we will get an error because the \n# bucket is not empty. \n\n#`s3fs::s3_bucket_delete(bucket_path)` works fine though.\ns3fs::s3_bucket_delete(bucket_path)"
  },
  {
    "objectID": "posts/aws-storage-processx/index.html",
    "href": "posts/aws-storage-processx/index.html",
    "title": "Syncing files to aws with R",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, dpi=60, out.width = \"100%\")\noptions(scipen=999)\noptions(knitr.kable.NA = '--') #'--'\noptions(knitr.kable.NAN = '--')\n\n\nInspired by https://blog.djnavarro.net/posts/2022-03-17_using-aws-s3-in-r/ by Danielle Navarro.\nNote to self - /Users/airvine/Projects/repo/new_graphiti/_freeze/posts/aws-storage-processx/index/execute-results/html.json is created when I render this document. This seems to be what is published to website after 1. the github_actions workflow is run to generate the gh-pages branch (on github runner) 2. the site is published with gitpages from github.\n“Quick” post to document where I got to with syncing files to aws with R. Didn’t love the aws.s3::sync function because from what I could tell I could not tell it to delete files if they were not present locally or in a bucket (I could be wrong).\nThen climbed into s3fs which mirrors the fs package and seems a bit more user friendly than the aws.s3 package for managing files. It is created by Dyfan Jones who also is the top contributor to paws!! He seems like perhaps as much of a beast as one of the contributors to s3fs who is Scott Chamberlain.\nFor the sync issue figured why not just call the aws command line tool from R. processx is an insane package that might be the mother of all packages. It allows you to run command line tools from R with flexibility for some things like setting the directory where the command is called from in the processx called function (big deal as far as I can tell).\nWe need to set up our aws account online. The blog above from Danielle Navarro covers that I believe (I struggled through it a long time ago). I should use a ~/.aws/credentials file but don’t yet. I have my credentials in my ~/.Renviron file as well as in my ~/.bash_profile (probably a ridiculous setup). They are:\nAWS_ACCESS_KEY_ID='my_key'\nAWS_DEFAULT_REGION='my_region'\nAWS_SECRET_ACCESS_KEY='my_secret_key'\n\n\nCode\n# library(aws.s3)\nlibrary(processx)\n# library(paws) #this is the mom.  Couple examples of us hashed out here\nlibrary(s3fs)\n# library(aws.iam) #not useing - set permissions\nlibrary(here) #helps us with working directory issues related to the `environment` we operate in when rendering\n\n\n\nSee buckets using the s3fs package.\n\nCurrent buckets are:\n\n\nCode\ns3fs::s3_dir_ls(refresh = TRUE) \n\n\n[1] \"s3://23cog\"\n\n\n\n\nCode\n# First we set up our AWS s3 file system. I am actually not sure this is necessary but I did it.  Will turn the chunk off\n# to not repeat.\n# s3fs::s3_file_system(profile_name = \"s3fs_example\")\n\n\n\n\nCreate a Bucket\nLet’s generate the name of the bucket based on the name of the repo but due to aws bucket naming rules we need to swap out our underscores for hyphens! Maybe a good enough reason to change our naming conventions for our repos on github!!\n\n\nCode\nbucket_name &lt;- basename(here::here()) |&gt; \n  stringr::str_replace_all(\"_\", \"-\") \n\nbucket_path &lt;- s3fs::s3_path(bucket_name)\n\ns3fs::s3_bucket_create( bucket_path)  \n\n\n[1] \"s3://new-graphiti\"\n\n\n\n\nSync Files to Bucket\nWe build a little wrapper function to help us debug issues when running system commands with processx.\n\n\nCode\nsys_call &lt;- function(){\n  result &lt;- tryCatch({\n    processx::run(\n      command,\n      args = args,\n      echo = TRUE,            # Print the command output live\n      wd = working_directory, # Set the working directory\n      spinner = TRUE,         # Show a spinner\n      timeout = 60            # Timeout after 60 seconds\n    )\n  }, error = function(e) {\n    # Handle errors: e.g., print a custom error message\n    cat(\"An error occurred: \", e$message, \"\\n\")\n    NULL  # Return NULL or another appropriate value\n  })\n  \n  # Check if the command was successful\n  if (!is.null(result)) {\n    cat(\"Exit status:\", result$status, \"\\n\")\n    cat(\"Output:\\n\", result$stdout)\n  } else {\n    cat(\"Failed to execute the command properly.\\n\")\n  }\n}\n\n\n\nThen we specify our command and arguments. To achieve the desired behavior of including only files in the assets/* directory, you need to combine the order of --exclude and --include flags appropriately (exclude everything first thenn include what we want):\n\n\nCode\ncommand &lt;- \"aws\"\nargs &lt;- c('s3', 'sync', '.', bucket_path, '--delete', '--exclude', '*', '--include', 'posts/*')\n\nworking_directory = here::here() #we could just remove from funciton to get the current wd but its nice to have so we leave\n\n\nNow lets put a tester file in our directory and sync it to our bucket. We will remove it later to test if it is removed on sync.\n\n\nCode\nfile.create(here::here('posts/test.txt'))\n\n\n[1] TRUE\n\n\nRun our little function to sync the files to the bucket.\n\n\nCode\nsys_call()\n\n\nCompleted 237 Bytes/511.7 KiB (3.0 KiB/s) with 12 file(s) remaining\nupload: posts/_metadata.yml to s3://new-graphiti/posts/_metadata.yml\nCompleted 237 Bytes/511.7 KiB (3.0 KiB/s) with 11 file(s) remaining\nCompleted 2.0 KiB/511.7 KiB (11.9 KiB/s) with 11 file(s) remaining \nupload: posts/logos-equipment/index.qmd to s3://new-graphiti/posts/logos-equipment/index.qmd\nCompleted 2.0 KiB/511.7 KiB (11.9 KiB/s) with 10 file(s) remaining\nCompleted 3.6 KiB/511.7 KiB (20.8 KiB/s) with 10 file(s) remaining\nupload: posts/snakecase/index.qmd to s3://new-graphiti/posts/snakecase/index.qmd\nCompleted 3.6 KiB/511.7 KiB (20.8 KiB/s) with 9 file(s) remaining\nCompleted 8.6 KiB/511.7 KiB (48.5 KiB/s) with 9 file(s) remaining\nupload: posts/snakecase/thumbnail.jpg to s3://new-graphiti/posts/snakecase/thumbnail.jpg\nCompleted 8.6 KiB/511.7 KiB (48.5 KiB/s) with 8 file(s) remaining\nCompleted 13.9 KiB/511.7 KiB (74.3 KiB/s) with 8 file(s) remaining\nupload: posts/aws-storage-permissions/index.qmd to s3://new-graphiti/posts/aws-storage-permissions/index.qmd\nCompleted 13.9 KiB/511.7 KiB (74.3 KiB/s) with 7 file(s) remaining\nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 7 file(s) remaining\nupload: posts/aws-storage-processx/image.jpg to s3://new-graphiti/posts/aws-storage-processx/image.jpg\nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 6 file(s) remaining\nupload: posts/test.txt to s3://new-graphiti/posts/test.txt        \nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 5 file(s) remaining\nCompleted 26.0 KiB/511.7 KiB (119.1 KiB/s) with 5 file(s) remaining\nupload: posts/aws-storage-processx/index.rmarkdown to s3://new-graphiti/posts/aws-storage-processx/index.rmarkdown\nCompleted 26.0 KiB/511.7 KiB (119.1 KiB/s) with 4 file(s) remaining\nCompleted 34.0 KiB/511.7 KiB (152.0 KiB/s) with 4 file(s) remaining\nupload: posts/aws-storage-permissions/image.jpg to s3://new-graphiti/posts/aws-storage-permissions/image.jpg\nCompleted 34.0 KiB/511.7 KiB (152.0 KiB/s) with 3 file(s) remaining\nCompleted 42.2 KiB/511.7 KiB (180.5 KiB/s) with 3 file(s) remaining\nupload: posts/aws-storage-processx/index.qmd to s3://new-graphiti/posts/aws-storage-processx/index.qmd\nCompleted 42.2 KiB/511.7 KiB (180.5 KiB/s) with 2 file(s) remaining\nCompleted 83.0 KiB/511.7 KiB (256.0 KiB/s) with 2 file(s) remaining\nupload: posts/logos-equipment/image.jpg to s3://new-graphiti/posts/logos-equipment/image.jpg\nCompleted 83.0 KiB/511.7 KiB (256.0 KiB/s) with 1 file(s) remaining\nCompleted 339.0 KiB/511.7 KiB (830.9 KiB/s) with 1 file(s) remaining\nCompleted 511.7 KiB/511.7 KiB (654.4 KiB/s) with 1 file(s) remaining\nupload: posts/snakecase/all.jpeg to s3://new-graphiti/posts/snakecase/all.jpeg\nExit status: 0 \nOutput:\n Completed 237 Bytes/511.7 KiB (3.0 KiB/s) with 12 file(s) remaining\nupload: posts/_metadata.yml to s3://new-graphiti/posts/_metadata.yml\nCompleted 237 Bytes/511.7 KiB (3.0 KiB/s) with 11 file(s) remaining\nCompleted 2.0 KiB/511.7 KiB (11.9 KiB/s) with 11 file(s) remaining \nupload: posts/logos-equipment/index.qmd to s3://new-graphiti/posts/logos-equipment/index.qmd\nCompleted 2.0 KiB/511.7 KiB (11.9 KiB/s) with 10 file(s) remaining\nCompleted 3.6 KiB/511.7 KiB (20.8 KiB/s) with 10 file(s) remaining\nupload: posts/snakecase/index.qmd to s3://new-graphiti/posts/snakecase/index.qmd\nCompleted 3.6 KiB/511.7 KiB (20.8 KiB/s) with 9 file(s) remaining\nCompleted 8.6 KiB/511.7 KiB (48.5 KiB/s) with 9 file(s) remaining\nupload: posts/snakecase/thumbnail.jpg to s3://new-graphiti/posts/snakecase/thumbnail.jpg\nCompleted 8.6 KiB/511.7 KiB (48.5 KiB/s) with 8 file(s) remaining\nCompleted 13.9 KiB/511.7 KiB (74.3 KiB/s) with 8 file(s) remaining\nupload: posts/aws-storage-permissions/index.qmd to s3://new-graphiti/posts/aws-storage-permissions/index.qmd\nCompleted 13.9 KiB/511.7 KiB (74.3 KiB/s) with 7 file(s) remaining\nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 7 file(s) remaining\nupload: posts/aws-storage-processx/image.jpg to s3://new-graphiti/posts/aws-storage-processx/image.jpg\nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 6 file(s) remaining\nupload: posts/test.txt to s3://new-graphiti/posts/test.txt        \nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 5 file(s) remaining\nCompleted 26.0 KiB/511.7 KiB (119.1 KiB/s) with 5 file(s) remaining\nupload: posts/aws-storage-processx/index.rmarkdown to s3://new-graphiti/posts/aws-storage-processx/index.rmarkdown\nCompleted 26.0 KiB/511.7 KiB (119.1 KiB/s) with 4 file(s) remaining\nCompleted 34.0 KiB/511.7 KiB (152.0 KiB/s) with 4 file(s) remaining\nupload: posts/aws-storage-permissions/image.jpg to s3://new-graphiti/posts/aws-storage-permissions/image.jpg\nCompleted 34.0 KiB/511.7 KiB (152.0 KiB/s) with 3 file(s) remaining\nCompleted 42.2 KiB/511.7 KiB (180.5 KiB/s) with 3 file(s) remaining\nupload: posts/aws-storage-processx/index.qmd to s3://new-graphiti/posts/aws-storage-processx/index.qmd\nCompleted 42.2 KiB/511.7 KiB (180.5 KiB/s) with 2 file(s) remaining\nCompleted 83.0 KiB/511.7 KiB (256.0 KiB/s) with 2 file(s) remaining\nupload: posts/logos-equipment/image.jpg to s3://new-graphiti/posts/logos-equipment/image.jpg\nCompleted 83.0 KiB/511.7 KiB (256.0 KiB/s) with 1 file(s) remaining\nCompleted 339.0 KiB/511.7 KiB (830.9 KiB/s) with 1 file(s) remaining\nCompleted 511.7 KiB/511.7 KiB (654.4 KiB/s) with 1 file(s) remaining\nupload: posts/snakecase/all.jpeg to s3://new-graphiti/posts/snakecase/all.jpeg\n\n\nThen we can see our bucket contents - as well as list our bucket contents and capture them.\n\n\nCode\ns3fs::s3_dir_tree(bucket_path)\n\n\ns3://new-graphiti\n└── posts\n    ├── _metadata.yml\n    ├── test.txt\n    ├── aws-storage-permissions\n    │   ├── image.jpg\n    │   └── index.qmd\n    ├── aws-storage-processx\n    │   ├── image.jpg\n    │   ├── index.qmd\n    │   └── index.rmarkdown\n    ├── logos-equipment\n    │   ├── image.jpg\n    │   └── index.qmd\n    └── snakecase\n        ├── all.jpeg\n        ├── index.qmd\n        └── thumbnail.jpg\n\n\nCode\nt &lt;- s3fs::s3_dir_info(bucket_path, recurse = TRUE)\n\n\nNow we will remove test.txt\n\n\nCode\nfile.remove(here::here('posts/test.txt'))\n\n\n[1] TRUE\n\n\nNow we sync again.\n\n\nCode\nsys_call()\n\n\ndelete: s3://new-graphiti/posts/test.txt\nExit status: 0 \nOutput:\n delete: s3://new-graphiti/posts/test.txt\n\n\nList our bucket contents and capture them again\n\n\nCode\ns3_dir_tree(bucket_path)\n\n\ns3://new-graphiti\n└── posts\n    ├── _metadata.yml\n    ├── aws-storage-permissions\n    │   ├── image.jpg\n    │   └── index.qmd\n    ├── aws-storage-processx\n    │   ├── image.jpg\n    │   ├── index.qmd\n    │   └── index.rmarkdown\n    ├── logos-equipment\n    │   ├── image.jpg\n    │   └── index.qmd\n    └── snakecase\n        ├── all.jpeg\n        ├── index.qmd\n        └── thumbnail.jpg\n\n\nCode\nt2 &lt;- s3fs::s3_dir_info(bucket_path, recurse = TRUE)\n\n\nCompare the file structure before and after our sync.\n\n\nCode\nwaldo::compare(t$key, t2$key)\n\n\n     old                             | new                                 \n [9] \"posts/snakecase/all.jpeg\"      | \"posts/snakecase/all.jpeg\"      [9] \n[10] \"posts/snakecase/index.qmd\"     | \"posts/snakecase/index.qmd\"     [10]\n[11] \"posts/snakecase/thumbnail.jpg\" | \"posts/snakecase/thumbnail.jpg\" [11]\n[12] \"posts/test.txt\"                -                                     \n\n\nSuccess!!\n\n\nTo Do\nWe need to build the call to sync the other way (cloud to local) in a way that perhaps nukes local files if they are not on the cloud. This is because we need to collaborate within our team so we do things like one person will change the name of images so when the other person syncs they will have only the newly named image in their local directory.\n\nThis all deserved consideration as it could get really messy from a few different angles (ie. one person adds files they don’t want nuked and then they get nukes. There are lots of different options for doing things so we will get there.)\n\n\nDelete Bucket\nLets delete the bucket.\n\n\nCode\n#\nHere is the command line approach that we will turn off in favor of the s3fs approach.\nargs &lt;- c('s3', 'rb', bucket_path, '--force')\nsys_call()\n\n\n\n\nCode\n# Here is the `s3fs` way to \"delete\" all the versions.  \n# list all the files in the bucket\nfl &lt;- s3fs::s3_dir_ls(bucket_path, recurse = TRUE, refresh = TRUE)\n\n# list all the version info for all the files\nvi &lt;- fl |&gt; \n  purrr::map_df(s3fs::s3_file_version_info)\n\ns3fs::s3_file_delete(path = vi$uri)\n\n\n\n\nCode\ns3fs::s3_bucket_delete(bucket_path)\n\n\n[1] \"s3://new-graphiti\"\n\n\nAs we have tried this before we know that if we tell it we want to delete a bucket with versioned files in it we need to empty the bucket first including delete_markers. That is easy in the aws console with th UI but seems tricky. There is a bunch of discussion on options to this here https://stackoverflow.com/questions/29809105/how-do-i-delete-a-versioned-bucket-in-aws-s3-using-the-cli . Thinking a good way around it (and a topic for another post) would be to apply a lifecycle-configuration to the bucket that deletes all versions of files after a day - allowing you to delete bucket after they expire (as per the above post). Really we may want to have a lifecycle-configuration on all our versioned buckets to keep costs down anyway but deserves more thought and perhaps another post.\n\n\nCode\n# old notes\n# We are going to test creating a bucket with versioning on.  This has large implications for billing with some details\n# of how it works [here](https://aws.amazon.com/blogs/aws/amazon-s3-enhancement-versioning/) with example of costs [here](https://aws.amazon.com/s3/faqs/?nc1=h_ls).  Thinking we may want versioned buckets for things like `sqlite`\n# \"snapshot\" databases but definitely not for things like images."
  },
  {
    "objectID": "posts/2024-06-30-land-cover/index.html",
    "href": "posts/2024-06-30-land-cover/index.html",
    "title": "Mapping Land Cover with R",
    "section": "",
    "text": "Visualize and quantify remotely sense land cover data…. Here is a first start. We will use the European Space Agency’s WorldCover product which provides global land cover maps for the years 2020 and 2021 at 10 meter resolution based on the combination of Sentinel-1 radar data and Sentinel-2 imagery. We will use the 2021 dataset for mapping an area of the Skeena watershed near Houston, British Columbia.\nThis post was inspired - with much of the code copied - from a repository on GitHub from the wonderfully talented Milos Popovic.\nFirst thing we will do is load our packages. If you do not have the packages installed yet you can change the update_pkgs param in the yml of this file to TRUE. Using pak is great because it allows you to update your packages when you want to.\nCode\npkgs_cran &lt;- c(\n  \"usethis\",\n  \"rstac\",\n  \"here\",\n  \"fs\",\n  \"terra\",\n  \"tidyverse\",\n  \"rayshader\",\n  \"sf\",\n  \"classInt\",\n  \"rgl\",\n  \"tidyterra\",\n  \"tabulapdf\",\n  \"bcdata\",\n  \"ggplot\",\n  \"ggdark\",\n  \"knitr\",\n  \"DT\",\n  \"htmlwidgets\")\n\npkgs_gh &lt;- c(\n  \"poissonconsulting/fwapgr\",\n  \"NewGraphEnvironment/rfp\"\n )\n\npkgs &lt;- c(pkgs_cran, pkgs_gh)\n\nif(params$update_pkgs){\n  # install the pkgs\n  lapply(pkgs,\n         pak::pkg_install,\n         ask = FALSE)\n}\n\n# load the pkgs\npkgs_ld &lt;- c(pkgs_cran,\n             basename(pkgs_gh))\ninvisible(\n  lapply(pkgs_ld,\n       require,\n       character.only = TRUE)\n)\n\nsource(here::here(\"scripts/functions.R\"))"
  },
  {
    "objectID": "posts/2024-06-30-land-cover/index.html#downsample-the-land-cover-raster",
    "href": "posts/2024-06-30-land-cover/index.html#downsample-the-land-cover-raster",
    "title": "Mapping Land Cover with R",
    "section": "Downsample the Land Cover Raster",
    "text": "Downsample the Land Cover Raster\nHere we downsample the land cover raster to the same resolution as the DEM for the purposes of rendering our map of the larger area in a reasonable amount of time.\n\n\nCode\n# Here we resample the land cover raster to the same resolution as the DEM.\nland_cover_raster_resampled &lt;- terra::resample(\n    land_cover_raster,\n    dem,\n    method = \"near\",\n    threads = TRUE\n)\n\n# terra::plot(land_cover_raster_resampled)"
  },
  {
    "objectID": "posts/2024-06-30-land-cover/index.html#get-additional-data",
    "href": "posts/2024-06-30-land-cover/index.html#get-additional-data",
    "title": "Mapping Land Cover with R",
    "section": "Get Additional Data",
    "text": "Get Additional Data\nWe could use some data for context such as major streams and the railway. We get the streams and railway from data distribution bc api using the bcdata package. Our rfp package calls just allow some extra sanity checks and convenience moves on the bcdata::bcdc_query_geodata function. It’s not really necessary but can be helpful (ex. can use small cap layer and column names and will throw an informative error if the name of the columns specified are input incorrectly).\n\n\n\nCode\n# grab all the railways\nl_rail &lt;- rfp::rfp_bcd_get_data(\n    bcdata_record_id = \"whse_basemapping.gba_railway_tracks_sp\"\n) |&gt; \n  sf::st_transform(4326) |&gt; \n  janitor::clean_names() \n\n# streams in the bulkley and then filter to just keep the big ones\nl_streams &lt;- rfp::rfp_bcd_get_data(\n  bcdata_record_id = \"whse_basemapping.fwa_stream_networks_sp\",\n  col_filter = \"watershed_group_code\",\n  col_filter_value = \"BULK\",\n  # grab a smaller object by including less columns\n  col_extract = c(\"linear_feature_id\", \"stream_order\", \"gnis_name\", \"downstream_route_measure\", \"blue_line_key\", \"length_metre\")\n) |&gt; \n  sf::st_transform(4326) |&gt; \n  janitor::clean_names() |&gt; \n  dplyr::filter(stream_order &gt; 4)\n\n\nNow we trim up those layers. We have some functions to validate and repair geometries and then we clip them to our area of interest.\n\n\nCode\nlayers_to_trim &lt;- tibble::lst(l_rail, l_streams)\n\n# Function to validate and repair geometries\nvalidate_geometries &lt;- function(layer) {\n  layer &lt;- sf::st_make_valid(layer)\n  layer &lt;- layer[sf::st_is_valid(layer), ]\n  return(layer)\n}\n\n# Apply validation to the AOI and layers\naoi &lt;- validate_geometries(aoi)\nlayers_to_trim &lt;- purrr::map(layers_to_trim, validate_geometries)\n\n# clip them  with purrr and sf\nlayers_trimmed &lt;- purrr::map(\n  layers_to_trim,\n  ~ sf::st_intersection(.x, aoi)\n)"
  },
  {
    "objectID": "posts/2024-06-30-land-cover/index.html#get-legend-values",
    "href": "posts/2024-06-30-land-cover/index.html#get-legend-values",
    "title": "Mapping Land Cover with R",
    "section": "Get Legend Values",
    "text": "Get Legend Values\nSo we need to map the values in the raster to the actual land cover classes. We can do this by extracting the cross reference table from the pdf provided in the metatdata of the data. We will use the tabulapdf package to extract the table and do some work to collapse it into a cross-referenceing tool we can use for land cover classifications and subsequent color schemes.\n\n\nCode\n# extract the cross reference table from the pdf\npdf_file &lt;- \"https://esa-worldcover.s3.eu-central-1.amazonaws.com/v200/2021/docs/WorldCover_PUM_V2.0.pdf\"\npage &lt;- 15\n\n# table_map &lt;- tabulapdf::locate_areas(pdf_file, pages = page)\n# table_coords &lt;- list(as.numeric(unlist(table_map[[1]])))\n\ntable_coords &lt;- list(c(94.55745,  74.66493, 755.06007, 550.41094))\n\n\nxref_raw &lt;- tabulapdf::extract_tables(\n  pdf_file,\n  pages = page,\n  method = \"lattice\",\n  area = table_coords,\n  guess = FALSE\n)\n\n# ##this is how we make a clean dataframe\nxref_raw2 &lt;- xref_raw |&gt; \n  purrr::pluck(1) |&gt;\n  tibble::as_tibble() |&gt;\n  janitor::row_to_names(1) |&gt;\n  janitor::clean_names()\n\nxref_raw3 &lt;- xref_raw2 |&gt; \n  tidyr::fill(code, .direction = \"down\")\n\n# Custom function to concatenate rows within each group\ncollapse_rows &lt;- function(df) {\n  df |&gt; \n    dplyr::summarise(across(everything(), ~ paste(na.omit(.), collapse = \" \")))\n}\n\n# Group by code and apply the custom function\nxref &lt;- xref_raw3 |&gt;\n  dplyr::group_by(code) |&gt;\n  dplyr::group_modify(~ collapse_rows(.x)) |&gt;\n  dplyr::ungroup() |&gt; \n  dplyr::mutate(code = as.numeric(code)) |&gt; \n  dplyr::arrange(code) |&gt; \n  purrr::set_names(c(\"code\", \"land_cover_class\", \"lccs_code\", \"definition\", \"color_code\")) |&gt; \n  # now we make a list of the color codes and convert to hex. Even though we don't actually need them here...\n  dplyr::mutate(color_code = purrr::map(color_code, ~ as.numeric(strsplit(.x, \",\")[[1]])),\n                color = purrr::map_chr(color_code, ~ rgb(.x[1], .x[2], .x[3], maxColorValue = 255))) |&gt; \n  dplyr::relocate(definition, .after = last_col())\n\n\nmy_dt_table(xref, cols_freeze_left = 0, page_length = 5)\n\n\n\n\n\n\nWe seem to get issues when the colors we have in our tiff does not match our cross-reference table. For this reason we will remove any values in the xref object that are not in the rasters that we are plotting.\n\nAlso - looks like when we combined our tiffs together with terra::mosaic we lost the color table associated with the SpatRaster object. We can recover that table with terra::coltab(land_cover_raster_raw[[1]])"
  },
  {
    "objectID": "posts/2024-06-30-land-cover/index.html#plot",
    "href": "posts/2024-06-30-land-cover/index.html#plot",
    "title": "Mapping Land Cover with R",
    "section": "Plot",
    "text": "Plot\nOk. Let’s plot it up. We will use ggplot2 and tidyterra to plot the land cover raster and then add the streams and railway on top of that.\n\n\nCode\ncolor_table &lt;- terra::coltab(land_cover_raster_raw[[1]])[[1]]\n\ncoltab(land_cover_raster_resampled) &lt;- color_table\n\nxref_cleaned &lt;- xref |&gt; \n  filter(code %in% sort(unique(terra::values(land_cover_raster_resampled))))\n\n\nmap &lt;- ggplot() +\n  tidyterra::geom_spatraster(\n    data = as.factor(land_cover_raster_resampled),\n    use_coltab = TRUE,\n    maxcell = Inf\n  ) +\n  tidyterra::scale_fill_coltab(\n    data = as.factor(land_cover_raster_resampled),\n    name = \"ESA Land Cover\",\n    labels = xref_cleaned$land_cover_class\n  ) +\n  # geom_sf(\n  #     data = aoi,\n  #     fill = \"transparent\",\n  #     color = \"white\",\n  #     linewidth = .5\n  # ) +\n  geom_sf(\n    data = layers_trimmed$l_streams,\n    color = \"blue\",\n    size = 1\n  ) +\n  geom_sf(\n    data = layers_trimmed$l_rail,\n    color = \"black\",\n    size = 1\n  ) +\n  ggdark::dark_theme_void()\n\n\nmap"
  },
  {
    "objectID": "posts/2024-06-30-land-cover/index.html#plot-refined-area",
    "href": "posts/2024-06-30-land-cover/index.html#plot-refined-area",
    "title": "Mapping Land Cover with R",
    "section": "Plot Refined Area",
    "text": "Plot Refined Area\nFirst we need to clip the landcover raster to the buffered area. We are not going to use the resampled raster because we want a more detailed view of the land cover classes for this much smaller area. The computational time to render the plot will be fine at the original resolution.\n\n\nCode\nland_cover_sample &lt;- terra::crop(land_cover_raster, aoi_refined_buffered, snap = \"near\", mask = TRUE, extend = TRUE)\n#dimensions  : 259, 951, 1  multiple stream segments\n#dimensions  : 262, 958, 1 single stream segment\n\n\n\nWe lose our color values with the crop. We see that with has.colors(land_cover_sample).\n\n\nCode\nhas.colors(land_cover_sample)\n\n\n[1] FALSE\n\n\n\nLet’s add them back in with the terra::coltab function.\n\n\nCode\ncoltab(land_cover_sample) &lt;- color_table\n\n\n\nNow we should be able to plot what we have. Let’s re-trim up our extra data layers to the refined area of interest and add those in as well.\n\n\nCode\n# clip them  with purrr and sf\nlayers_trimmed &lt;- purrr::map(\n  layers_to_trim,\n  ~ sf::st_intersection(.x, aoi_refined_buffered)\n) \n\nxref_cleaned &lt;- xref |&gt; \n  dplyr::filter(code %in% sort(unique(terra::values(land_cover_sample))))\n\nmap &lt;- ggplot2::ggplot() +\n  tidyterra::geom_spatraster(\n    data = as.factor(land_cover_sample),\n    use_coltab = TRUE,\n    maxcell = Inf\n  ) +\n  tidyterra::scale_fill_coltab(\n    data = as.factor(land_cover_sample),\n    name = \"ESA Land Cover\",\n    labels = xref_cleaned$land_cover_class\n  ) +\n  ggplot2::geom_sf(\n    data = layers_trimmed$l_rail,\n    color = \"black\",\n    size = 1\n  ) +\n  ggplot2::geom_sf(\n    data = layers_trimmed$l_streams,\n    color = \"blue\",\n    size = 1\n  ) +\n  ggdark::dark_theme_void()\n\n# save the plot\n# ggsave(here::here('posts', params$post_dir_name, \"image.jpg\"), width = 10, height = 10)\n    \nmap"
  },
  {
    "objectID": "posts/2024-06-30-land-cover/index.html#neexdzii-kwah",
    "href": "posts/2024-06-30-land-cover/index.html#neexdzii-kwah",
    "title": "Mapping Land Cover with R",
    "section": "Neexdzii Kwah",
    "text": "Neexdzii Kwah\nNow lets make a table and a simple bargraph to present the results.\n\n\nCode\n# Calculate the area of each cell (assuming your raster is in lat/lon coordinates)\ncell_area &lt;- terra::cellSize(land_cover_raster_resampled, unit = \"ha\")\n\n# Summarize the raster values\nland_cover_summary_raw &lt;- terra::freq(land_cover_raster_resampled, digits = 0) |&gt; \n  dplyr::mutate(area_ha = round(count * cell_area[1]$area),1) |&gt; \n  # make a column that is the percentage of the total area\n  dplyr::mutate(percent_area = round((area_ha / sum(area_ha) * 100), 1))\n\n# now we add the xref code and land_cover_class to the summary\nland_cover_summary &lt;- land_cover_summary_raw |&gt; \n  dplyr::left_join(xref, by = c(\"value\" = \"code\")) |&gt; \n  dplyr::select(land_cover_class, area_ha, percent_area) |&gt; \n  dplyr::arrange(desc(area_ha))\n\n\n\n\nCode\nmy_caption &lt;- \"Land Cover Class by Refined Area of Interest\"\n\nland_cover_summary |&gt; \n  knitr::kable(caption = my_caption)\n\n\n\nLand Cover Class by Refined Area of Interest\n\n\nland_cover_class\narea_ha\npercent_area\n\n\n\n\nTree cover\n194158\n84.3\n\n\nGrassland\n28940\n12.6\n\n\nPermanent water bodies\n3731\n1.6\n\n\nMoss and lichen\n2850\n1.2\n\n\nBuilt-up\n329\n0.1\n\n\nCropland\n264\n0.1\n\n\nBare / sparse vegetation\n158\n0.1\n\n\nHerbaceous wetland\n9\n0.0\n\n\nShrubland\n1\n0.0\n\n\n\n\n\n\n\nCode\nplot_title &lt;- \"Land Cover Class for Area of Interest\"\n\n# \nland_cover_summary |&gt; \n  # convert land_cover_class to factor and arrange based on the area_ha\n  dplyr::mutate(land_cover_class = forcats::fct_reorder(land_cover_class, area_ha)) |&gt;\n  ggplot2::ggplot(ggplot2::aes(x = land_cover_class, y = area_ha)) +\n  ggplot2::geom_col() +\n  ggplot2::coord_flip() +\n  ggplot2::labs(title= plot_title,\n                x = \"Land Cover Class\",\n                y = \"Area (ha)\") +\n  ggplot2::theme_minimal() \n\n\n\n\n\n\n\n\n\nCode\n  # cowplot::theme_minimal_grid()"
  },
  {
    "objectID": "posts/2024-06-30-land-cover/index.html#refined-area-of-interest",
    "href": "posts/2024-06-30-land-cover/index.html#refined-area-of-interest",
    "title": "Mapping Land Cover with R",
    "section": "Refined Area of Interest",
    "text": "Refined Area of Interest\nNow lets make a table and a simple bargraph to present the results.\n\n\nCode\n# Calculate the area of each cell (assuming your raster is in lat/lon coordinates)\ncell_area &lt;- terra::cellSize(land_cover_sample, unit = \"ha\")\n\n# Summarize the raster values\nland_cover_summary_raw &lt;- terra::freq(land_cover_sample, digits = 0) |&gt; \n  dplyr::mutate(area_ha = round(count * cell_area[1]$area),1) |&gt; \n  # make a column that is the percentage of the total area\n  dplyr::mutate(percent_area = round((area_ha / sum(area_ha) * 100), 1))\n\n# now we add the xref codde and land_cover_class to the summary\nland_cover_summary &lt;- land_cover_summary_raw |&gt; \n  dplyr::left_join(xref, by = c(\"value\" = \"code\")) |&gt; \n  dplyr::select(land_cover_class, area_ha, percent_area) |&gt; \n  dplyr::arrange(desc(area_ha))\n\n\n\n\nCode\nmy_caption &lt;- \"Land Cover Class for Refined Area\"\n\nland_cover_summary |&gt; \n  knitr::kable(caption = my_caption)\n\n\n\nLand Cover Class for Refined Area\n\n\nland_cover_class\narea_ha\npercent_area\n\n\n\n\nTree cover\n283\n66.6\n\n\nGrassland\n89\n20.9\n\n\nBuilt-up\n19\n4.5\n\n\nPermanent water bodies\n19\n4.5\n\n\nCropland\n10\n2.4\n\n\nBare / sparse vegetation\n4\n0.9\n\n\nMoss and lichen\n1\n0.2\n\n\n\n\n\n\n\nCode\nplot_title &lt;- \"Land Cover Class for Refined Area\"\n\n# \nland_cover_summary |&gt; \n  # convert land_cover_class to factor and arrange based on the area_ha\n  dplyr::mutate(land_cover_class = forcats::fct_reorder(land_cover_class, area_ha)) |&gt;\n  ggplot2::ggplot(ggplot2::aes(x = land_cover_class, y = area_ha)) +\n  ggplot2::geom_col() +\n  ggplot2::coord_flip() +\n  ggplot2::labs(title= plot_title,\n                x = \"Land Cover Class\",\n                y = \"Area (ha)\") +\n  ggplot2::theme_minimal()"
  }
]