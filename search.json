[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "rws",
    "section": "",
    "text": "raNdom workish stufF"
  },
  {
    "objectID": "posts/2024-06-19-precipitation/index.html",
    "href": "posts/2024-06-19-precipitation/index.html",
    "title": "Mapping and Plotting Precipitation with R",
    "section": "",
    "text": "WARNING - this work is stolen!! I have adapted this from a repository on GitHub from the wonderfully talented Milos Agathon. All credit to Milos! What a boss - really awesome stuff. Also of note is the image used for the blog. That is Cotey Gallagher… I hope she doesn’t sue me. https://www.linkedin.com/pulse/how-crazy-would-could-really-rain-cats-dogs-cotey-gallagher/\n\nReally interested in quantifying weather data for specific areas that we are working…. Here is a first start.\n\nFirst thing we will do is load our packages. If you do not have the packages installed yet you can change the update_pkgs param in the yml of this file to TRUE. Using pak is great because it allows you to update your packages when you want to.\n\n\nCode\npkgs &lt;- c(\n  \"here\",\n  \"fs\",\n  \"pRecipe\",\n  \"giscoR\",\n  \"terra\",\n  \"tidyverse\",\n  \"rayshader\",\n  \"sf\",\n  \"classInt\",\n  \"rgl\")\n\nif(params$update_pkgs){\n  # install the pkgs\n  lapply(pkgs,\n         pak::pkg_install,\n         ask = FALSE)\n}\n\n# load the pkgs\ninvisible(\n  lapply(pkgs,\n       require,\n       character.only = TRUE)\n)\n\n\n\nDefine our Area of Interest\nHere we diverge a bit from Milos version as we are going to load a custom area of interest. We will be connecting to our remote database using Poisson Consulting’s fwapgr::fwa_watershed_at_measure function which leverages the in database FWA_WatershedAtMeasure function from Simon Norris’ wonderful fwapg package. We use a blue line key and a downstream route measure to define our area of interest which is the Neexdzii Kwa (a.k.a Upper Bulkley River) near Houston, British Columbia.\n\nAs per the Freshwater Atlas of BC - the blue line key is:\n\nUniquely identifies a single flow line such that a main channel and a secondary channel with the same watershed code would have different blue line keys (the Fraser River and all side channels have different blue line keys).\n\n\nA downstream route measure is:\n\nThe distance, in meters, along the route from the mouth of the route to the feature. This distance is measured from the mouth of the containing route to the downstream end of the feature.\n\n\n\nCode\n# lets build a custom watersehed just for upstream of the confluence of Neexdzii Kwa and Wetzin Kwa\n# blueline key\nblk &lt;- 360873822\n# downstream route measure\ndrm &lt;- 166030.4\n\naoi &lt;- fwapgr::fwa_watershed_at_measure(blue_line_key = blk, \n                                        downstream_route_measure = drm) |&gt; \n  sf::st_transform(4326)\n\n\n\n\nRetrieve the Precipitation Data\nFor this example we will retrieve our preciptiation data from Multi-Source Weighted-Ensemble Precipitation using the pRecipe package.\n\n\nCode\n# let's create our data directory\n\ndir_data &lt;- here::here('posts', params$post_dir_name, \"data\")\n\nfs::dir_create(dir_data)\n\n\nTo actually download the data we are going to put a chunk option that allows us to just execute the code once and update it with the update_gis param in our yml. We will use usethis::use_git_ignore to add the data to our .gitignore file so that we do not commit that insano enourmouse file to our git repository.\n\n\nCode\npRecipe::download_data(\n    dataset = \"mswep\",\n    path = dir_data,\n    domain = \"raw\",\n    timestep = \"yearly\"\n)\n\nusethis::use_git_ignore(paste0('posts/', params$post_dir_name, \"/data/*\"))\n\n\nNow we read in our freshly downloaded .nc file and clip to our area of interest and transform the data into a dataframe. We need to remove the data from 2023 because it is only for January as per the .nc filename mswep_tp_mm_global_197902_202301_025_yearly.nc.\n\n\nCode\n# get the name of the file with a .nc at the end\nnc_file &lt;- fs::dir_ls(dir_data, pattern = \".nc\")\n\n# list.files()\n\nmswep_data &lt;- terra::rast(\n  nc_file\n) |&gt;\nterra::crop(\n    aoi\n)\n\nnames(mswep_data) &lt;- 1979:2023\n\nmswep_df &lt;- mswep_data |&gt;\n    as.data.frame(xy = TRUE) |&gt;\n    tidyr::pivot_longer(\n        !c(\"x\", \"y\"),\n        names_to = \"year\",\n        values_to = \"precipitation\"\n    ) |&gt; \n  # 2023 is not complete so we remove it\n    dplyr::filter(year != 2023)\n\n\n\n\nPlot the Precipitation Data by Year\nFirst thing we do here is highjack the plot function from Milos.\n\n\nCode\ntheme_for_the_win &lt;- function(){\n    theme_minimal() +\n    theme(\n        axis.line = element_blank(),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        legend.position = \"right\",\n        legend.title = element_text(\n            size = 11, color = \"grey10\"\n        ),\n        legend.text = element_text(\n            size = 10, color = \"grey10\"\n        ),\n        panel.grid.major = element_line(\n            color = NA\n        ),\n        panel.grid.minor = element_line(\n            color = NA\n        ),\n        plot.background = element_rect(\n            fill = NA, color = NA\n        ),\n        legend.background = element_rect(\n            fill = \"white\", color = NA\n        ),\n        panel.border = element_rect(\n            fill = NA, color = NA\n        ),\n        plot.margin = unit(\n            c(\n                t = 0, r = 0,\n                b = 0, l = 0\n            ), \"lines\"\n        )\n    )\n}\n\nbreaks &lt;- classInt::classIntervals(\n    mswep_df$precipitation,\n    n = 5,\n    style = \"equal\"\n)$brks\n\ncolors &lt;- hcl.colors(\n    n = length(breaks),\n    palette = \"Temps\",\n    rev = TRUE\n)\n\n\nNow we plot the data by year.\n\n\nCode\nmap1 &lt;- ggplot(\n    data = mswep_df\n) +\ngeom_raster(\n    aes(\n        x = x,\n        y = y,\n        fill = precipitation\n    )\n) +\ngeom_contour(\n    aes(\n       x = x,\n       y = y,\n       z = precipitation\n    ), color = \"white\" # add this line\n) +\ngeom_sf(\n    data = aoi,\n    fill = \"transparent\",\n    color = \"grey10\",\n    size = .5\n) +\nscale_fill_gradientn(\n    name = \"mm\",\n    colors = colors,\n    breaks = breaks,\n    labels = round(breaks, 0), # use round(breaks, 0)\n    limits = c(\n        min(mswep_df$precipitation),\n        max(mswep_df$precipitation)\n    )\n) +\nfacet_wrap(~year) +\nguides(\n    fill = guide_colourbar(\n        direction = \"vertical\",\n        barheight = unit(50, \"mm\"),\n        barwidth = unit(5, \"mm\"),\n        title.position = \"top\",\n        label.position = \"right\",\n        title.hjust = .5,\n        label.hjust = .5,\n        ncol = 1,\n        byrow = FALSE\n    )\n) +\ntheme_for_the_win()\n\nmap1\n\n\n\n\n\n\n\n\n\nPretty cool. Interesting to see really wet and dry years in the last 20 years or so such as the wet ones in 2004, 2007, 2011 and 2020 and the dry ones in 2000, 2010, 2014, 2021 and 2022. The contours on the maps are really interesting as they show the gradients which generally run west to east - but occasionally run south to north.\n\n\nAverage Precipitation\nNow we will average all the years together to get an average precipitation map.\n\n\nCode\nmswep_average_df &lt;- mswep_df |&gt;\n    dplyr::group_by(\n        x, y, .drop = FALSE\n    ) |&gt;\n    dplyr::summarise(\n        precipitation = mean(precipitation)\n    ) |&gt; \n  dplyr::mutate(year = \"average\")\n\n\nbreaks &lt;- classInt::classIntervals(\n    mswep_average_df$precipitation,\n    n = 5,\n    style = \"equal\"\n)$brks\n\n\ncolors &lt;- hcl.colors(\n    n = length(breaks),\n    palette = \"Temps\",\n    rev = TRUE\n)\n\nmap_average &lt;- ggplot(\n    data = mswep_average_df\n) +\ngeom_raster(\n    aes(\n        x = x,\n        y = y,\n        fill = precipitation\n    )\n) +\ngeom_contour(\n    aes(\n       x = x,\n       y = y,\n       z = precipitation\n    ), color = \"white\" # add this line\n) +\ngeom_sf(\n    data = aoi,\n    fill = \"transparent\",\n    color = \"grey10\",\n    size = .5\n) +\nscale_fill_gradientn(\n    name = \"mm\",\n    colors = colors,\n    breaks = breaks,\n    labels = round(breaks, 0), # use round(breaks, 0)\n    limits = c(\n        min(mswep_average_df$precipitation),\n        max(mswep_average_df$precipitation)\n    )\n) +\nguides(\n    fill = guide_colourbar(\n        direction = \"vertical\",\n        barheight = unit(50, \"mm\"),\n        barwidth = unit(5, \"mm\"),\n        title.position = \"top\",\n        label.position = \"right\",\n        title.hjust = .5,\n        label.hjust = .5,\n        ncol = 1,\n        byrow = FALSE\n    )\n) +\ntheme_for_the_win()\n\nmap_average\n\n\n\n\n\n\n\n\n\n\n\nCompare the Average Precipitation to a Specific Year\nWe often talk about a “dry” year or a “wet” year. Let’s compare the average precipitation to a specific year. We will build a little function to do this so that we can easily compare any year to the average.\n\n\nCode\nmap_vs_average &lt;- function(year_compare){\n\nmswep_df_2022 &lt;- mswep_df |&gt;\n      dplyr::filter(year == year_compare) |&gt; \n      dplyr::mutate(year = as.character(year))\n\nmswep_df_compare &lt;- bind_rows(mswep_average_df, mswep_df_2022)\n\nbreaks &lt;- classInt::classIntervals(\n    mswep_df_compare$precipitation,\n    n = 5,\n    style = \"equal\"\n)$brks\n\ncolors &lt;- hcl.colors(\n    n = length(breaks),\n    palette = \"Temps\",\n    rev = TRUE\n)\n\nggplot(\n    data = mswep_df_compare\n) +\n  facet_wrap(~year) +\ngeom_raster(\n    aes(\n        x = x,\n        y = y,\n        fill = precipitation\n    )\n) +\ngeom_contour(\n    aes(\n       x = x,\n       y = y,\n       z = precipitation\n    ), color = \"white\" # add this line\n) +\ngeom_sf(\n    data = aoi,\n    fill = \"transparent\",\n    color = \"grey10\",\n    size = .5\n) +\nscale_fill_gradientn(\n    name = \"mm\",\n    colors = colors,\n    breaks = breaks,\n    labels = round(breaks, 0), # use round(breaks, 0)\n    limits = c(\n        min(mswep_df_compare$precipitation),\n        max(mswep_df_compare$precipitation)\n    )\n) +\nguides(\n    fill = guide_colourbar(\n        direction = \"vertical\",\n        barheight = unit(50, \"mm\"),\n        barwidth = unit(5, \"mm\"),\n        title.position = \"top\",\n        label.position = \"right\",\n        title.hjust = .5,\n        label.hjust = .5,\n        ncol = 1,\n        byrow = FALSE\n    )\n) +\ntheme_for_the_win()\n}\n\n\nFirst let’s check out 2022 as it seemed pretty dry.\n\n\nCode\nmap_vs_average(2022)\n\n\n\n\n\n\n\n\n\nNow let’s look at 2020 as that seemed really we with lots of streams flowing really nicely.\n\n\nCode\nmap_vs_average(2020)\n\n\n\n\n\n\n\n\n\nDefinitely wetter than average.\n\n\n3D Contour Map\nLet’s make an interactive 3D contour map of the average precipitation data. Use the mouse to rotate the map and zoom in and out!\n\n\nCode\n{\n  \n  rayshader::plot_gg(\n    ggobj = map_average,\n    width = 7,\n    height = 7,\n    scale = 250,\n    solid = FALSE,\n    shadow = TRUE,\n    shadowcolor = \"white\",\n    shadowwidth = 0,\n    shadow_intensity = 1,\n    # window.size = c(600, 600),\n    zoom = .7,\n    phi = 85\n    # theta = 0\n  )\n  rgl::rglwidget(width = 982, height = 1025, reuse = FALSE)\n}\n\n\n\n\n\n\nThanks Miles!"
  },
  {
    "objectID": "posts/snakecase/index.html",
    "href": "posts/snakecase/index.html",
    "title": "snake_case vs Everything_Else",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, dpi=60, out.width = \"100%\")\noptions(scipen=999)\noptions(knitr.kable.NA = '--') #'--'\noptions(knitr.kable.NAN = '--')\n\n\nBeen preferring snakecase naming convention for all files and columns and variables for a long time for the following reasons:\n\nIt’s much easier to type. Reaching for the shift button is a pain.\nArguably easier to read. I find it easier to read snake_case than CamelCase.\nCan make it easy to name things in a fashion that allows you to dissect, what, something, is by the way it is named and allows automatic sorting to group similar things together. This presentation by Jenny Bryan is a good read - https://speakerdeck.com/jennybc/how-to-name-files-the-sequel.\n\n\n\nCode\nknitr::include_graphics(\"thumbnail.jpg\")\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nWorth noting that sometimes-rules-need-to-be-broken sometimes such as when you are naming chunks in Rmarkdown. It breaks our cross-references.\n\nAs we see here though - it doesn’t matter in Quarto (ex. see Figure 1) vs Figure 2).\n\n\nCode\nknitr::include_graphics(\"all.jpeg\")\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\n“no names have an anonymous function”.\n    -Michael Sumner"
  },
  {
    "objectID": "posts/2024-05-27-references-bib-succinct/index.html",
    "href": "posts/2024-05-27-references-bib-succinct/index.html",
    "title": "Cleaning up bib files flexibly with Zotero and R",
    "section": "",
    "text": "Here we will clean up a bib file exported from Zotero using R to contain only the entries found in a Rmarkdown file.\nCode\nlibrary(here)\n\n\nhere() starts at /Users/airvine/Projects/repo/new_graphiti\n\n\nCode\nlibrary(stringr)\nlibrary(knitr)\n\n# get the name of this post directory\npost_dir &lt;- paste0(here::here(), \"/posts/\", params$post_dir_name)\npost_dir_fig &lt;- paste0(post_dir, \"/fig/\")\nLet’s define our .bib files\nCode\nrmd_file &lt;- \"~/Projects/repo/fish_passage_peace_2023_reporting/fish_passage_peace_2023_reporting.Rmd\"\nbib_file &lt;- paste0(here::here(), \"/assets/NewGraphEnvironment.bib\")\noutput_file &lt;- \"~/Projects/repo/fish_passage_peace_2023_reporting/references.bib\"\nHere are the functions:\nCode\n# Function to extract citations from an RMarkdown file\nbib_extract_citations &lt;- function(rmd_file, additional_citations = c()) {\n  # Read the entire RMarkdown file\n  lines &lt;- readLines(rmd_file)\n  \n  # Concatenate all lines into a single string\n  text &lt;- paste(lines, collapse = \" \")\n  \n  # Regular expression to find citations in the form of @this_citation or [@that_citation; @another_citation]\n  citation_pattern &lt;- \"@[a-zA-Z0-9_:-]+\"\n  \n  # Extract all citations\n  citations &lt;- str_extract_all(text, citation_pattern)[[1]]\n  \n  # Remove the leading '@' from the citations\n  citations &lt;- unique(sub(\"^@\", \"\", citations))\n  \n  # Combine with additional citations\n  all_citations &lt;- unique(c(citations, additional_citations))\n  \n  return(all_citations)\n}\n\n# Function to clean a BibTeX file to keep only cited entries\nbib_clean &lt;- function(bib_file, citations, output_file) {\n  file.create(output_file)\n  # Read the entire BibTeX file\n  lines &lt;- readLines(bib_file)\n  \n  # Initialize variables\n  keep_entry &lt;- FALSE\n  cleaned_lines &lt;- c()\n  \n  for (line in lines) {\n    # Check if the line starts a new citation entry\n    if (grepl(\"^@\", line)) {\n      # Extract the citation key\n      citation_key &lt;- sub(\"^@.*\\\\{([^,]+),.*\", \"\\\\1\", line)\n      \n      # Determine if the entry should be kept\n      keep_entry &lt;- citation_key %in% citations\n    }\n    \n    # Add the line to cleaned_lines if the entry is to be kept\n    if (keep_entry) {\n      cleaned_lines &lt;- c(cleaned_lines, line)\n    }\n  }\n  \n  # Write the cleaned lines to the output file\n  writeLines(cleaned_lines, output_file)\n  \n  cat(\"Cleaned BibTeX file created:\", output_file, \"\\n\")\n}\nAs a big part of the motivation to do this is to reduce the bloat in our repositories we will add the default name of the bib file to the .gitignore of this repo.\nCode\nknitr::include_graphics(paste0(post_dir_fig, \"Screen Shot 2024-05-27 at 1.40.44 PM.png\"))\n\n\n\n\n\n\n\n\n\nCode\nknitr::include_graphics(paste0(post_dir_fig, \"Screen Shot 2024-05-27 at 1.40.55 PM.png\"))"
  },
  {
    "objectID": "posts/2024-05-27-references-bib-succinct/index.html#extract-citations-from-the-rmarkdown-file",
    "href": "posts/2024-05-27-references-bib-succinct/index.html#extract-citations-from-the-rmarkdown-file",
    "title": "Cleaning up bib files flexibly with Zotero and R",
    "section": "Extract citations from the RMarkdown file",
    "text": "Extract citations from the RMarkdown file\n\n\nCode\n# Extract citations from the RMarkdown file\ncitations &lt;- bib_extract_citations(rmd_file, additional_citations = nocites)"
  },
  {
    "objectID": "posts/2024-05-27-references-bib-succinct/index.html#clean-the-bibtex-file-to-keep-only-cited-entries",
    "href": "posts/2024-05-27-references-bib-succinct/index.html#clean-the-bibtex-file-to-keep-only-cited-entries",
    "title": "Cleaning up bib files flexibly with Zotero and R",
    "section": "Clean the BibTeX file to keep only cited entries",
    "text": "Clean the BibTeX file to keep only cited entries\n\n\nCode\nbib_clean(bib_file, citations, output_file)\n\n\nCleaned BibTeX file created: ~/Projects/repo/fish_passage_peace_2023_reporting/references.bib"
  },
  {
    "objectID": "posts/aws-storage-permissions/index.html",
    "href": "posts/aws-storage-permissions/index.html",
    "title": "Setting aws bucket permissions with R",
    "section": "",
    "text": "Here we will set up an s3 bucket with a policy that allows the public to read from the bucket, but not from a specific directory, and allows a particular aws_account_id root user as well as a defined user to write to the bucket.\nAlthough we are stoked on the s3fs package for working with s3 buckets, we will use the paws package more than perhaps necessary here - only to learn about how it all works. Seems like s3fs is the way to go for common moves but paws is the “mom” providing the structure and guidance to that package.\n\n\nCode\nlibrary(paws)\nlibrary(here)\n\n\nhere() starts at /Users/airvine/Projects/repo/new_graphiti\n\n\nCode\nlibrary(jsonlite)\nlibrary(stringr)\nlibrary(s3fs)\n\n\nList our current buckets\n\n\nCode\ns3 &lt;- paws::s3()\ns3$list_buckets()\n\n\n$Buckets\n$Buckets[[1]]\n$Buckets[[1]]$Name\n[1] \"23cog\"\n\n$Buckets[[1]]$CreationDate\n[1] \"2023-03-17 00:07:12 GMT\"\n\n\n\n$Owner\n$Owner$DisplayName\n[1] \"al\"\n\n$Owner$ID\n[1] \"f5267b02e31758d1efea79b4eaef5d0423efb3e6a54ab869dc860bcc68ebae2d\"\n\n\n\nCreate Bucket\nLet’s create a bucket called the same name as this repository.\n\n\nCode\nmy_bucket_name &lt;- basename(here::here()) |&gt; \n  stringr::str_replace_all(\"_\", \"-\") \n\nbucket_path &lt;- s3fs::s3_path(my_bucket_name)\n\n\n\n\nCode\ns3$create_bucket(Bucket = my_bucket_name,\n  CreateBucketConfiguration = list(\n    LocationConstraint = Sys.getenv(\"AWS_DEFAULT_REGION\")\n  ))\n\n\n$Location\n[1] \"http://new-graphiti.s3.amazonaws.com/\"\n\n\n\n\nAdd the policy to the bucket.\n\nImportant - First we need to allow “new public policies” to be added to the bucket. We do this by deleteing the public access block. This is a security feature that prevents public access to the bucket. We will remove it so we can add our own policy. Took a while to catch this.\n\n\n\nCode\ns3$delete_public_access_block(\n  Bucket = my_bucket_name\n)\n\n\nlist()\n\n\n\nWrite the policy for the bucket Here is a function to make a generic policy for an s3 bucket that allows public to read from the bucket, but not from a specific directory, and allows a particular aws_account_id to write to the bucket. Plus + it allows you to provide Presigned URLs so we can provide temporary access to private objects without having to change the overall bucket or object permissions.\n\nKey thing here is that if we want a user to override the policy placed on a directory or file after we Deny access we need to add a condition to the policy that exempts the user (paste0(\"arn:aws:iam::\", aws_account_id, \":user/\", user_access_permission)) from the Deny.\n\n\nCode\n# https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example1.html\n#https://chatgpt.com/share/16106509-a34d-4f69-bf95-cd5eb2649707\naws_policy_write &lt;- function(bucket_name, \n                             bucket_dir_private, \n                             aws_account_id, \n                             user_access_permission, \n                             write_json = FALSE, \n                             dir_output = \"policy\", \n                             file_name = \"policy.json\") {\n  policy &lt;- list(\n    Version = \"2012-10-17\",\n    Statement = list(\n      list(\n        Effect = \"Allow\",\n        Principal = \"*\",\n        Action = \"s3:GetObject\",\n        Resource = paste0(\"arn:aws:s3:::\", bucket_name, \"/*\")\n      ),\n      list(\n        Effect = \"Deny\",\n        Principal = \"*\",\n        Action = \"s3:GetObject\",\n        Resource = paste0(\"arn:aws:s3:::\", bucket_name, \"/\", bucket_dir_private, \"/*\"),\n        # IMPORTANT - Denies everyone from getting objects from the private directory except for user_access_permission\n        Condition = list(\n          StringNotEquals = list(\n            \"aws:PrincipalArn\" = paste0(\"arn:aws:iam::\", aws_account_id, \":user/\", user_access_permission)\n          )\n        )\n      ),\n      list(\n        Effect = \"Allow\",\n        Principal = list(AWS = paste0(\"arn:aws:iam::\", aws_account_id, \":root\")),\n        Action = c(\"s3:DeleteObject\", \"s3:PutObject\"),\n        Resource = paste0(\"arn:aws:s3:::\", bucket_name, \"/*\")\n      )\n      #going to leave this here for now\n      # list(\n      #   Effect = \"Allow\",\n      #   Principal = list(AWS = paste0(\"arn:aws:iam::\", aws_account_id, \":user/\", user_access_permission)),\n      #   Action = c(\"s3:GetBucketLocation\", \"s3:ListBucket\"),\n      #   Resource = paste0(\"arn:aws:s3:::\", bucket_name)\n      # ),\n      # list(\n      #   Effect = \"Allow\",\n      #   Principal = list(AWS = paste0(\"arn:aws:iam::\", aws_account_id, \":user/\", user_access_permission)),\n      #   Action = \"s3:GetObject\",\n      #   Resource = paste0(\"arn:aws:s3:::\", bucket_name, \"/*\")\n      # )\n    )\n  )\n  \n  json_policy &lt;- jsonlite::toJSON(policy, pretty = TRUE, auto_unbox = TRUE)\n  \n  if (write_json) {\n    dir.create(dir_output, showWarnings = FALSE)\n    output_path &lt;- file.path(dir_output, file_name)\n    write(json_policy, file = output_path)\n    message(\"Policy written to \", output_path)\n  } else {\n    return(json_policy)\n  }\n}\n\n\nNow we can write the policy to the bucket.\n\n\nCode\n# run the function to build the json policy statement\nmy_policy &lt;- aws_policy_write(bucket_name = my_bucket_name, \n                         bucket_dir_private = \"private\",\n                         aws_account_id = Sys.getenv(\"AWS_ACCOUNT_ID\"),\n                         user_access_permission = \"airvine\",\n                         write_json = FALSE\n                         )\n\n# push the policy to the bucket\ns3$put_bucket_policy(\n  Bucket = my_bucket_name,\n  Policy = my_policy,\n  ExpectedBucketOwner = Sys.getenv(\"AWS_ACCOUNT_ID\")\n)\n\n\nlist()\n\n\n\n\nCode\n# this is cool - Check the policy was added correctly.\ns3$get_bucket_policy(my_bucket_name)\n\n\n\n\nAdd some files to the bucket\nFirst we add a photo to the main bucket. Going to use s3fs for this since I haven’t actually done just one file yet… We are using the here package to get the path to the image due to rendering complexities.\n\n\nCode\ns3fs::s3_file_copy(\n  path = paste0(here::here(), \"/posts/\", params$post_dir_name, \"/image.jpg\"),\n  bucket_path\n)\n\n\n[1] \"s3://new-graphiti/image.jpg\"\n\n\nThen we add one to the private directory.\n\n\nCode\ns3fs::s3_dir_create(\n  path = paste0(bucket_path, \"/private\")\n)\n\n\n[1] \"s3://new-graphiti/private\"\n\n\nCode\ns3fs::s3_file_copy(\n  path = paste0(here::here(), \"/posts/\", params$post_dir_name, \"/image.jpg\"),\n  paste0(bucket_path, \"/private\")\n)\n\n\n[1] \"s3://new-graphiti/private/image.jpg\"\n\n\n\n\nAccess the bucket\nLet’s see if we can add the images to this post.\nCreate the paths to the images.\n\n\nCode\n# s3fs::s3_dir_info(bucket_path, recurse = TRUE)\nimage_path &lt;- paste0(\"https://\", my_bucket_name, \".s3.amazonaws.com/image.jpg\")\nimage_path_private &lt;- paste0(\"https://\", my_bucket_name, \".s3.amazonaws.com/private/image.jpg\")\n\n\nAccess the public image.\n\n\nCode\nknitr::include_graphics(image_path)\n\n\n\n\n\n\n\n\n\nGood to go.\nAnd now access the private image.\n\n\nCode\nknitr::include_graphics(image_path_private)\n\n\n\n\n\n\n\n\n\n💣 Jackpot! We have the image in the “private” bucket so can’t access them from the post without permission.\n\n\nProvide temporary access to an object\nBecause we granted ourselves access to the private directory and our IAM roles have the correct privileges, we can create a Presigned URL to provide temporary access to the private image. We will set the maximum of 7 days for the URL to be valid. That means that at 2024-06-03 14:48 the URL will no longer work and the image below will no longer render in this post!\n\n\nCode\nknitr::include_graphics(\n  s3fs::s3_file_url(\n    s3_dir_ls(paste0(bucket_path, \"/private\")),\n    604800,\n    \"get_object\"\n  )\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# this is the cmd line way\nurl_file_share &lt;- s3_dir_ls(paste0(bucket_path, \"/private\"))\ncommand &lt;- \"aws\"\nargs &lt;- c('s3', 'presign', url_file_share, '--expires-in', '604800')\n\nworking_directory = here::here() #we could just remove from funciton to get the current wd but its nice to have so we leave\n\n# loaded this function from the other file. should put in functions file or package\nsys_call()\n\n\nIn order to rerun our post we need to delete the bucket. When we do rerun - we use the s3fs package to do it\n\n\nCode\n# Dont delete the bucket or the post wont render! ha\n# Burn down the bucket 🔥.  If we try to use `s3$delete_bucket(Bucket = my_bucket_name)` we will get an error because the \n# bucket is not empty. \n\n#`s3fs::s3_bucket_delete(bucket_path)` works fine though.\ns3fs::s3_bucket_delete(bucket_path)"
  },
  {
    "objectID": "posts/logos-equipment/index.html",
    "href": "posts/logos-equipment/index.html",
    "title": "Logos and Equipment List Somewhere Accessible",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, dpi=60, out.width = \"100%\")\noptions(scipen=999)\noptions(knitr.kable.NA = '--') #'--'\noptions(knitr.kable.NAN = '--')\n\n\nThis here is a post showing where we are now keeping the company equipment details as well as the company logos. We need these things to be accessible to all team members from all repos so we have put them here since this is a public repo.\nHere is the location of the equipment lists that we use in safety/field planning:\nhttps://raw.githubusercontent.com/NewGraphEnvironment/new_graphiti/main/assets/data/equipment.csv\n\n\nCode\nreadr::read_csv(\n  url(\n    glue::glue(\"https://raw.githubusercontent.com/{params$repo_owner}/{params$repo_name}/main/assets/data/equipment.csv\")\n  )\n)|&gt; \n  fpr::fpr_kable(font = 12)\n\n\n\n\n\n\n\neq_item\neq_pers_standard\neq_truck\neq_safety\neq_task1\neq_task2\neq_task3\neq_task4\neq_task5\neq_task6\neq_task7\nmateo_winterschedt\n\n\n\n\nclinometer\nx\n--\n--\nall\nelectrofishing\nfish passage\n--\n--\n--\n--\nx\n\n\nfield vest\nx\n--\n--\nall\n--\nfish passage\n--\n--\n--\n--\nx\n\n\nnote book\nx\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nGPS\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\nx\n\n\nSuncreen\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nBugspray\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nPolarized glasses\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nBear Spray\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\nx\n\n\nphone/camera\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nbattery pack booster for phone\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nHat\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nfirst aid kit personal\nx\n--\nx\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nWaders\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\nx\n\n\nBoots\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\nx\n\n\nExtra clothes\nx\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nrain gear\nx\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nSki poles\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nwater\nx\n--\nx\nall\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nfood\nx\n--\nx\nall\n--\nfish passage\n--\n--\n--\n--\n--\n\n\ngloves work\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nglasses safety\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nheadlamp\nx\n--\nx\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nOakton Multimeter\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\nTurbidity Meter LaMotte 2020e\n--\n--\n--\n--\n--\n--\nenvironmental monitoring\nwq\n--\n--\n--\n\n\nHand saw\n--\nx\nx\nall\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\nBackpack Electrofisher\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\nstop nets x 4\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\nsalt blocks\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\nloose salt\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\ndip nets x 2\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\nLinesman Gloves x 3\n--\n--\nx\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\ntape measure hand\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\ntape measure eslon\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\npilon x 2\n--\nx\n--\nall\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nMeasuring board\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nScale\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nPermits\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nBackroads Mapbook\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nLocational maps\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nFish ID book\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nBackground Documents\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nradio handheld\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nradio truck\n--\nx\n--\n--\n--\n--\n--\n--\n--\n--\nx\n\n\nradio chargers x 3\n--\n--\nx\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nSatelite communicator\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nField Safety Plan\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nfirst aid kit level 1\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nFirst Aid binder stocked\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nSite Cards / Field Guide\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nMinnow Traps\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nCatfood\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nFlagging\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nLaptop w/basecamp\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nGPS cable\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nLazer level\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nAssessment cards fish passage\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nUAV radio\n--\n--\nx\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nUAV\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nUAV landing pad\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nUAV GC tape\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nUAV safety plan (when required)\n--\n--\nx\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nUAV registration\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nUAV license\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nUAV radio license\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nFlow meter\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nATV\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nThrow bags\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\npolaski\n--\nx\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nshovel\n--\nx\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nfire extinguisher backpack\n--\nx\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nfire extinguisher pressurized\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nbucket rigid x 2\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nbucket foldable\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nclove oil kit w/ instructions\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\ngloves leather\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nhard hat\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nsteel toed boots\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nsharpies\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nhand lens\nx\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nATV gas\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nATV lock\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nUAV battery charger\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nwader disinfectant kit\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\ntruck tow rope\n--\nx\nx\nall\n--\n--\n--\n--\n--\n--\n--\n\n\ntruck jack\n--\nx\nx\nall\n--\n--\n--\n--\n--\n--\n--\n\n\nGPS batteries\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\nATV helmets\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nBattery booster\n--\nx\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nCompressor 12V\n--\nx\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nRubber boots (no-slip soles)\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nSmall BT Speaker (for bears)\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nGPS Case waterproof\n--\n--\n--\nsnorkling\n--\n--\n--\n--\n--\n--\n--\n\n\nNotebook waterproof\n--\n--\n--\nsnorkling\n--\n--\n--\n--\n--\n--\n--\n\n\nDrysuits\n--\n--\n--\nsnorkling\n--\n--\n--\n--\n--\n--\n--\n\n\nSnorkels\n--\n--\n--\nsnorkling\n--\n--\n--\n--\n--\n--\n--\n\n\ndrysuit gloves\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nGoggles\n--\n--\n--\nsnorkling\n--\n--\n--\n--\n--\n--\n--\n\n\nFanny pack\n--\n--\n--\nsnorkling\n--\n--\n--\n--\n--\n--\n--\n\n\nlarge backpack\n--\n--\n--\n--\nelectrofishing\n--\n--\n--\n--\n--\n--\n\n\nTow strap\n--\nx\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nrange finder\nx\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\n\n\n\n\n\n\n\nLogos:\nhttps://raw.githubusercontent.com/NewGraphEnvironment/new_graphiti/main/assets/logos\nThere are many — so here is a list of their names and locations online:\n\n\nCode\nfile_names &lt;- fs::dir_ls(\n  glue::glue(here::here(\"assets/logos\")),\n  glob = c(\"*.png\", \"*.jpg\", \"*.jpeg\"),\n  recurse = TRUE\n) \n\n\ntibble::tibble(path = file_names) |&gt; \n    dplyr::mutate(path = stringr::str_replace_all(path, \"/Users/airvine/Projects/repo/new_graphiti\", \"https://github.com/NewGraphEnvironment/new_graphiti/tree/main\")) |&gt; \n  fpr::fpr_kable(font = 12)\n\n\n\n\n\n\n\npath\n\n\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/BLACK/PNG/nge-full_black.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/BLACK/PNG/nge-icon_black.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/BLACK/PNG/nge-icon_name_big_black.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/BLACK/PNG/nge-icon_name_black.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/BLACK/PNG/nge-name_black.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/BLACK/PNG/nge-name_research_consulting_black.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/BLACK/PNG/nge-research_consulting_black.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/COLOR/PNG/nge-full_color.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/COLOR/PNG/nge-icon.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/COLOR/PNG/nge-icon_name.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/COLOR/PNG/nge-icon_name_big_color.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/COLOR/PNG/nge-name_color.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/COLOR/PNG/nge-name_research_consulting_color.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/COLOR/PNG/nge-research_consulting_color.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/GREYSCALE/PNG/nge-full_greyscale.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/GREYSCALE/PNG/nge-icon_gradient_grey.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/GREYSCALE/PNG/nge-icon_name_big_greyscale.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/GREYSCALE/PNG/nge-icon_name_greyscale.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/GREYSCALE/PNG/nge-name_greyscale.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/GREYSCALE/PNG/nge-name_research_consulting_greyscale.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/GREYSCALE/PNG/nge-research_consulting_greyscale.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/WHITE/PNG/nge-full_white.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/WHITE/PNG/nge-icon_name_big_white.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/WHITE/PNG/nge-icon_name_white.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/WHITE/PNG/nge-icon_white.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/WHITE/PNG/nge-name_research_consulting_white.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/WHITE/PNG/nge-name_white.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/WHITE/PNG/nge-research_consulting_white.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/pixel.png"
  },
  {
    "objectID": "posts/aws-storage-processx/index.html",
    "href": "posts/aws-storage-processx/index.html",
    "title": "Syncing files to aws with R",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, dpi=60, out.width = \"100%\")\noptions(scipen=999)\noptions(knitr.kable.NA = '--') #'--'\noptions(knitr.kable.NAN = '--')\n\n\nInspired by https://blog.djnavarro.net/posts/2022-03-17_using-aws-s3-in-r/ by Danielle Navarro.\nNote to self - /Users/airvine/Projects/repo/new_graphiti/_freeze/posts/aws-storage-processx/index/execute-results/html.json is created when I render this document. This seems to be what is published to website after 1. the github_actions workflow is run to generate the gh-pages branch (on github runner) 2. the site is published with gitpages from github.\n“Quick” post to document where I got to with syncing files to aws with R. Didn’t love the aws.s3::sync function because from what I could tell I could not tell it to delete files if they were not present locally or in a bucket (I could be wrong).\nThen climbed into s3fs which mirrors the fs package and seems a bit more user friendly than the aws.s3 package for managing files. It is created by Dyfan Jones who also is the top contributor to paws!! He seems like perhaps as much of a beast as one of the contributors to s3fs who is Scott Chamberlain.\nFor the sync issue figured why not just call the aws command line tool from R. processx is an insane package that might be the mother of all packages. It allows you to run command line tools from R with flexibility for some things like setting the directory where the command is called from in the processx called function (big deal as far as I can tell).\nWe need to set up our aws account online. The blog above from Danielle Navarro covers that I believe (I struggled through it a long time ago). I should use a ~/.aws/credentials file but don’t yet. I have my credentials in my ~/.Renviron file as well as in my ~/.bash_profile (probably a ridiculous setup). They are:\nAWS_ACCESS_KEY_ID='my_key'\nAWS_DEFAULT_REGION='my_region'\nAWS_SECRET_ACCESS_KEY='my_secret_key'\n\n\nCode\n# library(aws.s3)\nlibrary(processx)\n# library(paws) #this is the mom.  Couple examples of us hashed out here\nlibrary(s3fs)\n# library(aws.iam) #not useing - set permissions\nlibrary(here) #helps us with working directory issues related to the `environment` we operate in when rendering\n\n\n\nSee buckets using the s3fs package.\n\nCurrent buckets are:\n\n\nCode\ns3fs::s3_dir_ls(refresh = TRUE) \n\n\n[1] \"s3://23cog\"\n\n\n\n\nCode\n# First we set up our AWS s3 file system. I am actually not sure this is necessary but I did it.  Will turn the chunk off\n# to not repeat.\n# s3fs::s3_file_system(profile_name = \"s3fs_example\")\n\n\n\n\nCreate a Bucket\nLet’s generate the name of the bucket based on the name of the repo but due to aws bucket naming rules we need to swap out our underscores for hyphens! Maybe a good enough reason to change our naming conventions for our repos on github!!\n\n\nCode\nbucket_name &lt;- basename(here::here()) |&gt; \n  stringr::str_replace_all(\"_\", \"-\") \n\nbucket_path &lt;- s3fs::s3_path(bucket_name)\n\ns3fs::s3_bucket_create( bucket_path)  \n\n\n[1] \"s3://new-graphiti\"\n\n\n\n\nSync Files to Bucket\nWe build a little wrapper function to help us debug issues when running system commands with processx.\n\n\nCode\nsys_call &lt;- function(){\n  result &lt;- tryCatch({\n    processx::run(\n      command,\n      args = args,\n      echo = TRUE,            # Print the command output live\n      wd = working_directory, # Set the working directory\n      spinner = TRUE,         # Show a spinner\n      timeout = 60            # Timeout after 60 seconds\n    )\n  }, error = function(e) {\n    # Handle errors: e.g., print a custom error message\n    cat(\"An error occurred: \", e$message, \"\\n\")\n    NULL  # Return NULL or another appropriate value\n  })\n  \n  # Check if the command was successful\n  if (!is.null(result)) {\n    cat(\"Exit status:\", result$status, \"\\n\")\n    cat(\"Output:\\n\", result$stdout)\n  } else {\n    cat(\"Failed to execute the command properly.\\n\")\n  }\n}\n\n\n\nThen we specify our command and arguments. To achieve the desired behavior of including only files in the assets/* directory, you need to combine the order of --exclude and --include flags appropriately (exclude everything first thenn include what we want):\n\n\nCode\ncommand &lt;- \"aws\"\nargs &lt;- c('s3', 'sync', '.', bucket_path, '--delete', '--exclude', '*', '--include', 'posts/*')\n\nworking_directory = here::here() #we could just remove from funciton to get the current wd but its nice to have so we leave\n\n\nNow lets put a tester file in our directory and sync it to our bucket. We will remove it later to test if it is removed on sync.\n\n\nCode\nfile.create(here::here('posts/test.txt'))\n\n\n[1] TRUE\n\n\nRun our little function to sync the files to the bucket.\n\n\nCode\nsys_call()\n\n\nCompleted 237 Bytes/511.7 KiB (3.0 KiB/s) with 12 file(s) remaining\nupload: posts/_metadata.yml to s3://new-graphiti/posts/_metadata.yml\nCompleted 237 Bytes/511.7 KiB (3.0 KiB/s) with 11 file(s) remaining\nCompleted 2.0 KiB/511.7 KiB (11.9 KiB/s) with 11 file(s) remaining \nupload: posts/logos-equipment/index.qmd to s3://new-graphiti/posts/logos-equipment/index.qmd\nCompleted 2.0 KiB/511.7 KiB (11.9 KiB/s) with 10 file(s) remaining\nCompleted 3.6 KiB/511.7 KiB (20.8 KiB/s) with 10 file(s) remaining\nupload: posts/snakecase/index.qmd to s3://new-graphiti/posts/snakecase/index.qmd\nCompleted 3.6 KiB/511.7 KiB (20.8 KiB/s) with 9 file(s) remaining\nCompleted 8.6 KiB/511.7 KiB (48.5 KiB/s) with 9 file(s) remaining\nupload: posts/snakecase/thumbnail.jpg to s3://new-graphiti/posts/snakecase/thumbnail.jpg\nCompleted 8.6 KiB/511.7 KiB (48.5 KiB/s) with 8 file(s) remaining\nCompleted 13.9 KiB/511.7 KiB (74.3 KiB/s) with 8 file(s) remaining\nupload: posts/aws-storage-permissions/index.qmd to s3://new-graphiti/posts/aws-storage-permissions/index.qmd\nCompleted 13.9 KiB/511.7 KiB (74.3 KiB/s) with 7 file(s) remaining\nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 7 file(s) remaining\nupload: posts/aws-storage-processx/image.jpg to s3://new-graphiti/posts/aws-storage-processx/image.jpg\nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 6 file(s) remaining\nupload: posts/test.txt to s3://new-graphiti/posts/test.txt        \nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 5 file(s) remaining\nCompleted 26.0 KiB/511.7 KiB (119.1 KiB/s) with 5 file(s) remaining\nupload: posts/aws-storage-processx/index.rmarkdown to s3://new-graphiti/posts/aws-storage-processx/index.rmarkdown\nCompleted 26.0 KiB/511.7 KiB (119.1 KiB/s) with 4 file(s) remaining\nCompleted 34.0 KiB/511.7 KiB (152.0 KiB/s) with 4 file(s) remaining\nupload: posts/aws-storage-permissions/image.jpg to s3://new-graphiti/posts/aws-storage-permissions/image.jpg\nCompleted 34.0 KiB/511.7 KiB (152.0 KiB/s) with 3 file(s) remaining\nCompleted 42.2 KiB/511.7 KiB (180.5 KiB/s) with 3 file(s) remaining\nupload: posts/aws-storage-processx/index.qmd to s3://new-graphiti/posts/aws-storage-processx/index.qmd\nCompleted 42.2 KiB/511.7 KiB (180.5 KiB/s) with 2 file(s) remaining\nCompleted 83.0 KiB/511.7 KiB (256.0 KiB/s) with 2 file(s) remaining\nupload: posts/logos-equipment/image.jpg to s3://new-graphiti/posts/logos-equipment/image.jpg\nCompleted 83.0 KiB/511.7 KiB (256.0 KiB/s) with 1 file(s) remaining\nCompleted 339.0 KiB/511.7 KiB (830.9 KiB/s) with 1 file(s) remaining\nCompleted 511.7 KiB/511.7 KiB (654.4 KiB/s) with 1 file(s) remaining\nupload: posts/snakecase/all.jpeg to s3://new-graphiti/posts/snakecase/all.jpeg\nExit status: 0 \nOutput:\n Completed 237 Bytes/511.7 KiB (3.0 KiB/s) with 12 file(s) remaining\nupload: posts/_metadata.yml to s3://new-graphiti/posts/_metadata.yml\nCompleted 237 Bytes/511.7 KiB (3.0 KiB/s) with 11 file(s) remaining\nCompleted 2.0 KiB/511.7 KiB (11.9 KiB/s) with 11 file(s) remaining \nupload: posts/logos-equipment/index.qmd to s3://new-graphiti/posts/logos-equipment/index.qmd\nCompleted 2.0 KiB/511.7 KiB (11.9 KiB/s) with 10 file(s) remaining\nCompleted 3.6 KiB/511.7 KiB (20.8 KiB/s) with 10 file(s) remaining\nupload: posts/snakecase/index.qmd to s3://new-graphiti/posts/snakecase/index.qmd\nCompleted 3.6 KiB/511.7 KiB (20.8 KiB/s) with 9 file(s) remaining\nCompleted 8.6 KiB/511.7 KiB (48.5 KiB/s) with 9 file(s) remaining\nupload: posts/snakecase/thumbnail.jpg to s3://new-graphiti/posts/snakecase/thumbnail.jpg\nCompleted 8.6 KiB/511.7 KiB (48.5 KiB/s) with 8 file(s) remaining\nCompleted 13.9 KiB/511.7 KiB (74.3 KiB/s) with 8 file(s) remaining\nupload: posts/aws-storage-permissions/index.qmd to s3://new-graphiti/posts/aws-storage-permissions/index.qmd\nCompleted 13.9 KiB/511.7 KiB (74.3 KiB/s) with 7 file(s) remaining\nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 7 file(s) remaining\nupload: posts/aws-storage-processx/image.jpg to s3://new-graphiti/posts/aws-storage-processx/image.jpg\nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 6 file(s) remaining\nupload: posts/test.txt to s3://new-graphiti/posts/test.txt        \nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 5 file(s) remaining\nCompleted 26.0 KiB/511.7 KiB (119.1 KiB/s) with 5 file(s) remaining\nupload: posts/aws-storage-processx/index.rmarkdown to s3://new-graphiti/posts/aws-storage-processx/index.rmarkdown\nCompleted 26.0 KiB/511.7 KiB (119.1 KiB/s) with 4 file(s) remaining\nCompleted 34.0 KiB/511.7 KiB (152.0 KiB/s) with 4 file(s) remaining\nupload: posts/aws-storage-permissions/image.jpg to s3://new-graphiti/posts/aws-storage-permissions/image.jpg\nCompleted 34.0 KiB/511.7 KiB (152.0 KiB/s) with 3 file(s) remaining\nCompleted 42.2 KiB/511.7 KiB (180.5 KiB/s) with 3 file(s) remaining\nupload: posts/aws-storage-processx/index.qmd to s3://new-graphiti/posts/aws-storage-processx/index.qmd\nCompleted 42.2 KiB/511.7 KiB (180.5 KiB/s) with 2 file(s) remaining\nCompleted 83.0 KiB/511.7 KiB (256.0 KiB/s) with 2 file(s) remaining\nupload: posts/logos-equipment/image.jpg to s3://new-graphiti/posts/logos-equipment/image.jpg\nCompleted 83.0 KiB/511.7 KiB (256.0 KiB/s) with 1 file(s) remaining\nCompleted 339.0 KiB/511.7 KiB (830.9 KiB/s) with 1 file(s) remaining\nCompleted 511.7 KiB/511.7 KiB (654.4 KiB/s) with 1 file(s) remaining\nupload: posts/snakecase/all.jpeg to s3://new-graphiti/posts/snakecase/all.jpeg\n\n\nThen we can see our bucket contents - as well as list our bucket contents and capture them.\n\n\nCode\ns3fs::s3_dir_tree(bucket_path)\n\n\ns3://new-graphiti\n└── posts\n    ├── _metadata.yml\n    ├── test.txt\n    ├── aws-storage-permissions\n    │   ├── image.jpg\n    │   └── index.qmd\n    ├── aws-storage-processx\n    │   ├── image.jpg\n    │   ├── index.qmd\n    │   └── index.rmarkdown\n    ├── logos-equipment\n    │   ├── image.jpg\n    │   └── index.qmd\n    └── snakecase\n        ├── all.jpeg\n        ├── index.qmd\n        └── thumbnail.jpg\n\n\nCode\nt &lt;- s3fs::s3_dir_info(bucket_path, recurse = TRUE)\n\n\nNow we will remove test.txt\n\n\nCode\nfile.remove(here::here('posts/test.txt'))\n\n\n[1] TRUE\n\n\nNow we sync again.\n\n\nCode\nsys_call()\n\n\ndelete: s3://new-graphiti/posts/test.txt\nExit status: 0 \nOutput:\n delete: s3://new-graphiti/posts/test.txt\n\n\nList our bucket contents and capture them again\n\n\nCode\ns3_dir_tree(bucket_path)\n\n\ns3://new-graphiti\n└── posts\n    ├── _metadata.yml\n    ├── aws-storage-permissions\n    │   ├── image.jpg\n    │   └── index.qmd\n    ├── aws-storage-processx\n    │   ├── image.jpg\n    │   ├── index.qmd\n    │   └── index.rmarkdown\n    ├── logos-equipment\n    │   ├── image.jpg\n    │   └── index.qmd\n    └── snakecase\n        ├── all.jpeg\n        ├── index.qmd\n        └── thumbnail.jpg\n\n\nCode\nt2 &lt;- s3fs::s3_dir_info(bucket_path, recurse = TRUE)\n\n\nCompare the file structure before and after our sync.\n\n\nCode\nwaldo::compare(t$key, t2$key)\n\n\n     old                             | new                                 \n [9] \"posts/snakecase/all.jpeg\"      | \"posts/snakecase/all.jpeg\"      [9] \n[10] \"posts/snakecase/index.qmd\"     | \"posts/snakecase/index.qmd\"     [10]\n[11] \"posts/snakecase/thumbnail.jpg\" | \"posts/snakecase/thumbnail.jpg\" [11]\n[12] \"posts/test.txt\"                -                                     \n\n\nSuccess!!\n\n\nTo Do\nWe need to build the call to sync the other way (cloud to local) in a way that perhaps nukes local files if they are not on the cloud. This is because we need to collaborate within our team so we do things like one person will change the name of images so when the other person syncs they will have only the newly named image in their local directory.\n\nThis all deserved consideration as it could get really messy from a few different angles (ie. one person adds files they don’t want nuked and then they get nukes. There are lots of different options for doing things so we will get there.)\n\n\nDelete Bucket\nLets delete the bucket.\n\n\nCode\n#\nHere is the command line approach that we will turn off in favor of the s3fs approach.\nargs &lt;- c('s3', 'rb', bucket_path, '--force')\nsys_call()\n\n\n\n\nCode\n# Here is the `s3fs` way to \"delete\" all the versions.  \n# list all the files in the bucket\nfl &lt;- s3fs::s3_dir_ls(bucket_path, recurse = TRUE, refresh = TRUE)\n\n# list all the version info for all the files\nvi &lt;- fl |&gt; \n  purrr::map_df(s3fs::s3_file_version_info)\n\ns3fs::s3_file_delete(path = vi$uri)\n\n\n\n\nCode\ns3fs::s3_bucket_delete(bucket_path)\n\n\n[1] \"s3://new-graphiti\"\n\n\nAs we have tried this before we know that if we tell it we want to delete a bucket with versioned files in it we need to empty the bucket first including delete_markers. That is easy in the aws console with th UI but seems tricky. There is a bunch of discussion on options to this here https://stackoverflow.com/questions/29809105/how-do-i-delete-a-versioned-bucket-in-aws-s3-using-the-cli . Thinking a good way around it (and a topic for another post) would be to apply a lifecycle-configuration to the bucket that deletes all versions of files after a day - allowing you to delete bucket after they expire (as per the above post). Really we may want to have a lifecycle-configuration on all our versioned buckets to keep costs down anyway but deserves more thought and perhaps another post.\n\n\nCode\n# old notes\n# We are going to test creating a bucket with versioning on.  This has large implications for billing with some details\n# of how it works [here](https://aws.amazon.com/blogs/aws/amazon-s3-enhancement-versioning/) with example of costs [here](https://aws.amazon.com/s3/faqs/?nc1=h_ls).  Thinking we may want versioned buckets for things like `sqlite`\n# \"snapshot\" databases but definitely not for things like images."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "new graphiti",
    "section": "",
    "text": "Mapping and Plotting Precipitation with R\n\n\n\n\n\n\nprecipitation\n\n\nR\n\n\ndrought\n\n\nrayshader\n\n\n\n\n\n\n\n\n\nJun 19, 2024\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning up bib files flexibly with Zotero and R\n\n\n\n\n\n\nnews\n\n\nbibtex\n\n\nR\n\n\ncitations\n\n\n\n\n\n\n\n\n\nMay 27, 2024\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nSetting aws bucket permissions with R\n\n\n\n\n\n\naws\n\n\ns3\n\n\nr\n\n\npaws\n\n\n\n\n\n\n\n\n\nMay 24, 2024\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nSyncing files to aws with R\n\n\n\n\n\n\naws\n\n\ns3\n\n\nr\n\n\npaws\n\n\nprocessx\n\n\n\n\n\n\n\n\n\nMay 23, 2024\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nLogos and Equipment List Somewhere Accessible\n\n\n\n\n\n\nassets\n\n\n\n\n\n\n\n\n\nMay 7, 2024\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nLower snake_case vs Everything_Else\n\n\n\n\n\n\nnames\n\n\n\n\n\n\n\n\n\nMay 7, 2024\n\n\nal\n\n\n\n\n\n\nNo matching items"
  }
]