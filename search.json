[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "new graphiti",
    "section": "",
    "text": "Setting up TiTiler to serve COGs of UAV imagery on AWS with leaflet and Elastic Beanstalk\n\n\n\n\n\n\naws\n\n\ns3\n\n\ns3sf\n\n\nleaflet\n\n\nCOG\n\n\ntitiler\n\n\n\n\n\n\n\n\n\nJan 17, 2025\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nGetting details of historic orthophoto imagery with R\n\n\n\n\n\n\nfwapg\n\n\nr\n\n\nbcdata\n\n\nimagery\n\n\napi\n\n\n\n\n\n\n\n\n\nNov 15, 2024\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nCORS for serving of COGs of UAV imagery on AWS with R\n\n\n\n\n\n\naws\n\n\ns3\n\n\nr\n\n\npaws\n\n\ns3sf\n\n\nleaflet\n\n\nleafem\n\n\nCOG\n\n\nCORS\n\n\n\n\n\n\n\n\n\nSep 21, 2024\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nMapping Land Cover with R\n\n\n\n\n\n\nland cover\n\n\nR\n\n\nplanetary computer\n\n\nremote sensing\n\n\n\n\n\n\n\n\n\nJul 1, 2024\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nMapping and Plotting Precipitation with R\n\n\n\n\n\n\nprecipitation\n\n\nR\n\n\ndrought\n\n\nrayshader\n\n\n\n\n\n\n\n\n\nJun 19, 2024\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning up bib files flexibly with Zotero and R\n\n\n\n\n\n\nnews\n\n\nbibtex\n\n\nR\n\n\ncitations\n\n\n\n\n\n\n\n\n\nMay 27, 2024\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nSetting aws bucket permissions with R\n\n\n\n\n\n\naws\n\n\ns3\n\n\nr\n\n\npaws\n\n\n\n\n\n\n\n\n\nMay 24, 2024\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nSyncing files to aws with R\n\n\n\n\n\n\naws\n\n\ns3\n\n\nr\n\n\npaws\n\n\nprocessx\n\n\n\n\n\n\n\n\n\nMay 23, 2024\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nLogos and Equipment List Somewhere Accessible\n\n\n\n\n\n\nassets\n\n\n\n\n\n\n\n\n\nMay 7, 2024\n\n\nal\n\n\n\n\n\n\n\n\n\n\n\n\nLower snake_case vs Everything_Else\n\n\n\n\n\n\nnames\n\n\n\n\n\n\n\n\n\nMay 7, 2024\n\n\nal\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-05-27-references-bib-succinct/index.html",
    "href": "posts/2024-05-27-references-bib-succinct/index.html",
    "title": "Cleaning up bib files flexibly with Zotero and R",
    "section": "",
    "text": "Here we will clean up a bib file exported from Zotero using R to contain only the entries found in a Rmarkdown file.\nCode\nlibrary(here)\n\n\nhere() starts at /Users/airvine/Projects/repo/new_graphiti\n\n\nCode\nlibrary(stringr)\nlibrary(knitr)\n\n# get the name of this post directory\npost_dir &lt;- paste0(here::here(), \"/posts/\", params$post_dir_name)\npost_dir_fig &lt;- paste0(post_dir, \"/fig/\")\nLet’s define our .bib files\nCode\nrmd_file &lt;- \"~/Projects/repo/fish_passage_peace_2023_reporting/fish_passage_peace_2023_reporting.Rmd\"\nbib_file &lt;- paste0(here::here(), \"/assets/NewGraphEnvironment.bib\")\noutput_file &lt;- \"~/Projects/repo/fish_passage_peace_2023_reporting/references.bib\"\nHere are the functions:\nCode\n# Function to extract citations from an RMarkdown file\nbib_extract_citations &lt;- function(rmd_file, additional_citations = c()) {\n  # Read the entire RMarkdown file\n  lines &lt;- readLines(rmd_file)\n  \n  # Concatenate all lines into a single string\n  text &lt;- paste(lines, collapse = \" \")\n  \n  # Regular expression to find citations in the form of @this_citation or [@that_citation; @another_citation]\n  citation_pattern &lt;- \"@[a-zA-Z0-9_:-]+\"\n  \n  # Extract all citations\n  citations &lt;- str_extract_all(text, citation_pattern)[[1]]\n  \n  # Remove the leading '@' from the citations\n  citations &lt;- unique(sub(\"^@\", \"\", citations))\n  \n  # Combine with additional citations\n  all_citations &lt;- unique(c(citations, additional_citations))\n  \n  return(all_citations)\n}\n\n# Function to clean a BibTeX file to keep only cited entries\nbib_clean &lt;- function(bib_file, citations, output_file) {\n  file.create(output_file)\n  # Read the entire BibTeX file\n  lines &lt;- readLines(bib_file)\n  \n  # Initialize variables\n  keep_entry &lt;- FALSE\n  cleaned_lines &lt;- c()\n  \n  for (line in lines) {\n    # Check if the line starts a new citation entry\n    if (grepl(\"^@\", line)) {\n      # Extract the citation key\n      citation_key &lt;- sub(\"^@.*\\\\{([^,]+),.*\", \"\\\\1\", line)\n      \n      # Determine if the entry should be kept\n      keep_entry &lt;- citation_key %in% citations\n    }\n    \n    # Add the line to cleaned_lines if the entry is to be kept\n    if (keep_entry) {\n      cleaned_lines &lt;- c(cleaned_lines, line)\n    }\n  }\n  \n  # Write the cleaned lines to the output file\n  writeLines(cleaned_lines, output_file)\n  \n  cat(\"Cleaned BibTeX file created:\", output_file, \"\\n\")\n}\nAs a big part of the motivation to do this is to reduce the bloat in our repositories we will add the default name of the bib file to the .gitignore of this repo.\nCode\nknitr::include_graphics(paste0(post_dir_fig, \"Screen Shot 2024-05-27 at 1.40.44 PM.png\"))\n\n\n\n\n\n\n\n\n\nCode\nknitr::include_graphics(paste0(post_dir_fig, \"Screen Shot 2024-05-27 at 1.40.55 PM.png\"))"
  },
  {
    "objectID": "posts/2024-05-27-references-bib-succinct/index.html#extract-citations-from-the-rmarkdown-file",
    "href": "posts/2024-05-27-references-bib-succinct/index.html#extract-citations-from-the-rmarkdown-file",
    "title": "Cleaning up bib files flexibly with Zotero and R",
    "section": "Extract citations from the RMarkdown file",
    "text": "Extract citations from the RMarkdown file\n\n\nCode\n# Extract citations from the RMarkdown file\ncitations &lt;- bib_extract_citations(rmd_file, additional_citations = nocites)"
  },
  {
    "objectID": "posts/2024-05-27-references-bib-succinct/index.html#clean-the-bibtex-file-to-keep-only-cited-entries",
    "href": "posts/2024-05-27-references-bib-succinct/index.html#clean-the-bibtex-file-to-keep-only-cited-entries",
    "title": "Cleaning up bib files flexibly with Zotero and R",
    "section": "Clean the BibTeX file to keep only cited entries",
    "text": "Clean the BibTeX file to keep only cited entries\n\n\nCode\nbib_clean(bib_file, citations, output_file)\n\n\nCleaned BibTeX file created: ~/Projects/repo/fish_passage_peace_2023_reporting/references.bib"
  },
  {
    "objectID": "posts/2024-06-19-precipitation/index.html",
    "href": "posts/2024-06-19-precipitation/index.html",
    "title": "Mapping and Plotting Precipitation with R",
    "section": "",
    "text": "Really interested in quantifying and visualizing weather data for specific areas that we are working…. Here is a first start.\n\nWARNING - this work is stolen!! I have adapted this from a repository on GitHub from the wonderfully talented Milos Popovic. All credit to Milos! What a boss - really awesome stuff.\n\nAlso of note is the image used for the blog. That is Cotey Gallagher… I hope she doesn’t sue me. https://www.linkedin.com/pulse/how-crazy-would-could-really-rain-cats-dogs-cotey-gallagher/\n\nFirst thing we will do is load our packages. If you do not have the packages installed yet you can change the update_pkgs param in the yml of this file to TRUE. Using pak is great because it allows you to update your packages when you want to.\n\n\nCode\npkgs_cran &lt;- c(\n  \"here\",\n  \"fs\",\n  \"pRecipe\",\n  \"giscoR\",\n  \"terra\",\n  \"tidyverse\",\n  \"rayshader\",\n  \"sf\",\n  \"classInt\",\n  \"rgl\")\n\npkgs_gh &lt;- c(\n  \"poissonconsulting/pgfeatureserv\",\n  \"poissonconsulting/fwapgr\",\n  \"NewGraphEnvironment/rfp\"\n  # we will turn this off since the function it uses won't run for folks without db credentials\n  # \"NewGraphEnvironment/fpr\"\n )\n\npkgs &lt;- c(pkgs_cran, pkgs_gh)\n\nif(params$update_pkgs){\n  # install the pkgs\n  lapply(pkgs,\n         pak::pkg_install,\n         ask = FALSE)\n}\n\n# load the pkgs\npkgs_ld &lt;- c(pkgs_cran,\n             basename(pkgs_gh))\ninvisible(\n  lapply(pkgs_ld,\n       require,\n       character.only = TRUE)\n)\n\n\n\nDefine our Area of Interest\nHere we diverge a bit from Milos version as we are going to load a custom area of interest. We will be connecting to our remote database using Poisson Consulting’s fwapgr::fwa_watershed_at_measure function which leverages the in database FWA_WatershedAtMeasure function from Simon Norris’ wonderful fwapg package.\n\nWe use a blue line key and a downstream route measure to define our area of interest which is the Neexdzii Kwa (a.k.a Upper Bulkley River) near Houston, British Columbia.\n\nAs per the Freshwater Atlas of BC - the blue line key:\n\nUniquely identifies a single flow line such that a main channel and a secondary channel with the same watershed code would have different blue line keys (the Fraser River and all side channels have different blue line keys).\n\n\nA downstream route measure is:\n\nThe distance, in meters, along the route from the mouth of the route to the feature. This distance is measured from the mouth of the containing route to the downstream end of the feature.\n\n\n\nCode\n# lets build a custom watersehed just for upstream of the confluence of Neexdzii Kwa and Wetzin Kwa\n# blueline key\nblk &lt;- 360873822\n# downstream route measure\ndrm &lt;- 166030.4\n\naoi &lt;- fwapgr::fwa_watershed_at_measure(blue_line_key = blk, \n                                        downstream_route_measure = drm) |&gt; \n  sf::st_transform(4326)\n\n\n\n\nRetrieve the Precipitation Data\nFor this example we will retrieve our precipitation data from Multi-Source Weighted-Ensemble Precipitation using the pRecipe package.\n\n\nCode\n# let's create our data directory\ndir_data &lt;- here::here('posts', params$post_dir_name, \"data\")\n\nfs::dir_create(dir_data)\n\n\nTo actually download the data we are going to put a chunk option that allows us to just execute the code once and update it with the update_gis param in our yml. We will use usethis::use_git_ignore to add the data to our .gitignore file so that we do not commit that insano enourmouse file to our git repository.\n\n\nCode\npRecipe::download_data(\n    dataset = \"mswep\",\n    path = dir_data,\n    domain = \"raw\",\n    timestep = \"yearly\"\n)\n\nusethis::use_git_ignore(paste0('posts/', params$post_dir_name, \"/data/*\"))\n\n\nNow we read in our freshly downloaded .nc file and clip to our area of interest.\n\n\nCode\n# get the name of the file with a .nc at the end\nnc_file &lt;- fs::dir_ls(dir_data, glob = \"*.nc\")\n\nmswep_data &lt;- terra::rast(\n  nc_file\n) |&gt;\nterra::crop(\n    aoi\n)\n\n\nNext we extract the years of the data from the filename of the .nc file and then transform the data into a dataframe. We need to remove the data from 2023 because it is only for January as per the filename:\nmswep_tp_mm_global_197902_202301_025_yearly.nc\n\n\nCode\n# the names of the datasets are arbitrary (precipitation_1:precipitation_45) \n# we will rename the datasets to the years.  \n# here we extract 2023 from the nc_file name of the file using regex\nyear_end &lt;- as.numeric(stringr::str_extract(basename(nc_file), \"(?&lt;=_\\\\d{6}_)\\\\d{4}\"))\nyear_start &lt;- as.numeric(stringr::str_extract(basename(nc_file), \"(?&lt;=_)[0-9]{4}(?=[0-9]{2}_[0-9]{6}_)\"))\n\n# assign the names to replace \nnames(mswep_data) &lt;- year_start:year_end\n\n\nmswep_df &lt;- mswep_data |&gt;\n    as.data.frame(xy = TRUE) |&gt;\n    tidyr::pivot_longer(\n        !c(\"x\", \"y\"),\n        names_to = \"year\",\n        values_to = \"precipitation\"\n    ) |&gt; \n  # 2023 is not complete so we remove it\n    dplyr::filter(year != 2023)\n\n\n\n\nGet Additional Data\nWe could use some data for context such as major streams, highways and the railway. We get the streams and railway from data distribution bc api using the bcdata package. Our rfp package calls just allow some extra sanity checks on the bcdata::bcdc_query_geodata function. It’s not really necessary but can be helpful when errors occur (ex. the name of the column to filter on is input incorrectly).\n\n\n\nCode\n# grab all the railways\nl_rail &lt;- rfp::rfp_bcd_get_data(\n    bcdata_record_id = stringr::str_to_upper(\"whse_basemapping.gba_railway_tracks_sp\")\n) |&gt; \n  sf::st_transform(4326) |&gt; \n  janitor::clean_names() \n\n# streams in the bulkley and then filter to just keep the big ones\nl_streams &lt;- rfp::rfp_bcd_get_data(\n  bcdata_record_id = stringr::str_to_upper(\"whse_basemapping.fwa_stream_networks_sp\"),\n  col_filter = stringr::str_to_upper(\"watershed_group_code\"),\n  col_filter_value = \"BULK\",\n  # grab a smaller object by including less columns\n  col_extract = stringr::str_to_upper(c(\"linear_feature_id\", \"stream_order\"))\n) |&gt; \n  sf::st_transform(4326) |&gt; \n  janitor::clean_names() |&gt; \n  dplyr::filter(stream_order &gt; 4)\n\n\nBecause the highways we use in our mapping are not available for direct download from the Data Distribution BC api (some other versions are here we will query them from our remote database. The function used (fpr::fpr_db_query) is a wrapper around the DBI::dbGetQuery function that allows us to query our remote database by calling our environmental variables and making a connection. This will not work without the proper credentials so if you were trying to reproduce this and don’t have the credentials you won’t be able to retrieve the roads. To get around this we have stored the trimmed roads data in the data directory of this post so we can read it in from there.\n\n\nCode\n# highways\n# define the type of roads we want to include using the transport_line_type_code. We will include RA1 and RH1 (Road arerial/highway major)\nrd_codes &lt;- c(\"RA1\", \"RH1\")\nl_rds &lt;- fpr::fpr_db_query(\n  query = glue::glue(\"SELECT transport_line_id, structured_name_1, transport_line_type_code, geom FROM whse_basemapping.transport_line WHERE transport_line_type_code IN ({glue::glue_collapse(glue::single_quote(rd_codes), sep = ', ')})\")\n  )|&gt; \n  sf::st_transform(4326) \n\n\nsf::st_intersection(l_rds, \n                    # we will remove all the aoi columns except the geometry so we don't get all the aoi columns appended\n                    aoi |&gt; dplyr::select(geometry)) |&gt; \n  sf::st_write(here::here('posts', params$post_dir_name, \"data\", \"l_rds.gpkg\"), delete_dsn = TRUE)\n\n\nNow we trim up all those layers. We have some functions to validate and repair geometries and then we clip them to our area of interest.\n\n\nCode\n# we don't actually need to trim the rds since we already did that but for simplicity we will do it again\n  l_rds &lt;- sf::st_read(here::here('posts', params$post_dir_name, \"data\", \"l_rds.gpkg\"), quiet = TRUE) \n\n\nlayers_to_trim &lt;- tibble::lst(l_rail, l_streams, l_rds)\n\n# Function to validate and repair geometries\nvalidate_geometries &lt;- function(layer) {\n  layer &lt;- sf::st_make_valid(layer)\n  layer &lt;- layer[sf::st_is_valid(layer), ]\n  return(layer)\n}\n\n# Apply validation to the AOI and layers\naoi &lt;- validate_geometries(aoi)\nlayers_to_trim &lt;- purrr::map(layers_to_trim, validate_geometries)\n\n# clip them  with purrr and sf\nlayers_trimmed &lt;- purrr::map(\n  layers_to_trim,\n  ~ sf::st_intersection(.x, aoi)\n) \n\n\n\n\nPlot the Precipitation Data by Year\nFirst thing we do here is highjack the plot function from Milos.\n\n\nCode\ntheme_for_the_win &lt;- function(){\n    theme_minimal() +\n    theme(\n        axis.line = element_blank(),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        legend.position = \"right\",\n        legend.title = element_text(\n            size = 11, color = \"grey10\"\n        ),\n        legend.text = element_text(\n            size = 10, color = \"grey10\"\n        ),\n        panel.grid.major = element_line(\n            color = NA\n        ),\n        panel.grid.minor = element_line(\n            color = NA\n        ),\n        plot.background = element_rect(\n            fill = NA, color = NA\n        ),\n        legend.background = element_rect(\n            fill = \"white\", color = NA\n        ),\n        panel.border = element_rect(\n            fill = NA, color = NA\n        ),\n        plot.margin = unit(\n            c(\n                t = 0, r = 0,\n                b = 0, l = 0\n            ), \"lines\"\n        )\n    )\n}\n\nbreaks &lt;- classInt::classIntervals(\n    mswep_df$precipitation,\n    n = 5,\n    style = \"equal\"\n)$brks\n\ncolors &lt;- hcl.colors(\n    n = length(breaks),\n    palette = \"Temps\",\n    rev = TRUE\n)\n\n\nNow we plot the data by year.\n\n\nCode\nmap1 &lt;- ggplot(\n    data = mswep_df\n) +\ngeom_raster(\n    aes(\n        x = x,\n        y = y,\n        fill = precipitation\n    )\n) +\ngeom_contour(\n    aes(\n       x = x,\n       y = y,\n       z = precipitation\n    ), color = \"white\" # add this line\n) +\ngeom_sf(\n    data = aoi,\n    fill = \"transparent\",\n    color = \"grey10\",\n    size = .5\n) +\nscale_fill_gradientn(\n    name = \"mm\",\n    colors = colors,\n    breaks = breaks,\n    labels = round(breaks, 0), # use round(breaks, 0)\n    limits = c(\n        min(mswep_df$precipitation),\n        max(mswep_df$precipitation)\n    )\n) +\nfacet_wrap(~year) +\nguides(\n    fill = guide_colourbar(\n        direction = \"vertical\",\n        barheight = unit(50, \"mm\"),\n        barwidth = unit(5, \"mm\"),\n        title.position = \"top\",\n        label.position = \"right\",\n        title.hjust = .5,\n        label.hjust = .5,\n        ncol = 1,\n        byrow = FALSE\n    )\n) +\ntheme_for_the_win()\n\nmap1\n\n\n\n\n\n\n\n\n\nPretty cool. Interesting to see really wet and dry years in the last 20 years or so such as the wet ones in 2004, 2007, 2011 and 2020 and the dry ones in 2000, 2010, 2014, 2021 and 2022. The contours on the maps are really interesting as they show the gradients which generally run west to east - but occasionally run south to north.\n\n\nAverage Precipitation\nNow we will average all the years together to get an average precipitation map. We will add our additional layers for context too. Roads are black, railways are yellow and streams are blue.\n\n\nCode\nmswep_average_df &lt;- mswep_df |&gt;\n  dplyr::group_by(\n    x, y, .drop = FALSE\n  ) |&gt;\n  dplyr::summarise(\n    precipitation = mean(precipitation)\n  ) |&gt; \n  dplyr::mutate(year = \"average\")\n\n\nbreaks &lt;- classInt::classIntervals(\n  mswep_average_df$precipitation,\n  n = 5,\n  style = \"equal\"\n)$brks\n\n\ncolors &lt;- hcl.colors(\n  n = length(breaks),\n  palette = \"Temps\",\n  rev = TRUE\n)\n\nmap_average &lt;- ggplot(\n  data = mswep_average_df\n) +\n  geom_raster(\n    aes(\n      x = x,\n      y = y,\n      fill = precipitation\n    )\n  ) +\n  geom_contour(\n    aes(\n      x = x,\n      y = y,\n      z = precipitation\n    ), color = \"white\" # add this line\n  ) +\n  geom_sf(\n    data = aoi,\n    fill = \"transparent\",\n    color = \"grey10\",\n    size = .5\n  ) +\n  scale_fill_gradientn(\n    name = \"mm\",\n    colors = colors,\n    breaks = breaks,\n    labels = round(breaks, 0), # use round(breaks, 0)\n    limits = c(\n      min(mswep_average_df$precipitation),\n      max(mswep_average_df$precipitation)\n    )\n  ) +\n  guides(\n    fill = guide_colourbar(\n      direction = \"vertical\",\n      barheight = unit(50, \"mm\"),\n      barwidth = unit(5, \"mm\"),\n      title.position = \"top\",\n      label.position = \"right\",\n      title.hjust = .5,\n      label.hjust = .5,\n      ncol = 1,\n      byrow = FALSE\n    )\n  ) +\n  geom_sf(\n    data = layers_trimmed$l_rds,\n    color = \"black\",\n    size = .8\n  ) +\n  geom_sf(\n    data = layers_trimmed$l_streams,\n    color = \"blue\",\n    size = .8\n  ) +\n  geom_sf(\n    data = layers_trimmed$l_rail,\n    color = \"yellow\",\n    size = .8\n  ) +\n  theme_for_the_win()\n\nmap_average\n\n\n\n\n\n\n\n\n\n\n\nCompare the Average Precipitation to a Specific Year\nWe often talk about a “dry” year or a “wet” year. Let’s compare the average precipitation to a specific year. We will build a little function to do this so that we can easily compare any year to the average.\n\n\nCode\nmap_vs_average &lt;- function(year_compare){\n  \n  mswep_df_2022 &lt;- mswep_df |&gt;\n    dplyr::filter(year == year_compare) |&gt; \n    dplyr::mutate(year = as.character(year))\n  \n  mswep_df_compare &lt;- bind_rows(mswep_average_df, mswep_df_2022)\n  \n  breaks &lt;- classInt::classIntervals(\n    mswep_df_compare$precipitation,\n    n = 5,\n    style = \"equal\"\n  )$brks\n  \n  colors &lt;- hcl.colors(\n    n = length(breaks),\n    palette = \"Temps\",\n    rev = TRUE\n  )\n  \n  ggplot(\n    data = mswep_df_compare\n  ) +\n    facet_wrap(~year) +\n    geom_raster(\n      aes(\n        x = x,\n        y = y,\n        fill = precipitation\n      )\n    ) +\n    geom_contour(\n      aes(\n        x = x,\n        y = y,\n        z = precipitation\n      ), color = \"white\" # add this line\n    ) +\n    geom_sf(\n      data = aoi,\n      fill = \"transparent\",\n      color = \"grey10\",\n      size = .5\n    ) +\n    scale_fill_gradientn(\n      name = \"mm\",\n      colors = colors,\n      breaks = breaks,\n      labels = round(breaks, 0), # use round(breaks, 0)\n      limits = c(\n        min(mswep_df_compare$precipitation),\n        max(mswep_df_compare$precipitation)\n      )\n    ) +\n    guides(\n      fill = guide_colourbar(\n        direction = \"vertical\",\n        barheight = unit(50, \"mm\"),\n        barwidth = unit(5, \"mm\"),\n        title.position = \"top\",\n        label.position = \"right\",\n        title.hjust = .5,\n        label.hjust = .5,\n        ncol = 1,\n        byrow = FALSE\n      )\n    ) +\n    geom_sf(\n      data = layers_trimmed$l_rds,\n      color = \"black\",\n      size = .8\n    ) +\n    geom_sf(\n      data = layers_trimmed$l_streams,\n      color = \"blue\",\n      size = .8\n    ) +\n    geom_sf(\n      data = layers_trimmed$l_rail,\n      color = \"yellow\",\n      size = .8\n    ) +\n    theme_for_the_win()\n}\n\n\nFirst let’s check out 2022 as it seemed pretty dry.\n\n\nCode\nmap_vs_average(2022)\n\n\n\n\n\n\n\n\n\nNow let’s look at 2020 as that seemed wet with lots of streams flowing really nicely.\n\n\nCode\nmap_vs_average(2020)\n\n\n\n\n\n\n\n\n\nDefinitely wetter than average.\n\n\n3D Contour Map\nLet’s make an interactive 3D contour map of the average precipitation data. Use the mouse to rotate the map and zoom in and out!\n\n\nCode\n{\n  \n  rayshader::plot_gg(\n    ggobj = map_average,\n    width = 7,\n    height = 7,\n    scale = 250,\n    solid = FALSE,\n    shadow = TRUE,\n    shadowcolor = \"white\",\n    shadowwidth = 0,\n    shadow_intensity = 1,\n    # window.size = c(600, 600),\n    zoom = .7,\n    phi = 30,\n    theta = 337.5\n    # fov =30\n  )\n  rgl::rglwidget(width = 982, height = 1025, reuse = FALSE)\n}\n\n\n\n\n\n\nThanks Miles!"
  },
  {
    "objectID": "posts/aws-storage-processx/index.html",
    "href": "posts/aws-storage-processx/index.html",
    "title": "Syncing files to aws with R",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, dpi=60, out.width = \"100%\")\noptions(scipen=999)\noptions(knitr.kable.NA = '--') #'--'\noptions(knitr.kable.NAN = '--')\n\n\nInspired by https://blog.djnavarro.net/posts/2022-03-17_using-aws-s3-in-r/ by Danielle Navarro.\nNote to self - /Users/airvine/Projects/repo/new_graphiti/_freeze/posts/aws-storage-processx/index/execute-results/html.json is created when I render this document. This seems to be what is published to website after 1. the github_actions workflow is run to generate the gh-pages branch (on github runner) 2. the site is published with gitpages from github.\n“Quick” post to document where I got to with syncing files to aws with R. Didn’t love the aws.s3::sync function because from what I could tell I could not tell it to delete files if they were not present locally or in a bucket (I could be wrong).\nThen climbed into s3fs which mirrors the fs package and seems a bit more user friendly than the aws.s3 package for managing files. It is created by Dyfan Jones who also is the top contributor to paws!! He seems like perhaps as much of a beast as one of the contributors to s3fs who is Scott Chamberlain.\nFor the sync issue figured why not just call the aws command line tool from R. processx is an insane package that might be the mother of all packages. It allows you to run command line tools from R with flexibility for some things like setting the directory where the command is called from in the processx called function (big deal as far as I can tell).\nWe need to set up our aws account online. The blog above from Danielle Navarro covers that I believe (I struggled through it a long time ago). I should use a ~/.aws/credentials file but don’t yet. I have my credentials in my ~/.Renviron file as well as in my ~/.bash_profile (probably a ridiculous setup). They are:\nAWS_ACCESS_KEY_ID='my_key'\nAWS_DEFAULT_REGION='my_region'\nAWS_SECRET_ACCESS_KEY='my_secret_key'\n\n\nCode\n# library(aws.s3)\nlibrary(processx)\n# library(paws) #this is the mom.  Couple examples of us hashed out here\nlibrary(s3fs)\n# library(aws.iam) #not useing - set permissions\nlibrary(here) #helps us with working directory issues related to the `environment` we operate in when rendering\n\n\n\nSee buckets using the s3fs package.\n\nCurrent buckets are:\n\n\nCode\ns3fs::s3_dir_ls(refresh = TRUE) \n\n\n[1] \"s3://23cog\"\n\n\n\n\nCode\n# First we set up our AWS s3 file system. I am actually not sure this is necessary but I did it.  Will turn the chunk off\n# to not repeat.\n# s3fs::s3_file_system(profile_name = \"s3fs_example\")\n\n\n\n\nCreate a Bucket\nLet’s generate the name of the bucket based on the name of the repo but due to aws bucket naming rules we need to swap out our underscores for hyphens! Maybe a good enough reason to change our naming conventions for our repos on github!!\n\n\nCode\nbucket_name &lt;- basename(here::here()) |&gt; \n  stringr::str_replace_all(\"_\", \"-\") \n\nbucket_path &lt;- s3fs::s3_path(bucket_name)\n\ns3fs::s3_bucket_create( bucket_path)  \n\n\n[1] \"s3://new-graphiti\"\n\n\n\n\nSync Files to Bucket\nWe build a little wrapper function to help us debug issues when running system commands with processx.\n\n\nCode\nsys_call &lt;- function(){\n  result &lt;- tryCatch({\n    processx::run(\n      command,\n      args = args,\n      echo = TRUE,            # Print the command output live\n      wd = working_directory, # Set the working directory\n      spinner = TRUE,         # Show a spinner\n      timeout = 60            # Timeout after 60 seconds\n    )\n  }, error = function(e) {\n    # Handle errors: e.g., print a custom error message\n    cat(\"An error occurred: \", e$message, \"\\n\")\n    NULL  # Return NULL or another appropriate value\n  })\n  \n  # Check if the command was successful\n  if (!is.null(result)) {\n    cat(\"Exit status:\", result$status, \"\\n\")\n    cat(\"Output:\\n\", result$stdout)\n  } else {\n    cat(\"Failed to execute the command properly.\\n\")\n  }\n}\n\n\n\nThen we specify our command and arguments. To achieve the desired behavior of including only files in the assets/* directory, you need to combine the order of --exclude and --include flags appropriately (exclude everything first thenn include what we want):\n\n\nCode\ncommand &lt;- \"aws\"\nargs &lt;- c('s3', 'sync', '.', bucket_path, '--delete', '--exclude', '*', '--include', 'posts/*')\n\nworking_directory = here::here() #we could just remove from funciton to get the current wd but its nice to have so we leave\n\n\nNow lets put a tester file in our directory and sync it to our bucket. We will remove it later to test if it is removed on sync.\n\n\nCode\nfile.create(here::here('posts/test.txt'))\n\n\n[1] TRUE\n\n\nRun our little function to sync the files to the bucket.\n\n\nCode\nsys_call()\n\n\nCompleted 237 Bytes/511.7 KiB (3.0 KiB/s) with 12 file(s) remaining\nupload: posts/_metadata.yml to s3://new-graphiti/posts/_metadata.yml\nCompleted 237 Bytes/511.7 KiB (3.0 KiB/s) with 11 file(s) remaining\nCompleted 2.0 KiB/511.7 KiB (11.9 KiB/s) with 11 file(s) remaining \nupload: posts/logos-equipment/index.qmd to s3://new-graphiti/posts/logos-equipment/index.qmd\nCompleted 2.0 KiB/511.7 KiB (11.9 KiB/s) with 10 file(s) remaining\nCompleted 3.6 KiB/511.7 KiB (20.8 KiB/s) with 10 file(s) remaining\nupload: posts/snakecase/index.qmd to s3://new-graphiti/posts/snakecase/index.qmd\nCompleted 3.6 KiB/511.7 KiB (20.8 KiB/s) with 9 file(s) remaining\nCompleted 8.6 KiB/511.7 KiB (48.5 KiB/s) with 9 file(s) remaining\nupload: posts/snakecase/thumbnail.jpg to s3://new-graphiti/posts/snakecase/thumbnail.jpg\nCompleted 8.6 KiB/511.7 KiB (48.5 KiB/s) with 8 file(s) remaining\nCompleted 13.9 KiB/511.7 KiB (74.3 KiB/s) with 8 file(s) remaining\nupload: posts/aws-storage-permissions/index.qmd to s3://new-graphiti/posts/aws-storage-permissions/index.qmd\nCompleted 13.9 KiB/511.7 KiB (74.3 KiB/s) with 7 file(s) remaining\nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 7 file(s) remaining\nupload: posts/aws-storage-processx/image.jpg to s3://new-graphiti/posts/aws-storage-processx/image.jpg\nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 6 file(s) remaining\nupload: posts/test.txt to s3://new-graphiti/posts/test.txt        \nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 5 file(s) remaining\nCompleted 26.0 KiB/511.7 KiB (119.1 KiB/s) with 5 file(s) remaining\nupload: posts/aws-storage-processx/index.rmarkdown to s3://new-graphiti/posts/aws-storage-processx/index.rmarkdown\nCompleted 26.0 KiB/511.7 KiB (119.1 KiB/s) with 4 file(s) remaining\nCompleted 34.0 KiB/511.7 KiB (152.0 KiB/s) with 4 file(s) remaining\nupload: posts/aws-storage-permissions/image.jpg to s3://new-graphiti/posts/aws-storage-permissions/image.jpg\nCompleted 34.0 KiB/511.7 KiB (152.0 KiB/s) with 3 file(s) remaining\nCompleted 42.2 KiB/511.7 KiB (180.5 KiB/s) with 3 file(s) remaining\nupload: posts/aws-storage-processx/index.qmd to s3://new-graphiti/posts/aws-storage-processx/index.qmd\nCompleted 42.2 KiB/511.7 KiB (180.5 KiB/s) with 2 file(s) remaining\nCompleted 83.0 KiB/511.7 KiB (256.0 KiB/s) with 2 file(s) remaining\nupload: posts/logos-equipment/image.jpg to s3://new-graphiti/posts/logos-equipment/image.jpg\nCompleted 83.0 KiB/511.7 KiB (256.0 KiB/s) with 1 file(s) remaining\nCompleted 339.0 KiB/511.7 KiB (830.9 KiB/s) with 1 file(s) remaining\nCompleted 511.7 KiB/511.7 KiB (654.4 KiB/s) with 1 file(s) remaining\nupload: posts/snakecase/all.jpeg to s3://new-graphiti/posts/snakecase/all.jpeg\nExit status: 0 \nOutput:\n Completed 237 Bytes/511.7 KiB (3.0 KiB/s) with 12 file(s) remaining\nupload: posts/_metadata.yml to s3://new-graphiti/posts/_metadata.yml\nCompleted 237 Bytes/511.7 KiB (3.0 KiB/s) with 11 file(s) remaining\nCompleted 2.0 KiB/511.7 KiB (11.9 KiB/s) with 11 file(s) remaining \nupload: posts/logos-equipment/index.qmd to s3://new-graphiti/posts/logos-equipment/index.qmd\nCompleted 2.0 KiB/511.7 KiB (11.9 KiB/s) with 10 file(s) remaining\nCompleted 3.6 KiB/511.7 KiB (20.8 KiB/s) with 10 file(s) remaining\nupload: posts/snakecase/index.qmd to s3://new-graphiti/posts/snakecase/index.qmd\nCompleted 3.6 KiB/511.7 KiB (20.8 KiB/s) with 9 file(s) remaining\nCompleted 8.6 KiB/511.7 KiB (48.5 KiB/s) with 9 file(s) remaining\nupload: posts/snakecase/thumbnail.jpg to s3://new-graphiti/posts/snakecase/thumbnail.jpg\nCompleted 8.6 KiB/511.7 KiB (48.5 KiB/s) with 8 file(s) remaining\nCompleted 13.9 KiB/511.7 KiB (74.3 KiB/s) with 8 file(s) remaining\nupload: posts/aws-storage-permissions/index.qmd to s3://new-graphiti/posts/aws-storage-permissions/index.qmd\nCompleted 13.9 KiB/511.7 KiB (74.3 KiB/s) with 7 file(s) remaining\nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 7 file(s) remaining\nupload: posts/aws-storage-processx/image.jpg to s3://new-graphiti/posts/aws-storage-processx/image.jpg\nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 6 file(s) remaining\nupload: posts/test.txt to s3://new-graphiti/posts/test.txt        \nCompleted 17.8 KiB/511.7 KiB (82.4 KiB/s) with 5 file(s) remaining\nCompleted 26.0 KiB/511.7 KiB (119.1 KiB/s) with 5 file(s) remaining\nupload: posts/aws-storage-processx/index.rmarkdown to s3://new-graphiti/posts/aws-storage-processx/index.rmarkdown\nCompleted 26.0 KiB/511.7 KiB (119.1 KiB/s) with 4 file(s) remaining\nCompleted 34.0 KiB/511.7 KiB (152.0 KiB/s) with 4 file(s) remaining\nupload: posts/aws-storage-permissions/image.jpg to s3://new-graphiti/posts/aws-storage-permissions/image.jpg\nCompleted 34.0 KiB/511.7 KiB (152.0 KiB/s) with 3 file(s) remaining\nCompleted 42.2 KiB/511.7 KiB (180.5 KiB/s) with 3 file(s) remaining\nupload: posts/aws-storage-processx/index.qmd to s3://new-graphiti/posts/aws-storage-processx/index.qmd\nCompleted 42.2 KiB/511.7 KiB (180.5 KiB/s) with 2 file(s) remaining\nCompleted 83.0 KiB/511.7 KiB (256.0 KiB/s) with 2 file(s) remaining\nupload: posts/logos-equipment/image.jpg to s3://new-graphiti/posts/logos-equipment/image.jpg\nCompleted 83.0 KiB/511.7 KiB (256.0 KiB/s) with 1 file(s) remaining\nCompleted 339.0 KiB/511.7 KiB (830.9 KiB/s) with 1 file(s) remaining\nCompleted 511.7 KiB/511.7 KiB (654.4 KiB/s) with 1 file(s) remaining\nupload: posts/snakecase/all.jpeg to s3://new-graphiti/posts/snakecase/all.jpeg\n\n\nThen we can see our bucket contents - as well as list our bucket contents and capture them.\n\n\nCode\ns3fs::s3_dir_tree(bucket_path)\n\n\ns3://new-graphiti\n└── posts\n    ├── _metadata.yml\n    ├── test.txt\n    ├── aws-storage-permissions\n    │   ├── image.jpg\n    │   └── index.qmd\n    ├── aws-storage-processx\n    │   ├── image.jpg\n    │   ├── index.qmd\n    │   └── index.rmarkdown\n    ├── logos-equipment\n    │   ├── image.jpg\n    │   └── index.qmd\n    └── snakecase\n        ├── all.jpeg\n        ├── index.qmd\n        └── thumbnail.jpg\n\n\nCode\nt &lt;- s3fs::s3_dir_info(bucket_path, recurse = TRUE)\n\n\nNow we will remove test.txt\n\n\nCode\nfile.remove(here::here('posts/test.txt'))\n\n\n[1] TRUE\n\n\nNow we sync again.\n\n\nCode\nsys_call()\n\n\ndelete: s3://new-graphiti/posts/test.txt\nExit status: 0 \nOutput:\n delete: s3://new-graphiti/posts/test.txt\n\n\nList our bucket contents and capture them again\n\n\nCode\ns3_dir_tree(bucket_path)\n\n\ns3://new-graphiti\n└── posts\n    ├── _metadata.yml\n    ├── aws-storage-permissions\n    │   ├── image.jpg\n    │   └── index.qmd\n    ├── aws-storage-processx\n    │   ├── image.jpg\n    │   ├── index.qmd\n    │   └── index.rmarkdown\n    ├── logos-equipment\n    │   ├── image.jpg\n    │   └── index.qmd\n    └── snakecase\n        ├── all.jpeg\n        ├── index.qmd\n        └── thumbnail.jpg\n\n\nCode\nt2 &lt;- s3fs::s3_dir_info(bucket_path, recurse = TRUE)\n\n\nCompare the file structure before and after our sync.\n\n\nCode\nwaldo::compare(t$key, t2$key)\n\n\n     old                             | new                                 \n [9] \"posts/snakecase/all.jpeg\"      | \"posts/snakecase/all.jpeg\"      [9] \n[10] \"posts/snakecase/index.qmd\"     | \"posts/snakecase/index.qmd\"     [10]\n[11] \"posts/snakecase/thumbnail.jpg\" | \"posts/snakecase/thumbnail.jpg\" [11]\n[12] \"posts/test.txt\"                -                                     \n\n\nSuccess!!\n\n\nTo Do\nWe need to build the call to sync the other way (cloud to local) in a way that perhaps nukes local files if they are not on the cloud. This is because we need to collaborate within our team so we do things like one person will change the name of images so when the other person syncs they will have only the newly named image in their local directory.\n\nThis all deserved consideration as it could get really messy from a few different angles (ie. one person adds files they don’t want nuked and then they get nukes. There are lots of different options for doing things so we will get there.)\n\n\nDelete Bucket\nLets delete the bucket.\n\n\nCode\n#\nHere is the command line approach that we will turn off in favor of the s3fs approach.\nargs &lt;- c('s3', 'rb', bucket_path, '--force')\nsys_call()\n\n\n\n\nCode\n# Here is the `s3fs` way to \"delete\" all the versions.  \n# list all the files in the bucket\nfl &lt;- s3fs::s3_dir_ls(bucket_path, recurse = TRUE, refresh = TRUE)\n\n# list all the version info for all the files\nvi &lt;- fl |&gt; \n  purrr::map_df(s3fs::s3_file_version_info)\n\ns3fs::s3_file_delete(path = vi$uri)\n\n\n\n\nCode\ns3fs::s3_bucket_delete(bucket_path)\n\n\n[1] \"s3://new-graphiti\"\n\n\nAs we have tried this before we know that if we tell it we want to delete a bucket with versioned files in it we need to empty the bucket first including delete_markers. That is easy in the aws console with th UI but seems tricky. There is a bunch of discussion on options to this here https://stackoverflow.com/questions/29809105/how-do-i-delete-a-versioned-bucket-in-aws-s3-using-the-cli . Thinking a good way around it (and a topic for another post) would be to apply a lifecycle-configuration to the bucket that deletes all versions of files after a day - allowing you to delete bucket after they expire (as per the above post). Really we may want to have a lifecycle-configuration on all our versioned buckets to keep costs down anyway but deserves more thought and perhaps another post.\n\n\nCode\n# old notes\n# We are going to test creating a bucket with versioning on.  This has large implications for billing with some details\n# of how it works [here](https://aws.amazon.com/blogs/aws/amazon-s3-enhancement-versioning/) with example of costs [here](https://aws.amazon.com/s3/faqs/?nc1=h_ls).  Thinking we may want versioned buckets for things like `sqlite`\n# \"snapshot\" databases but definitely not for things like images."
  },
  {
    "objectID": "posts/2024-11-15-bcdata-ortho-historic/index.html",
    "href": "posts/2024-11-15-bcdata-ortho-historic/index.html",
    "title": "Getting details of historic orthophoto imagery with R",
    "section": "",
    "text": "We would like to obtain historic ortho photo imagery so that we can compare historic watershed conditions compared to current (ex. floodplain vegetation clearing, channel morphology, etc.). For our use case — restoration baseline condition assessment and impact evaluation of land cover change — our goal is to reconstruct historical conditions for entire sub-basins, as far back as possible (e.g., 1930 or 1960), and programmatically compare these to recent remotely sensed land cover analysis.\nCode\nsuppressMessages(library(tidyverse))\nlibrary(ggplot2)\nlibrary(bcdata)\nlibrary(fwapgr)\nlibrary(knitr)\nsuppressMessages(library(sf))\nlibrary(crosstalk)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(DT)\nlibrary(htmltools)\nCode\npath_post &lt;- fs::path(\n  here::here(),\n  \"posts\",\n  params$post_name\n)\nCode\nstaticimports::import(\n  dir = fs::path(\n    path_post,\n    \"scripts\"\n  ),\n  outfile = fs::path(\n    path_post,\n    \"scripts\",\n    \"staticimports\",\n    ext = \"R\"\n  )\n)\nCode\nsource(\n  fs::path(\n    path_post,\n    \"scripts\",\n    \"staticimports\",\n    ext = \"R\"\n  )\n)\n\n\nlfile_name &lt;- function(dat_name = NULL, ext = \"geojson\") {\n  fs::path(\n    path_post,\n    \"data\",\n    paste0(dat_name, \".\", ext)\n  )\n}\n\nlburn_sf &lt;- function(dat = NULL, dat_name = NULL) {\n  if (is.null(dat_name)) {\n    cli::cli_abort(\"You must provide a name for the GeoJSON file using `dat_name`.\")\n  }\n  \n  dat |&gt;\n    sf::st_write(\n      lfile_name(dat_name),\n      delete_dsn = TRUE\n      # append = FALSE\n    )\n}\n\n# Function to validate and repair geometries\nlngs_geom_validate &lt;- function(layer) {\n  layer &lt;- sf::st_make_valid(layer)\n  layer &lt;- layer[sf::st_is_valid(layer), ]\n  return(layer)\n}\nCode\n# definet he buffer in m\nbuf &lt;- 1500"
  },
  {
    "objectID": "posts/2024-11-15-bcdata-ortho-historic/index.html#generate-an-area-of-interest",
    "href": "posts/2024-11-15-bcdata-ortho-historic/index.html#generate-an-area-of-interest",
    "title": "Getting details of historic orthophoto imagery with R",
    "section": "Generate an Area of Interest",
    "text": "Generate an Area of Interest\nHere we download our area of interest which is the Neexdzii Kwah River (a.k.a Upper Bulkley River) which is located between Houston, BC (just south of Smithers) and Topley, BC which is east of Houston and north of Burns Lake, BC. We hit up our remote database managed by Simon Norris with a package built by Poisson Consulting specifically for the task. We use the downstream_route_measure of the Bulkley River (identified through a unique blue_line_key) to query the watershed area upstream of the point where the Neexdzii Kwah River enters the Wedzin Kwah River (a.k.a Morice River). Since photopoint centres that fall just outside of the watershed can provide imagery of the edge areas of the watershed we buffer this area to an amount that we approximate is half the width or hieght of the ground distance captured by each image (1500m).\n\n\nCode\n# lets build a custom watersehed just for upstream of the confluence of Neexdzii Kwa and Wetzin Kwa\n# blueline key\nblk &lt;- 360873822\n# downstream route measure\ndrm &lt;- 166030.4\n\n\n\naoi_raw &lt;- fwapgr::fwa_watershed_at_measure(blue_line_key = blk, \n                                        downstream_route_measure = drm) |&gt; \n  # we put it in utm zone 9 so we can easily buffer using meters\n  sf::st_transform(32609) |&gt; \n  dplyr::select(geometry)\n\naoi &lt;- sf::st_buffer(\n  aoi_raw,\n  dist = buf\n) |&gt; \n  sf::st_transform(4326)\n\n\n#get the bounding box of our aoi\n# aoi_bb &lt;- sf::st_bbox(aoi)\n\n#lets burn this so we don't need to download each time\naoi_raw &lt;- lngs_geom_validate(aoi_raw)\naoi &lt;- lngs_geom_validate(aoi)\n\n\n\n# now lets buffer our aoi by 1000m\n\nlburn_sf(\n  aoi,\n  deparse(substitute(aoi)))\n\nlburn_sf(\n  aoi_raw,\n  deparse(substitute(aoi_raw)))\n\n# map_aoi &lt;- ggplot() +\n#   geom_sf(\n#       data = aoi_raw,\n#       fill = \"transparent\",\n#       color = \"black\",\n#       linewidth = .5\n#   ) +\n#   geom_sf(\n#       data = aoi,\n#       fill = \"transparent\",\n#       color = \"red\",\n#       linewidth = .5\n#   ) \n#   \n# map_aoi"
  },
  {
    "objectID": "posts/2024-11-15-bcdata-ortho-historic/index.html#download-spatial-data-layers-from-bc-data-catalogue",
    "href": "posts/2024-11-15-bcdata-ortho-historic/index.html#download-spatial-data-layers-from-bc-data-catalogue",
    "title": "Getting details of historic orthophoto imagery with R",
    "section": "Download Spatial Data Layers From BC DAta Catalogue",
    "text": "Download Spatial Data Layers From BC DAta Catalogue\nNext we grab a few key layers from the BC Data Catalogue API using convenience function from our rfp package (“Reproducable Field Products”) which wrap the provincially maintained bcdata package. We grab:\n\nRailways\nStreams in the Bulkley Watershed group that are 4th order or greater.\nOrthophoto Tile Polygons\nHistoric Imagery Points\nHistoric Imagery Polygons\nNTS 1:50,000 Grid (we will see why in a second)\nAir Photo Centroids\n\n\n\nCode\n# grab all the railways\nl_rail &lt;- rfp::rfp_bcd_get_data(\n    bcdata_record_id = \"whse_basemapping.gba_railway_tracks_sp\"\n) |&gt; \n  sf::st_transform(4326) |&gt; \n  janitor::clean_names() \n\n\n# streams in the bulkley and then filter to just keep the big ones\nl_streams &lt;- rfp::rfp_bcd_get_data(\n  bcdata_record_id = \"whse_basemapping.fwa_stream_networks_sp\",\n  col_filter = \"watershed_group_code\",\n  col_filter_value = \"BULK\",\n  # grab a smaller object by including less columns\n  col_extract = c(\"linear_feature_id\", \"stream_order\", \"gnis_name\", \"downstream_route_measure\", \"blue_line_key\", \"length_metre\")\n) |&gt; \n  sf::st_transform(4326) |&gt; \n  janitor::clean_names() |&gt; \n  dplyr::filter(stream_order &gt;= 4)\n\n# historic orthophotos\n# WHSE_IMAGERY_AND_BASE_MAPS.AIMG_HIST_INDEX_MAPS_POLY\n#https://catalogue.data.gov.bc.ca/dataset/airborne-imagery-historical-index-map-points\nl_imagery_tiles &lt;- rfp::rfp_bcd_get_data(\n  # https://catalogue.data.gov.bc.ca/dataset/orthophoto-tile-polygons/resource/f46aaf7b-58be-4a25-a678-79635d6eb986\n  bcdata_record_id = \"WHSE_IMAGERY_AND_BASE_MAPS.AIMG_ORTHOPHOTO_TILES_POLY\") |&gt; \n  sf::st_transform(4326) \n\nl_imagery_hist_pnts &lt;- rfp::rfp_bcd_get_data(\n  bcdata_record_id = \"WHSE_IMAGERY_AND_BASE_MAPS.AIMG_HIST_INDEX_MAPS_POINT\") |&gt; \n  sf::st_transform(4326) \n\nl_imagery_hist_poly &lt;- rfp::rfp_bcd_get_data(\n  bcdata_record_id = \"WHSE_IMAGERY_AND_BASE_MAPS.AIMG_HIST_INDEX_MAPS_POLY\") |&gt; \n  sf::st_transform(4326) \n\nl_imagery_grid &lt;- rfp::rfp_bcd_get_data(\n  bcdata_record_id = \"WHSE_BASEMAPPING.NTS_50K_GRID\") |&gt; \n  sf::st_transform(4326) \n\n\nFollowing download we run some clean up to ensure the geometry of our spatial files is “valid”, trim to our area of interest and burn locally so that every time we rerun iterations of this memo we don’t need to wait for the download process which takes a little longer than we want to wait.\n\n\nCode\n# get a list of the objects in our env that start with l_\nls &lt;- ls()[stringr::str_starts(ls(), \"l_\")] \n\nlayers_all &lt;- tibble::lst(\n  !!!mget(ls)\n)\n\n# Apply validation to the AOI and layers\nlayers_all &lt;- purrr::map(\n  layers_all, \n  lngs_geom_validate\n  )\n\n# clip them  with purrr and sf\nlayers_trimmed &lt;- purrr::map(\n  layers_all,\n  ~ sf::st_intersection(.x, aoi)\n) \n\n# Burn each `sf` object to GeoJSON\npurrr::walk2(\n  layers_trimmed,\n  names(layers_trimmed),\n  lburn_sf\n)\n\n\n\n\nCode\n# lets use the nts mapsheet to query the photo centroids to avoid a massive file download\ncol_value &lt;- layers_trimmed$l_imagery_grid |&gt; \n  dplyr::pull(map_tile) \n\nl_photo_centroids &lt;- rfp::rfp_bcd_get_data(\n  bcdata_record_id = \"WHSE_IMAGERY_AND_BASE_MAPS.AIMG_PHOTO_CENTROIDS_SP\",\n  col_filter = \"nts_tile\",\n  col_filter_value = col_value) |&gt; \n  sf::st_transform(4326) \n\n# Apply validation to the AOI and layers\nl_photo_centroids &lt;-lngs_geom_validate(l_photo_centroids)\n\n# clip to aoi - can use  layers_trimmed$aoi \nl_photo_centroids &lt;- sf::st_intersection(l_photo_centroids, aoi)\n\n\nlburn_sf(l_photo_centroids, \"l_photo_centroids\")\n\n\nNext - we read the layers back in. The download step is skipped now unless we turn it on again by changing the update_gis param in our memo yaml header to TRUE.\n\n\nCode\n# now we read in all the sf layers that are local so it is really quick\nlayers_to_load &lt;- fs::dir_ls(\n  fs::path(\n    path_post,\n    \"data\"),\n  glob = \"*.geojson\"\n)\n\nlayers_trimmed &lt;- layers_to_load |&gt;\n  purrr::map(\n    ~ sf::st_read(\n      .x, quiet = TRUE)\n  ) |&gt; \n  purrr::set_names(\n    nm =tools::file_path_sans_ext(\n      basename(\n        names(\n          layers_to_load\n        )\n      )\n    )\n  )"
  },
  {
    "objectID": "posts/2024-11-15-bcdata-ortho-historic/index.html#map-the-area-of-interest",
    "href": "posts/2024-11-15-bcdata-ortho-historic/index.html#map-the-area-of-interest",
    "title": "Getting details of historic orthophoto imagery with R",
    "section": "Map the Area of Interest",
    "text": "Map the Area of Interest\nArea of interest is mapped in Figure 1.\n\n\nCode\nmap &lt;- ggplot2::ggplot() +\n  ggplot2::geom_sf(\n      data = layers_trimmed$aoi_raw,\n      fill = \"transparent\",\n      color = \"black\",\n      linewidth = .5\n  ) +\n  ggplot2::geom_sf(\n      data = layers_trimmed$aoi,\n      fill = \"transparent\",\n      color = \"yellow\",\n      linewidth = .5\n  ) +\n  ggplot2::geom_sf(\n    data = layers_trimmed$l_streams,\n    color = \"blue\",\n    size = 1\n  ) +\n  ggplot2::geom_sf(\n    data = layers_trimmed$l_rail,\n    color = \"black\",\n    size = 1\n  ) +\n  ggplot2::geom_sf(\n    data = layers_trimmed$l_imagery_hist_pnts,\n    color = \"red\",\n    size = 2\n  ) +\n  # ggplot2::geom_sf(\n  #   data = layers_trimmed$l_imagery_hist_poly,\n  #   color = \"red\",\n  #   size = 10\n  # ) +\n  ggplot2::geom_sf(\n    data = layers_trimmed$l_imagery_grid,\n    alpha = 0.25,\n  ) +\n  ggplot2::geom_sf_text(\n    data = layers_trimmed$l_imagery_grid,\n    ggplot2::aes(label = map_tile),\n    size = 3  # Adjust size of the text labels as needed\n  )\n\nmap +\n  ggplot2::geom_sf_text(\n    data = layers_trimmed$l_streams |&gt; dplyr::distinct(gnis_name, .keep_all = TRUE),\n    ggplot2::aes(\n      label = gnis_name\n    ),\n    size = 2  # Adjust size of the text labels as needed\n  ) \n\n\n\n\n\n\n\n\nFigure 1: Area of interest. The buffered watershed used for historic airphoto analysis is shown in yellow."
  },
  {
    "objectID": "posts/2024-11-15-bcdata-ortho-historic/index.html#explore-the-bc-data-catalouge-imagery-layer-options",
    "href": "posts/2024-11-15-bcdata-ortho-historic/index.html#explore-the-bc-data-catalouge-imagery-layer-options",
    "title": "Getting details of historic orthophoto imagery with R",
    "section": "Explore the BC Data Catalouge Imagery Layer Options",
    "text": "Explore the BC Data Catalouge Imagery Layer Options\n\nOrthophoto Tile Polygons\nFor the Orthophoto Tile Polygons Historic Imagery Polygons layer the range of year_operational is 1996, 2013. This is not as far back as we would prefer to be looking.\n\n\nHistoric Imagery Points\nOK, seems we cannot get machine readable historical air photo information from the downloaded from the BC data catalogue Historic Imagery Points layer perhaps because the majority of the photos are not georeferenced? What we see in the map and table below (red dot on map) is one point which contains 8 records including links to pdfs and kmls which are basically a georeferenced drawing of where the imagery overlaps (Table 1 and Figure 2). From as far as I can tell - if we wanted to try to use the kmls or pdfs linked in the attribute tables of the “Historic Imagery Points” layer to select orthoimagery we would need to eyeball where the photo polygons overlap where we want to see imagery for and manually write down identifiers for photo by hand. Maybe I am missing something but it sure seems that way.\n\n\nCode\nmy_caption &lt;- \"The 'airborne-imagery-historical-index-map-points' datset for the area of interest\"\n#This what the information in the [Historic Imagery Points](https://catalogue.data.gov.bc.ca/dataset/airborne-imagery-historical-index-map-points) layer looks like.\n\nlayers_trimmed$l_imagery_hist_pnts |&gt; \n  dplyr::mutate(\n    kml_url = ngr::ngr_str_link_url(\n      url_base = \"https://openmaps.gov.bc.ca/flight_indices/kml/large_scale\", \n      url_resource = \n        fs::path_rel(\n          kml_url, start = \"https://openmaps.gov.bc.ca/flight_indices/kml/large_scale\"\n        )\n    ),\n    pdf_url = ngr::ngr_str_link_url(\n      url_base = \"https://openmaps.gov.bc.ca/flight_indices/pdf\", \n      url_resource = \n        fs::path_rel(pdf_url, start = \"https://openmaps.gov.bc.ca/flight_indices/pdf\")\n    )\n  )|&gt; \n  dplyr::select(-id) |&gt; \n  sf::st_drop_geometry() |&gt; \n  knitr::kable(\n    escape = FALSE,\n    caption = my_caption\n  )\n\n\n\n\nTable 1: The ‘airborne-imagery-historical-index-map-points’ datset for the area of interest\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhistorical_index_map_id\nscale_category\ngeoextent_mapsheet\nmap_tag\nstart_year\nend_year\nkml_url\npdf_url\nobjectid\nse_anno_cad_data\n\n\n\n\n557\nlarge\n093l_e\n093l_e_1\n1950\n1963\nurl_link\nurl_link\n1860\nNA\n\n\n558\nlarge\n093l_e\n093l_e_2\n1971\n1974\nurl_link\nurl_link\n1861\nNA\n\n\n559\nlarge\n093l_e\n093l_e_3\n1975\n1975\nurl_link\nurl_link\n1862\nNA\n\n\n560\nlarge\n093l_e\n093l_e_4\n1980\n1980\nurl_link\nurl_link\n1863\nNA\n\n\n561\nlarge\n093l_e\n093l_e_5\n1980\n1980\nurl_link\nurl_link\n1864\nNA\n\n\n562\nlarge\n093l_e\n093l_e_6\n1981\n1983\nurl_link\nurl_link\n1865\nNA\n\n\n563\nlarge\n093l_e\n093l_e_7\n1989\n1989\nurl_link\nurl_link\n1866\nNA\n\n\n564\nlarge\n093l_e\n093l_e_8\n1990\n1990\nurl_link\nurl_link\n1867\nNA\n\n\n\n\n\n\n\n\n\n\n\nCode\nmy_caption &lt;- \"Screenshot of kml downloaded from link provided in Historic Imagery Points.\"\nknitr::include_graphics(fs::path(\n  path_post,\n  \"fig\",\n  \"Screenshot1\",\n  ext = \"png\"\n  )\n)\n\n\n\n\n\n\n\n\nFigure 2: Screenshot of kml downloaded from link provided in Historic Imagery Points.\n\n\n\n\n\n\n\nHistoric Imagery Polygons\nIt appears we have the same sort of kml/pdf product as we saw in the Historic Imagery Points is being served through the Historic Imagery Polygons layer (Table 2).\n\n\nCode\nmy_caption &lt;- \"The 'airborne-imagery-historical-index-map-points' datset for the area of interest\"\n#This what the information in the [Historic Imagery Points](https://catalogue.data.gov.bc.ca/dataset/airborne-imagery-historical-index-map-points) layer looks like.\n\nlayers_trimmed$l_imagery_hist_poly |&gt; \n  dplyr::mutate(\n    kml_url = ngr::ngr_str_link_url(\n      url_base = \"https://openmaps.gov.bc.ca/flight_indices/kml/large_scale\", \n      url_resource = fs::path(basename(fs::path_dir(kml_url)), basename(kml_url))\n    ),\n    pdf_url = ngr::ngr_str_link_url(\n      url_base = \"https://openmaps.gov.bc.ca/flight_indices/pdf\", \n      url_resource = fs::path(\n        ngr::ngr_str_dir_from_path(pdf_url, levels = 3), \n        ngr::ngr_str_dir_from_path(pdf_url, levels = 2),\n        ngr::ngr_str_dir_from_path(pdf_url, levels = 1),\n        basename(pdf_url))\n    )\n  )|&gt; \n  dplyr::select(-id) |&gt; \n  sf::st_drop_geometry() |&gt; \n  dplyr::arrange(start_year) |&gt; \n  knitr::kable(\n    escape = FALSE,\n  caption = my_caption\n  )\n\n\n\n\nTable 2: The ‘airborne-imagery-historical-index-map-points’ datset for the area of interest\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhistorical_index_map_id\nscale_category\ngeoextent_mapsheet\nmap_tag\nstart_year\nend_year\nkml_url\npdf_url\nfeature_area_sqm\nfeature_length_m\nobjectid\nse_anno_cad_data\n\n\n\n\n866\nmedium\n093l\n093l_1\n1938\n1956\nurl_link\nurl_link\n14421427749\n481668.3\n2183\nNA\n\n\n862\nmedium\n093k\n093k_2\n1946\n1956\nurl_link\nurl_link\n14421427749\n481668.3\n2179\nNA\n\n\n557\nlarge\n093l_e\n093l_e_1\n1950\n1963\nurl_link\nurl_link\n7211433354\n352456.5\n1923\nNA\n\n\n549\nlarge\n093k_w\n093k_w_1\n1950\n1963\nurl_link\nurl_link\n7211433354\n352456.5\n1915\nNA\n\n\n863\nmedium\n093k\n093k_3\n1961\n1966\nurl_link\nurl_link\n14421427749\n481668.3\n2180\nNA\n\n\n867\nmedium\n093l\n093l_2\n1966\n1972\nurl_link\nurl_link\n14421427749\n481668.3\n2184\nNA\n\n\n550\nlarge\n093k_w\n093k_w_2\n1969\n1973\nurl_link\nurl_link\n7211433354\n352456.5\n1916\nNA\n\n\n1047\nsmall\n093nw\n093nw_1\n1970\n1975\nurl_link\nurl_link\n56959075420\n956978.8\n2312\nNA\n\n\n558\nlarge\n093l_e\n093l_e_2\n1971\n1974\nurl_link\nurl_link\n7211433354\n352456.5\n1924\nNA\n\n\n864\nmedium\n093k\n093k_4\n1971\n1971\nurl_link\nurl_link\n14421427749\n481668.3\n2181\nNA\n\n\n559\nlarge\n093l_e\n093l_e_3\n1975\n1975\nurl_link\nurl_link\n7211433354\n352456.5\n1925\nNA\n\n\n551\nlarge\n093k_w\n093k_w_3\n1975\n1980\nurl_link\nurl_link\n7211433354\n352456.5\n1917\nNA\n\n\n865\nmedium\n093k\n093k_5\n1977\n1985\nurl_link\nurl_link\n14421427749\n481668.3\n2182\nNA\n\n\n552\nlarge\n093k_w\n093k_w_4\n1978\n1985\nurl_link\nurl_link\n7211433354\n352456.5\n1918\nNA\n\n\n553\nlarge\n093k_w\n093k_w_5\n1978\n1988\nurl_link\nurl_link\n7211433354\n352456.5\n1919\nNA\n\n\n1048\nsmall\n093nw\n093nw_2\n1979\n1979\nurl_link\nurl_link\n56959075420\n956978.8\n2313\nNA\n\n\n560\nlarge\n093l_e\n093l_e_4\n1980\n1980\nurl_link\nurl_link\n7211433354\n352456.5\n1926\nNA\n\n\n561\nlarge\n093l_e\n093l_e_5\n1980\n1980\nurl_link\nurl_link\n7211433354\n352456.5\n1927\nNA\n\n\n562\nlarge\n093l_e\n093l_e_6\n1981\n1983\nurl_link\nurl_link\n7211433354\n352456.5\n1928\nNA\n\n\n868\nmedium\n093l\n093l_3\n1981\n1984\nurl_link\nurl_link\n14421427749\n481668.3\n2185\nNA\n\n\n1049\nsmall\n093nw\n093nw_3\n1982\n1984\nurl_link\nurl_link\n56959075420\n956978.8\n2314\nNA\n\n\n554\nlarge\n093k_w\n093k_w_6\n1985\n1988\nurl_link\nurl_link\n7211433354\n352456.5\n1920\nNA\n\n\n1050\nsmall\n093nw\n093nw_trim\n1986\n1988\nurl_link\nurl_link\n64069901258\n1020771.3\n2315\nNA\n\n\n563\nlarge\n093l_e\n093l_e_7\n1989\n1989\nurl_link\nurl_link\n7211433354\n352456.5\n1929\nNA\n\n\n555\nlarge\n093k_w\n093k_w_7\n1989\n1989\nurl_link\nurl_link\n7211433354\n352456.5\n1921\nNA\n\n\n556\nlarge\n093k_w\n093k_w_8\n1990\n1990\nurl_link\nurl_link\n7211433354\n352456.5\n1922\nNA\n\n\n564\nlarge\n093l_e\n093l_e_8\n1990\n1990\nurl_link\nurl_link\n7211433354\n352456.5\n1930\nNA\n\n\n\n\n\n\n\n\n\n\nAir Photo Centroids\nEach of the Air Photo Centroids are georeferenced with a date range of:\n\n\nCode\nrange(layers_trimmed$l_photo_centroids$photo_date)\n\n\n[1] \"1963-08-07\" \"2019-09-18\"\n\n\n\nWe visualize column metadata in Table 3 and map the centroids in our study area with Figure 3.\n\n\nCode\n# At this point we have downloaded two csvs (one for each NTS 1:50,000 mapsheet of course) with information about the airphotos including UTM coordinates that we will assume for now are the photo centres. In our next steps we read in what we have, turn into spatial object, trim to overall study area and plot.\n# list csvs\nls &lt;- fs::dir_ls(\n  fs::path(\n    path_post,\n    \"data\"),\n  glob = \"*.csv\"\n)\n\nphotos_raw &lt;- ls |&gt; \n  purrr::map_df(\n    readr::read_csv\n  ) |&gt; \n  sf::st_as_sf(\n    coords = c(\"Longitude\", \"Latitude\"), crs = 4326\n  ) |&gt; \n  janitor::clean_names() |&gt; \n  dplyr::mutate(photo_date = lubridate::mdy(photo_date)) \n\n\nphotos_aoi &lt;- sf::st_intersection(\n  photos_raw, \n  layers_trimmed$aoi |&gt; st_make_valid()\n  )\n\n\n\n\nCode\nbcdata::bcdc_describe_feature(\"WHSE_IMAGERY_AND_BASE_MAPS.AIMG_PHOTO_CENTROIDS_SP\") |&gt; \n  knitr::kable() |&gt; \n  kableExtra::scroll_box(width = \"100%\", height = \"500px\")\n\n\n\n\nTable 3: Metadata for the 'airphoto-centroids' layer in the BC Data Catalouge\n\n\n\n\n\n\n\ncol_name\nsticky\nremote_col_type\nlocal_col_type\ncolumn_comments\n\n\n\n\nid\nTRUE\nxsd:string\ncharacter\nNA\n\n\nAIRP_ID\nTRUE\nxsd:decimal\nnumeric\nAIRP_ID is the unique photo frame identifier, generated by the source APS system.\n\n\nFLYING_HEIGHT\nFALSE\nxsd:decimal\nnumeric\nFLYING_HEIGHT is the flying height above mean sea level in metres.\n\n\nPHOTO_YEAR\nTRUE\nxsd:decimal\nnumeric\nPHOTO_YEAR is the operational year to which this photograph is assigned.\n\n\nPHOTO_DATE\nFALSE\nxsd:date\ndate\nPHOTO_DATE is the date (year, month, and day) of exposure of the photograph.\n\n\nPHOTO_TIME\nFALSE\nxsd:string\ncharacter\nPHOTO_TIME is the time of exposure (hours, minutes, seconds), expressed in Pacific Standard Time, e.g., 9:43:09 AM.\n\n\nLATITUDE\nFALSE\nxsd:decimal\nnumeric\nLATITUDE is the geographic coordinate, in decimal degrees (dd.dddddd), of the location of the feature as measured from the equator, e.g., 55.323653.\n\n\nLONGITUDE\nFALSE\nxsd:decimal\nnumeric\nLONGITUDE is the geographic coordinate, in decimal degrees (dd.dddddd), of the location of the feature as measured from the prime meridian, e.g., -123.093544.\n\n\nFILM_ROLL\nFALSE\nxsd:string\ncharacter\nFILM_ROLL is a BC Government film roll identifier, e.g., bc5624.\n\n\nFRAME_NUMBER\nFALSE\nxsd:decimal\nnumeric\nFRAME_NUMBER is the sequential frame number of this photograph within a film roll.\n\n\nGEOREF_METADATA_IND\nFALSE\nxsd:string\ncharacter\nGEOREF_METADATA_IND indicates if georeferencing metadata exists for this photograph, i.e., Y, N.\n\n\nPUBLISHED_IND\nFALSE\nxsd:string\ncharacter\nPUBLISHED_IND indicates if this photograph's geometry and metadata should be exposed for viewing, i.e., Y,N.\n\n\nMEDIA\nFALSE\nxsd:string\ncharacter\nMEDIA describes the photographic medium on which this photograph was recorded, e.g. Film - BW.\n\n\nPHOTO_TAG\nFALSE\nxsd:string\ncharacter\nPHOTO_TAG is a combination of film roll identifier and frame number that uniquely identifies an air photo, e.g., bcc09012_035.\n\n\nBCGS_TILE\nFALSE\nxsd:string\ncharacter\nBCGS_TILE identifies the BCGS mapsheet within which the centre of this photograph is contained. The BCGW mapsheet could be 1:20,000, 1:10,000 or 1:5,000, e.g., 104a01414.\n\n\nNTS_TILE\nFALSE\nxsd:string\ncharacter\nNTS_TILE identifies the NTS 1:50,000 mapsheet tile within which the centre of this photograph is contained, e.g., 104A03.\n\n\nSCALE\nFALSE\nxsd:string\ncharacter\nSCALE of the photo with respect to ground based on a 9-inch square hardcopy print, e.g., 1:18,000.\n\n\nGROUND_SAMPLE_DISTANCE\nFALSE\nxsd:decimal\nnumeric\nGROUND_SAMPLE_DISTANCE indicates the distance on the ground in centimetres represented by a single pixel in the scanned or original digital version of this photograph.\n\n\nOPERATION_TAG\nFALSE\nxsd:string\ncharacter\nOPERATION_TAG is an alpha numeric shorthand operation identifier representing photographic medium, operation number, requesting agency and operational year of photography, e.g., D003FI15.\n\n\nFOCAL_LENGTH\nFALSE\nxsd:decimal\nnumeric\nFOCAL_LENGTH is the focal length of the lens, in millimetres, used to capture this photograph.\n\n\nTHUMBNAIL_IMAGE_URL\nFALSE\nxsd:string\ncharacter\nTHUMBNAIL_IMAGE_URL is a hyperlink to the 1/16th resolution thumbnail version of this image.\n\n\nFLIGHT_LOG_URL\nFALSE\nxsd:string\ncharacter\nFLIGHT_LOG_URL is a hyperlink to the scanned version of the original handwritten flight log page (film record) for this image.\n\n\nCAMERA_CALIBRATION_URL\nFALSE\nxsd:string\ncharacter\nCAMERA_CALIBRATION_URL is a hyperlink to the camera calibration report file for this image.\n\n\nPATB_GEOREF_URL\nFALSE\nxsd:string\ncharacter\nPATB_GEOREF_URL is a hyperlink to the PatB geo-referencing file for this image.\n\n\nFLIGHT_LINE_SEGMENT_ID\nFALSE\nxsd:decimal\nnumeric\nFLIGHT_LINE_SEGMENT_ID identifies the section of flight line to which this photograph belongs.\n\n\nOPERATION_ID\nFALSE\nxsd:decimal\nnumeric\nOPERATION_ID is a unique identifier for the operation to which this photograph belongs. It may be used by the data custodian to diagnose positional errors.\n\n\nFILM_RECORD_ID\nFALSE\nxsd:decimal\nnumeric\nFILM_RECORD_ID is a unique identifier for the film record to which this photograph belongs. It may be used by the data custodian to diagnose positional errors.\n\n\nSHAPE\nFALSE\ngml:PointPropertyType\nsfc geometry\nSHAPE is the column used to reference the spatial coordinates defining the feature.\n\n\nOBJECTID\nTRUE\nxsd:decimal\nnumeric\nOBJECTID is a column required by spatial layers that interact with ESRI ArcSDE. It is populated with unique values automatically by SDE.\n\n\nSE_ANNO_CAD_DATA\nFALSE\nxsd:hexBinary\nnumeric\nSE_ANNO_CAD_DATA is a binary column used by spatial tools to store annotation, curve features and CAD data when using the SDO_GEOMETRY storage data type.\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmap +\n  geom_sf(\n    data = layers_trimmed$l_photo_centroids,\n    alpha = 0.25\n  ) \n\n\n\n\n\n\n\n\nFigure 3: ‘airphoto-centroids’ dataset for the area of interest.\n\n\n\n\n\nThat is a lot of photos! 9388 photos to be exact!!!"
  },
  {
    "objectID": "posts/2024-11-15-bcdata-ortho-historic/index.html#refine-airphoto-centres-by-clipping-with-buffered-streams",
    "href": "posts/2024-11-15-bcdata-ortho-historic/index.html#refine-airphoto-centres-by-clipping-with-buffered-streams",
    "title": "Getting details of historic orthophoto imagery with R",
    "section": "Refine Airphoto Centres by Clipping with Buffered Streams",
    "text": "Refine Airphoto Centres by Clipping with Buffered Streams\nAlthough we are likely moveing on to a different strategy - this section details how we can obtain imagery IDs for photo centres that fall within a pre-determined distance from streams of interest in our study area.\n\n\nCode\n# amount to buffer all stream segments\nq_buffer &lt;- 1500\n# q_drm_main &lt;- 263795\n\n# length of streams other than selected explicity to buffer\nq_drm_other &lt;- 3000\n\n\nHere are our query parameters to narrow down the area within our study are watershed in which we want to find photos for:\n\nBuffer: 1500m - size of buffer used on either side of stream lines selected\nStream segments:\n\nBulkley River (gnis_name in the stream layer)\nMaxan Creek\nBuck Creek\nfor each remaining stream - segments of that stream which begin before 3000m from the downstream system (i.e. the first 3km) of stream.\n\n\n\n\nCode\n# We use the `downstream_route_measure` of the stream layer to exclude areas upstream of Bulkley Lake (also known as Taman Creek).  We find it in QGIS by highlighting the stream layer and clicking on our segment of interest while we have the information tool selected - the resulting pop-up looks like this in QGIS.\nknitr::include_graphics(fs::path(\n  path_post,\n  \"fig\",\n  \"Screenshot2\",\n  ext = \"png\"\n  )\n)\n\n\n\n\nCode\nr_streams &lt;- c(\"Maxan Creek\", \"Buck Creek\")\n\naoi_refined_raw &lt;- layers_trimmed$l_streams |&gt; \n  # removed  & downstream_route_measure &lt; q_drm_main for bulkley as doestn't cahnge 1960s query and increases beyond just by 5 photos\n  dplyr::filter(gnis_name == \"Bulkley River\"|\n                  gnis_name != \"Bulkley River\" & downstream_route_measure &lt; q_drm_other |\n                  gnis_name %in% r_streams) |&gt; \n  # dplyr::arrange(downstream_route_measure) |&gt;\n  # calculate when we get to length_m by adding up the length_metre field and filtering out everything up to length_m\n  # dplyr::filter(cumsum(length_metre) &lt;= length_m) |&gt;\n  sf::st_union() |&gt; \n  # we need to run st_sf or we get a sp object in a list...\n  sf::st_sf()\n  \naoi_refined_buffered &lt;- sf::st_buffer(\n  aoi_refined_raw,\n  q_buffer, endCapStyle = \"FLAT\"\n) \n\nphotos_aoi_refined &lt;- sf::st_intersection(\n  layers_trimmed$l_photo_centroids, \n  aoi_refined_buffered\n  )\n\n\nLet’s plot again and include our buffered areas around the first 3000m of streams (area in red) along with the location of the photo points that land within that area. Looks like this give us 2770 photos.\n\n\nCode\nmap +\n  geom_sf(\n    data = aoi_refined_buffered,\n    color = \"red\",\n    alpha= 0\n  ) +\n  geom_sf(\n    data = photos_aoi_refined,\n    alpha = 0.25,\n  ) +\n  geom_sf_text(\n    data = layers_trimmed$l_streams |&gt; dplyr::distinct(gnis_name, .keep_all = TRUE),\n    aes(\n      label = gnis_name\n    ),\n    size = 2  # Adjust size of the text labels as needed\n  ) \n\n\n\n\n\n\n\n\n\nThat is not as many photos - but still quite a few (2770).\n\n\nCode\n# @fig-dt1 below can be used to filter these photos from any time and/or mapsheet and export the result to csv or excel file.  \n#| label: fig-dt1\n#| tbl-cap: \"All photo centroids located with watershed study area.\"\nphotos_aoi_refined |&gt; \n  dplyr::select(-id) |&gt; \n  my_dt_table(cols_freeze_left = 0)"
  },
  {
    "objectID": "posts/2024-11-15-bcdata-ortho-historic/index.html#filter-photos-by-date",
    "href": "posts/2024-11-15-bcdata-ortho-historic/index.html#filter-photos-by-date",
    "title": "Getting details of historic orthophoto imagery with R",
    "section": "Filter Photos by Date",
    "text": "Filter Photos by Date\nNow lets map by year to see what our options are including the earliest photos possible. Here is our range to choose from:\n\n\nCode\nrange(photos_aoi_refined$photo_date)\n\n\n[1] \"1963-08-07\" \"2019-09-18\"\n\n\n`\n\n\nCode\nmap +\ngeom_sf(\n  data = photos_aoi_refined |&gt; dplyr::filter(photo_year &lt;= \"1975\")\n  ) +\n  facet_wrap(~ photo_year)\n\n\n\n\n\n\n\n\n\nWell - looks like we get really good coverage of the Bulkley River mainstem in 1968 then much better coverage of the Buck Creek drainage and Maxan Creek in 1971. For 1975 - the coverage of the Bulkley mainstem and Maxan Creek is pretty good…\n\nIf we just wanted the areas near the river and we don’t mind mixing years - we grab the photos from:\n\n1968 all\n1971 for the Buck Creek and Maxan Creek areas only\n1975 Maxan Creek only\n\n\n\n\nCode\n# spatially represent just Buck and Maxan, buffer and clip the 1971 photos\n# \"r_\" is for \"refine\"\nr_year1 &lt;- \"1968\"\nr_year2 &lt;- \"1971\"\nr_year3 &lt;- \"1975\"\n\nr_streams2 &lt;- c(\"Maxan Creek\")\n\nl_streams_refined1 &lt;- layers_trimmed$l_streams |&gt; \n  # we defined r_streams in chunk way above \n  dplyr::filter(gnis_name %in% r_streams) |&gt; \n  sf::st_union() |&gt; \n  # we need to run st_sf or we get a sp object in a list...\n  sf::st_sf()\n  \naoi_refined_buffered2 &lt;- sf::st_buffer(\n  l_streams_refined1,\n  q_buffer, endCapStyle = \"FLAT\"\n) \n\nl_streams_refined2 &lt;- layers_trimmed$l_streams |&gt; \n  # we defined r_streams in chunk way above \n  dplyr::filter(gnis_name %in% r_streams2) |&gt; \n  sf::st_union() |&gt; \n  # we need to run st_sf or we get a sp object in a list...\n  sf::st_sf()\n  \naoi_refined_buffered3 &lt;- sf::st_buffer(\n  l_streams_refined2,\n  q_buffer, endCapStyle = \"FLAT\"\n) \n\n# filter first year\nphotos1 &lt;- photos_aoi_refined |&gt; \n  dplyr::filter(\n      photo_year == r_year1\n  )\n\n# filter second year using just the streams we want to include\nphotos2 &lt;- sf::st_intersection(\n  layers_trimmed$l_photo_centroids |&gt; dplyr::filter(photo_year == r_year2), \n  aoi_refined_buffered2\n  )\n\n# filter second year using just the streams we want to include\nphotos3 &lt;- sf::st_intersection(\n  layers_trimmed$l_photo_centroids |&gt; dplyr::filter(photo_year == r_year3), \n  aoi_refined_buffered3\n  )\n\nphotos_all &lt;- dplyr::bind_rows(photos1, photos2, photos3)\n\n\nNow let’s have a look at the individual year components (Figure 4) as well as the whole dataset (Figure 5). We are privileged to potentially have the assistance of Mike Price to help us obtain this imagery from the UBC archives. If there are too many photos to grab as is - the table below can be filtered by photo_year to reduce the number of photos. The resulting filtered dataset can then be downloaded by pressing the CSV or Excel buttons at the bottom of the table….\n\n\nCode\nmy_caption &lt;- \"Amalgamated photo points presented by year.\"\nmap +\n  geom_sf(\n  data = photos_all\n  ) +\n  facet_wrap(~ photo_year)\n\n\n\n\n\n\n\n\nFigure 4: Amalgamated photo points presented by year.\n\n\n\n\n\n\n\nCode\nmy_caption &lt;- \"Amalgamated photo points\"\nmap +\n  geom_sf(\n  data = photos_all\n  ) +\n  geom_sf_text(\n    data = layers_trimmed$l_streams |&gt; dplyr::distinct(gnis_name, .keep_all = TRUE),\n    aes(\n      label = gnis_name\n    ),\n    size = 2  # Adjust size of the text labels as needed\n  ) \n\n\n\n\n\n\n\n\nFigure 5: Amalgamated photo points\n\n\n\n\n\n\nExport csv with Photo Information for Areas Adjacent to Streams\nLet’s burn out a csv that can be used to find the imagery for the 253 photos above.\n\n\nCode\nlfile_name_photos &lt;- function(dat = NULL){\n  fs::path(\n      path_post,\n      \"exports\",\n      paste(\n        \"airphotos\",\n        paste(range(dat$photo_date), collapse = \"_\"),\n        sep = \"_\"\n      ),\n      ext = \"csv\"\n    )\n}\n\nphotos_all |&gt; \n  readr::write_csv(\n    lfile_name_photos(photos_all), na =\"\"\n  )\n\n\nlpath_link &lt;- function(dat = NULL){\n  paste0(\n    \"https://github.com/NewGraphEnvironment/new_graphiti/tree/main/posts/2024-11-15-bcdata-ortho-historic/exports/\",\n    basename(\n      lfile_name_photos(dat)\n    )\n  )\n}\n\n\nWe can view and download exported csv files here but really we are perhaps better off using the widget below to get the csv file we need.\n\n\nCode\nphotos_all |&gt; \n  dplyr::select(-id) |&gt; \n  my_dt_table(cols_freeze_left = 0)"
  },
  {
    "objectID": "posts/2024-11-15-bcdata-ortho-historic/index.html#estimate-polygon-size-based-on-the-scale-and-a-9-x-9-negative",
    "href": "posts/2024-11-15-bcdata-ortho-historic/index.html#estimate-polygon-size-based-on-the-scale-and-a-9-x-9-negative",
    "title": "Getting details of historic orthophoto imagery with R",
    "section": "Estimate Polygon Size based on the Scale and a 9” x 9” Negative",
    "text": "Estimate Polygon Size based on the Scale and a 9” x 9” Negative\nHere we take a shot at deriving the image footprint using the scale and a negative size of 9” X 9” which seems to be what is recorded in the flight logs (haven’t checked every single one yet).\n\n\nCode\n# not accurate!!!!!!!!!!!! this is for the equator!!!\n# Add geometry\nphotos_poly_prep &lt;- layers_trimmed$l_photo_centroids |&gt; \n  dplyr::select(-id) |&gt; \n  sf::st_drop_geometry() |&gt; \n  # Parse scale\n  dplyr::mutate(\n    scale_parsed = as.numeric(stringr::str_remove(scale, \"1:\")),\n    width_m = 9 * scale_parsed * 0.0254,  # Width in meters\n    height_m = 9 * scale_parsed * 0.0254, # Height in meters\n  ) |&gt;\n  # Create geometry\n  dplyr::rowwise() |&gt; \n  dplyr::mutate(\n    geometry = list({\n      # Create polygon corners\n      center &lt;- c(longitude, latitude)\n      width_deg = (width_m / 2) / 111320  # Convert width to degrees (~111.32 km per degree latitude)\n      height_deg = (height_m / 2) / 111320 # Approximate for longitude; accurate near equator\n      \n      # Define corners\n      corners &lt;- matrix(\n        c(\n          center[1] - width_deg, center[2] - height_deg, # Bottom-left\n          center[1] + width_deg, center[2] - height_deg, # Bottom-right\n          center[1] + width_deg, center[2] + height_deg, # Top-right\n          center[1] - width_deg, center[2] + height_deg, # Top-left\n          center[1] - width_deg, center[2] - height_deg  # Close the polygon\n        ),\n        ncol = 2,\n        byrow = TRUE\n      )\n      \n      # Create polygon geometry\n      sf::st_polygon(list(corners))\n    })\n  ) |&gt; \n  dplyr::ungroup() |&gt;\n  # Convert to sf object\n  sf::st_as_sf(sf_column_name = \"geometry\", crs = 4326) \n\n\n\n\nCode\nphotos_poly_prep &lt;- layers_trimmed$l_photo_centroids |&gt; \n  dplyr::select(-id) |&gt; \n  sf::st_transform(crs = 32609) |&gt;  # Transform to UTM Zone 9 for accurate metric calculations\n  # Parse scale and calculate dimensions in meters\n  dplyr::mutate(\n    scale_parsed = as.numeric(stringr::str_remove(scale, \"1:\")),\n    width_m = 9 * scale_parsed * 0.0254,  # Width in meters\n    height_m = 9 * scale_parsed * 0.0254  # Height in meters\n  ) |&gt; \n  # Create geometry using UTM coordinates\n  dplyr::rowwise() |&gt; \n  dplyr::mutate(\n    geometry = list({\n      # Create polygon corners in UTM (meters)\n      center &lt;- sf::st_coordinates(geometry)  # Extract UTM coordinates\n      width_half = width_m / 2\n      height_half = height_m / 2\n      \n      # Define corners in meters\n      corners &lt;- matrix(\n        c(\n          center[1] - width_half, center[2] - height_half, # Bottom-left\n          center[1] + width_half, center[2] - height_half, # Bottom-right\n          center[1] + width_half, center[2] + height_half, # Top-right\n          center[1] - width_half, center[2] + height_half, # Top-left\n          center[1] - width_half, center[2] - height_half  # Close the polygon\n        ),\n        ncol = 2,\n        byrow = TRUE\n      )\n      \n      # Create polygon geometry\n      sf::st_polygon(list(corners))\n    })\n  ) |&gt; \n  dplyr::ungroup() |&gt; \n  # Convert to sf object with UTM Zone 9 CRS\n  sf::st_as_sf(sf_column_name = \"geometry\") |&gt; \n  sf::st_set_crs(32609) |&gt;  # Assign UTM Zone 9 CRS\n  # Transform back to WGS84 (if needed)\n  sf::st_transform(crs = 4326)\n\n\n\n\nCode\n# Assuming `photos_poly` has a `photo_year` column with years of interest\nphotos_poly &lt;- photos_poly_prep |&gt; \n  dplyr::filter(\n    photo_year %in% c(1968, 1971, 1975)\n    )\n\nl_photo_centroids_fltered &lt;- layers_trimmed$l_photo_centroids |&gt; \n      dplyr::filter(\n        photo_year %in% c(1968, 1971, 1975)\n    )\n\nyears &lt;- unique(photos_poly$photo_year)\n\nyears_centroids &lt;- unique(l_photo_centroids_fltered$photo_year)\n\nmap &lt;- my_leaflet()\n\n# Loop through each year and add polygons with the year as a group\nfor (year in years) {\n  map &lt;- map |&gt; \n    leaflet::addPolygons(\n      data = photos_poly |&gt; \n        dplyr::filter(\n          photo_year == year\n          ), \n      color = \"black\", \n      weight = 1, \n      smoothFactor = 0.5,\n      opacity = 1.0, \n      fillOpacity = 0,\n      group = paste0(\"Polygons - \", year)\n    )\n}\n\n# Add centroid layers for each year\nfor (year in years_centroids) {\n  map &lt;- map |&gt; \n    leaflet::addCircleMarkers(\n      data = l_photo_centroids_fltered |&gt; dplyr::filter(photo_year == year),\n      radius = 1,\n      color = \"black\",\n      fillOpacity = 0.7,\n      opacity = 1.0,\n      group = paste0(\"Centroids - \", year)\n    )\n}\n\nall_groups &lt;- c(paste0(\"Polygons - \", years), paste0(\"Centroids - \", years_centroids))\n\n# Add layer control to toggle year groups\nmap &lt;- map |&gt; \n  leaflet::addPolygons(\n    data = layers_trimmed$aoi, \n              color = \"black\", \n              weight = 1, \n              smoothFactor = 0.5,\n              opacity = 1.0, \n              fillOpacity = 0\n              ) |&gt; \n  # leaflet::addPolygons(\n  #   data = layers_trimmed$aoi_raw, \n  #             color = \"yellow\", \n  #             weight = 1, \n  #             smoothFactor = 0.5,\n  #             opacity = 1.0, \n  #             fillOpacity = 0\n  #             ) |&gt; \n  leaflet::addLayersControl(\n    baseGroups = c(\n      \"Esri.DeLorme\",\n      \"ESRI Aerial\"),\n    overlayGroups = all_groups,\n    options = leaflet::layersControlOptions(collapsed = FALSE)\n  ) |&gt; \n  leaflet.extras::addFullscreenControl()\n\nmap\n\n\n\n\n\n\n\n\nFigure 6"
  },
  {
    "objectID": "posts/2024-11-15-bcdata-ortho-historic/index.html#build-a-interactive-dashboard-that-allows-us-to-visualize-and-download-by-a-specific-year",
    "href": "posts/2024-11-15-bcdata-ortho-historic/index.html#build-a-interactive-dashboard-that-allows-us-to-visualize-and-download-by-a-specific-year",
    "title": "Getting details of historic orthophoto imagery with R",
    "section": "Build a Interactive Dashboard that allows us to Visualize and Download by a Specific Year",
    "text": "Build a Interactive Dashboard that allows us to Visualize and Download by a Specific Year\nUse the filter and slider to see the coverage for photos we have for an individual year then export to excel or csv file with the buttons at the bottom of the table below.\n\n\nCode\n# Wrap data frame in SharedData\nsd &lt;- crosstalk::SharedData$new(\n  layers_trimmed$l_photo_centroids |&gt; \n    dplyr::mutate(\n      thumbnail_image_url = ngr::ngr_str_link_url(\n        url_base = \"https://openmaps.gov.bc.ca/thumbs\", \n        url_resource = \n          fs::path_rel(\n            thumbnail_image_url, start = \"https://openmaps.gov.bc.ca/thumbs\"\n          )\n      ),\n      flight_log_url = ngr::ngr_str_link_url(\n        url_base = \"https://openmaps.gov.bc.ca/thumbs/logbooks/\", \n        url_resource = \n          fs::path_rel(flight_log_url, start = \"https://openmaps.gov.bc.ca/thumbs/logbooks/\")\n      )\n    )|&gt; \n    \n    dplyr::select(-id)\n)\n\n\n# Use SharedData like a dataframe with Crosstalk-enabled widgets\nmap3 &lt;- sd |&gt;\n  leaflet::leaflet(height = 500) |&gt; #height=500, width=780\n  leaflet::addProviderTiles(\"Esri.WorldTopoMap\", group = \"Topo\") |&gt;\n  leaflet::addProviderTiles(\"Esri.WorldImagery\", group = \"ESRI Aerial\") |&gt;\n  leaflet::addCircleMarkers(\n    radius = 3,\n    fillColor = \"black\",\n    color= \"#ffffff\",\n    stroke = TRUE,\n    fillOpacity = 1.0,\n    weight = 2,\n    opacity = 1.0\n  ) |&gt;\n  leaflet::addPolylines(data=layers_trimmed$l_streams,\n               opacity=0.75, \n               color = 'blue',\n               fillOpacity = 0.75, \n               weight=2) |&gt; \n    leaflet::addPolygons(\n    data = layers_trimmed$aoi, \n              color = \"black\", \n              weight = 1, \n              smoothFactor = 0.5,\n              opacity = 1.0, \n              fillOpacity = 0\n              ) |&gt; \n  leaflet::addLayersControl(\n    baseGroups = c(\n      \"Esri.DeLorme\",\n      \"ESRI Aerial\"),\n    options = leaflet::layersControlOptions(collapsed = F)) |&gt; \n  leaflet.extras::addFullscreenControl(position = \"bottomright\") |&gt; \n  leaflet::addScaleBar(position = \"bottomleft\")\n\n\nwidgets &lt;- crosstalk::bscols(\n  widths = c(3, 9),\n  crosstalk::filter_checkbox(\n    id = \"label\",\n    label =  \"Media Type\",\n    sharedData = sd,\n    group =  ~media\n  ),\n  crosstalk::filter_slider(\n    id = \"year_slider\",\n    label = \"Year\",\n    sharedData = sd,\n    column = ~photo_year,\n    round = 0\n  )\n)\n\nhtmltools::browsable(\n  htmltools::tagList(\n    widgets,\n    map3,\n    sd |&gt; my_dt_table(page_length = 5, escape = FALSE)\n  )\n)\n\n\n\n\n\n\nMedia Type\n\n\n\n\nDigital - Colour\n\n\n\n\n\nFilm - BW\n\n\n\n\n\nFilm - Colour\n\n\n\n\n\n\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\nHow many different flight log records are there?\n\n156\n\nWhat are the unique values of scale reported?\n\n1:10000, 1:12000, 1:15000, 1:15840, 1:16000, 1:18000, 1:20000, 1:30000, 1:31680, 1:40000, 1:50000, 1:60000, 1:70000, 1:8000, 9600"
  },
  {
    "objectID": "posts/2024-11-15-bcdata-ortho-historic/index.html#thinkgs-to-do",
    "href": "posts/2024-11-15-bcdata-ortho-historic/index.html#thinkgs-to-do",
    "title": "Getting details of historic orthophoto imagery with R",
    "section": "Thinkgs to do",
    "text": "Thinkgs to do\nAccording to Figure 2 some images have been georeferenced. However, using a kml with basically a picture drawn on it showing what appears to be the airp_id seems like a very difficult task. Guessing there is a decent chance that there is a digital file somewhere within the gov that details which airp_ids are on which kml/pdf and we could use that to remove photos from our chosen year from ones we wish to aquire and georeference."
  },
  {
    "objectID": "posts/logos-equipment/index.html",
    "href": "posts/logos-equipment/index.html",
    "title": "Logos and Equipment List Somewhere Accessible",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, dpi=60, out.width = \"100%\")\noptions(scipen=999)\noptions(knitr.kable.NA = '--') #'--'\noptions(knitr.kable.NAN = '--')\n\n\nThis here is a post showing where we are now keeping the company equipment details as well as the company logos. We need these things to be accessible to all team members from all repos so we have put them here since this is a public repo.\nHere is the location of the equipment lists that we use in safety/field planning:\nhttps://raw.githubusercontent.com/NewGraphEnvironment/new_graphiti/main/assets/data/equipment.csv\n\n\nCode\nreadr::read_csv(\n  url(\n    glue::glue(\"https://raw.githubusercontent.com/{params$repo_owner}/{params$repo_name}/main/assets/data/equipment.csv\")\n  )\n)|&gt; \n  fpr::fpr_kable(font = 12)\n\n\n\n\n\n\n\neq_item\neq_pers_standard\neq_truck\neq_safety\neq_task1\neq_task2\neq_task3\neq_task4\neq_task5\neq_task6\neq_task7\nmateo_winterschedt\n\n\n\n\nclinometer\nx\n--\n--\nall\nelectrofishing\nfish passage\n--\n--\n--\n--\nx\n\n\nfield vest\nx\n--\n--\nall\n--\nfish passage\n--\n--\n--\n--\nx\n\n\nnote book\nx\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nGPS\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\nx\n\n\nSuncreen\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nBugspray\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nPolarized glasses\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nBear Spray\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\nx\n\n\nphone/camera\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nbattery pack booster for phone\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nHat\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nfirst aid kit personal\nx\n--\nx\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nWaders\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\nx\n\n\nBoots\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\nx\n\n\nExtra clothes\nx\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nrain gear\nx\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nSki poles\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nwater\nx\n--\nx\nall\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nfood\nx\n--\nx\nall\n--\nfish passage\n--\n--\n--\n--\n--\n\n\ngloves work\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nglasses safety\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nheadlamp\nx\n--\nx\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nOakton Multimeter\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\nTurbidity Meter LaMotte 2020e\n--\n--\n--\n--\n--\n--\nenvironmental monitoring\nwq\n--\n--\n--\n\n\nHand saw\n--\nx\nx\nall\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\nBackpack Electrofisher\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\nstop nets x 4\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\nsalt blocks\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\nloose salt\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\ndip nets x 2\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\nLinesman Gloves x 3\n--\n--\nx\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\ntape measure hand\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\ntape measure eslon\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\npilon x 2\n--\nx\n--\nall\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nMeasuring board\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nScale\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nPermits\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nBackroads Mapbook\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nLocational maps\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nFish ID book\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nBackground Documents\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nradio handheld\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nradio truck\n--\nx\n--\n--\n--\n--\n--\n--\n--\n--\nx\n\n\nradio chargers x 3\n--\n--\nx\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nSatelite communicator\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nField Safety Plan\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nfirst aid kit level 1\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nFirst Aid binder stocked\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nSite Cards / Field Guide\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nMinnow Traps\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nCatfood\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nFlagging\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nLaptop w/basecamp\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nGPS cable\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nLazer level\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nAssessment cards fish passage\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nUAV radio\n--\n--\nx\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nUAV\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nUAV landing pad\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nUAV GC tape\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nUAV safety plan (when required)\n--\n--\nx\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nUAV registration\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nUAV license\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nUAV radio license\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nFlow meter\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nATV\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nThrow bags\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\npolaski\n--\nx\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nshovel\n--\nx\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nfire extinguisher backpack\n--\nx\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nfire extinguisher pressurized\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nbucket rigid x 2\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nbucket foldable\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nclove oil kit w/ instructions\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\ngloves leather\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nhard hat\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nsteel toed boots\nx\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nsharpies\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nhand lens\nx\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nATV gas\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nATV lock\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nUAV battery charger\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nwader disinfectant kit\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\ntruck tow rope\n--\nx\nx\nall\n--\n--\n--\n--\n--\n--\n--\n\n\ntruck jack\n--\nx\nx\nall\n--\n--\n--\n--\n--\n--\n--\n\n\nGPS batteries\n--\n--\n--\n--\nelectrofishing\nfish passage\n--\n--\n--\n--\n--\n\n\nATV helmets\n--\n--\n--\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nBattery booster\n--\nx\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nCompressor 12V\n--\nx\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nRubber boots (no-slip soles)\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nSmall BT Speaker (for bears)\n--\n--\nx\n--\n--\nfish passage\n--\n--\n--\n--\n--\n\n\nGPS Case waterproof\n--\n--\n--\nsnorkling\n--\n--\n--\n--\n--\n--\n--\n\n\nNotebook waterproof\n--\n--\n--\nsnorkling\n--\n--\n--\n--\n--\n--\n--\n\n\nDrysuits\n--\n--\n--\nsnorkling\n--\n--\n--\n--\n--\n--\n--\n\n\nSnorkels\n--\n--\n--\nsnorkling\n--\n--\n--\n--\n--\n--\n--\n\n\ndrysuit gloves\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nGoggles\n--\n--\n--\nsnorkling\n--\n--\n--\n--\n--\n--\n--\n\n\nFanny pack\n--\n--\n--\nsnorkling\n--\n--\n--\n--\n--\n--\n--\n\n\nlarge backpack\n--\n--\n--\n--\nelectrofishing\n--\n--\n--\n--\n--\n--\n\n\nTow strap\n--\nx\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\nrange finder\nx\n--\n--\n--\n--\n--\n--\n--\n--\n--\n--\n\n\n\n\n\n\n\n\n\nLogos:\nhttps://raw.githubusercontent.com/NewGraphEnvironment/new_graphiti/main/assets/logos\nThere are many — so here is a list of their names and locations online:\n\n\nCode\nfile_names &lt;- fs::dir_ls(\n  glue::glue(here::here(\"assets/logos\")),\n  glob = c(\"*.png\", \"*.jpg\", \"*.jpeg\"),\n  recurse = TRUE\n) \n\n\ntibble::tibble(path = file_names) |&gt; \n    dplyr::mutate(path = stringr::str_replace_all(path, \"/Users/airvine/Projects/repo/new_graphiti\", \"https://github.com/NewGraphEnvironment/new_graphiti/tree/main\")) |&gt; \n  fpr::fpr_kable(font = 12)\n\n\n\n\n\n\n\npath\n\n\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/BLACK/PNG/nge-full_black.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/BLACK/PNG/nge-icon_black.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/BLACK/PNG/nge-icon_name_big_black.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/BLACK/PNG/nge-icon_name_black.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/BLACK/PNG/nge-name_black.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/BLACK/PNG/nge-name_research_consulting_black.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/BLACK/PNG/nge-research_consulting_black.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/COLOR/PNG/nge-full_color.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/COLOR/PNG/nge-icon.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/COLOR/PNG/nge-icon_name.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/COLOR/PNG/nge-icon_name_big_color.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/COLOR/PNG/nge-name_color.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/COLOR/PNG/nge-name_research_consulting_color.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/COLOR/PNG/nge-research_consulting_color.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/GREYSCALE/PNG/nge-full_greyscale.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/GREYSCALE/PNG/nge-icon_gradient_grey.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/GREYSCALE/PNG/nge-icon_name_big_greyscale.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/GREYSCALE/PNG/nge-icon_name_greyscale.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/GREYSCALE/PNG/nge-name_greyscale.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/GREYSCALE/PNG/nge-name_research_consulting_greyscale.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/GREYSCALE/PNG/nge-research_consulting_greyscale.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/WHITE/PNG/nge-full_white.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/WHITE/PNG/nge-icon_name_big_white.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/WHITE/PNG/nge-icon_name_white.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/WHITE/PNG/nge-icon_white.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/WHITE/PNG/nge-name_research_consulting_white.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/WHITE/PNG/nge-name_white.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/logo_newgraph/WHITE/PNG/nge-research_consulting_white.png\n\n\nhttps://github.com/NewGraphEnvironment/new_graphiti/tree/main/assets/logos/pixel.png"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "rws",
    "section": "",
    "text": "raNdom workish stufF"
  },
  {
    "objectID": "posts/2025-01-16-leaflet-cog-aws-viewer/index.html",
    "href": "posts/2025-01-16-leaflet-cog-aws-viewer/index.html",
    "title": "Setting up TiTiler to serve COGs of UAV imagery on AWS with leaflet and Elastic Beanstalk",
    "section": "",
    "text": "Whoa Bobby-Joe.\nJourney here to set up a TiTiler on a remote server.\nThis is a continuation of a past post that you can find here. Thanks to ChatGPT for the help. Image by ChatGPT.\nWe already have a bucket set up to serve our COG imagery files and we have crafted a simple index.HTML file on AWS to point to a COG file and serve it out in a basic leaflet map. A little bit of complexity arrives because we want a tile service in order for these images to be able to be rendered in the browser using server side rendering. For that we need something like TiTiler running on a cloud instance. So we’re gonna document that set up on AWS here so we can find it again.\nTo enable scalability and simplify deployment we will use AWS Elastic Beanstalk (eb). We are on a mac so first thing we do is:\nbrew install a WSEBCLI. \nBecause we are already set up with credentials through environmental variables back when we set up awscli eb will link to those credentials automatically on initialization.\nSo next we need to identify a launch template for the eb environment as per these docs\nFirst thing is to find the latest Amazon Linux 2 AMI ID:\naws ssm get-parameters --names \"/aws/service/ami-amazon-linux-latest/al2023-ami-kernel-default-x86_64\" --region us-west-2\nwhich gives us\n{\n    \"Parameters\": [\n        {\n            \"Name\": \"/aws/service/ami-amazon-linux-latest/al2023-ami-kernel-default-x86_64\",\n            \"Type\": \"String\",\n            \"Value\": \"ami-093a4ad9a8cc370f4\",\n            \"Version\": 105,\n            \"LastModifiedDate\": \"2025-01-16T16:44:38.939000-08:00\",\n            \"ARN\": \"arn:aws:ssm:us-west-2::parameter/aws/service/ami-amazon-linux-latest/al2023-ami-kernel-default-x86_64\",\n            \"DataType\": \"text\"\n        }\n    ],\n    \"InvalidParameters\": []\n}\nSo then we do the long way and first create a launch template with the following cmd:\naws ec2 create-launch-template --launch-template-name TitilerTemplate \\\n--launch-template-data '{\n    \"ImageId\": \"ami-093a4ad9a8cc370f4\",\n    \"InstanceType\": \"t3.micro\"\n}'\nThis gives us back this which we use to get our LaunchTemplateId:\n{\n    \"LaunchTemplate\": {\n        \"LaunchTemplateId\": \"lt-049eff4ed7a9490f8\",\n        \"LaunchTemplateName\": \"TitilerTemplate\",\n        \"CreateTime\": \"2025-01-17T23:37:06+00:00\",\n        \"CreatedBy\": \"arn:aws:iam::{my-secret-account-id}:user/{my-secet-username}\",\n        \"DefaultVersionNumber\": 1,\n        \"LatestVersionNumber\": 1\n    }\n}\nThe default security group is likely not appropriate for a public-facing tile server because it might:\n\nAllow broad internal access within your AWS account, which is unnecessary.\nRestrict external traffic, preventing public access to your tiles.\n\nFor a public-facing tile server like Titiler, the security group should:\n\nAllow Inbound HTTP/HTTPS Traffic:\nOpen port 80 (HTTP) and port 443 (HTTPS) to the world (0.0.0.0/0).\nRestrict Unnecessary Access:\nLimit other inbound traffic (e.g., SSH or internal AWS traffic) unless explicitly needed.\nCreate a Custom Security Group\n\nHere’s how to set up a security group specifically for your tile server:\nCreate the Security Group:\naws ec2 create-security-group --group-name titilersecuritygroup \\\n    --description \"Security group for Titiler tile server\"\nAllow Public HTTP/HTTPS Access:\naws ec2 authorize-security-group-ingress --group-name titilersecuritygroup \\\n    --protocol tcp --port 80 --cidr 0.0.0.0/0\naws ec2 authorize-security-group-ingress --group-name titilersecuritygroup \\\n    --protocol tcp --port 443 --cidr 0.0.0.0/0\nGet the Security Group ID:\naws ec2 describe-security-groups --group-names titilersecuritygroup --query \"SecurityGroups[0].GroupId\" --output text\nUpdate the Launch Template: Add the Security Group ID to the Launch Template using its LaunchTemplateId:\nThen we make a litle launchtemplate.config file and put it in our main project directory elastic-beanstock in a .ebextensions directory. It looks like this with our SecurityGroups id added as per our last query:\noption_settings:\n  aws:autoscaling:launchconfiguration:\n    SecurityGroups: sg-xxxxxxxxxxxxxxxxxx\n    InstanceType: t3.micro\n    RootVolumeType: gp3\n    MonitoringInterval: \"1 minute\"\n    DisableIMDSv1: true\n    IamInstanceProfile: \"aws-elasticbeanstalk-ec2-role\"\nIn order to have an easy launch of Titiler we make a Dockerrun.aws.json file to go in our main elastic-beanstock roject directory we have created to do this work. The Dockerrun.aws.json file looks like this:\n{\n    \"AWSEBDockerrunVersion\": \"1\",\n    \"Image\": {\n        \"Name\": \"developmentseed/titiler\",\n        \"Update\": \"true\"\n    },\n    \"Ports\": [\n        {\n            \"ContainerPort\": 80\n        }\n    ]\n}\nThen we create a trust-policy.json in our main elastic-beanstock directory to allow eb to:\n\nLaunch and terminate EC2 instances.\nCreate and manage security groups.\nConfigure Auto Scaling.\nSet up Elastic Load Balancers.\nAccess S3 buckets for deployments.\n\nIt looks like this:\n{\n   \"Version\": \"2012-10-17\",\n   \"Statement\": [\n       {\n           \"Effect\": \"Allow\",\n           \"Principal\": {\n               \"Service\": \"elasticbeanstalk.amazonaws.com\"\n           },\n           \"Action\": \"sts:AssumeRole\"\n       }\n   ]\n}\nNow we attach the policy\naws iam attach-role-policy --role-name aws-elasticbeanstalk-service-role \\\n--policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess\nTo be sure - we verify the policy is attached:\naws iam list-attached-role-policies --role-name aws-elasticbeanstalk-service-role\nYou should see AmazonEC2FullAccess in the output.\nVerify the VPC. A VPC (Virtual Private Cloud) is a private, isolated network within AWS where you can launch and manage AWS resources like EC2 instances, databases, and load balancers. Run this command to see the route tables for each subnet and determine if they are public:\naws ec2 describe-vpcs --query \"Vpcs[?IsDefault].VpcId\" --region us-west-2 --output text\nNext it gets weird - Find the Default Route Table with a query that includes our uniqye VpcId which we recieved from our last query:\naws ec2 describe-route-tables --filters Name=vpc-id,Values=vpc-XXXXXXXXXXXXX --region us-west-2\nBecause the default route table is connected to an Internet Gateway - subnets need to be explicitly associated with this route table. Look for entries with “DestinationCidrBlock”: “0.0.0.0/0” and “GatewayId”: “igw-xxxxxxxx” in the output. These indicate that the subnet is public.; those without are private:\naws ec2 associate-route-table --route-table-id rtb-xx --subnet-id subnet-xxx\naws ec2 associate-route-table --route-table-id rtb-xx --subnet-id subnet-xx\naws ec2 associate-route-table --route-table-id rtb-x --subnet-id subnet-x\naws ec2 associate-route-table --route-table-id rtb-xx --subnet-id subnet-xx\nUpdate your VPCId in your .ebextensions/launchtemplate.config. Also Ensure your configuration file includes the associated subnets:\noption_settings:\n  aws:autoscaling:launchconfiguration:\n    SecurityGroups: sg-xxxxx\n    InstanceType: t3.micro\n    RootVolumeType: gp3\n    MonitoringInterval: \"1 minute\"\n    DisableIMDSv1: true\n    IamInstanceProfile: \"aws-elasticbeanstalk-ec2-role\"\n  aws:ec2:vpc:\n    VPCId: vpc-xxx\n    Subnets: subnet-xx,subnet-xx,subnet-xx,subnet-xx\n\nNow we create the env:\neb create titiler-env \nOnce that is completed we can find our Elastic Beanstalk environment’s CNAME with:\neb status\nIts CNAME: titiler-env.eba-s4jhubvr.us-west-2.elasticbeanstalk.com\nHere is what our setup file structure looks like.\n\n\nCode\nfs::dir_tree(\"/Users/airvine/Projects/repo/elastic-beanstalk\", recurse = TRUE, all = TRUE)\n\n\n/Users/airvine/Projects/repo/elastic-beanstalk\n├── .ebextensions\n│   └── launchtemplate.config\n├── .elasticbeanstalk\n│   └── config.yml\n├── .gitignore\n├── Dockerrun.aws.json\n└── trust-policy.json\n\n\nSo we are going to add this to the index file that we serve in another AWS bucket to serve out our leaflet map with the COG.\n'http://titiler-env.eba-s4jhubvr.us-west-2.elasticbeanstalk.com/cog/tiles/{z}/{x}/{y}.png?url=https://23cog.s3.amazonaws.com/20210906lampreymoricetribv220230317.tif'\nGeez… Here it is and can be seen in alll its fullscreen glory here!!!"
  },
  {
    "objectID": "posts/aws-storage-permissions/index.html",
    "href": "posts/aws-storage-permissions/index.html",
    "title": "Setting aws bucket permissions with R",
    "section": "",
    "text": "Here we will set up an s3 bucket with a policy that allows the public to read from the bucket, but not from a specific directory, and allows a particular aws_account_id root user as well as a defined user to write to the bucket.\nAlthough we are stoked on the s3fs package for working with s3 buckets, we will use the paws package more than perhaps necessary here - only to learn about how it all works. Seems like s3fs is the way to go for common moves but paws is the “mom” providing the structure and guidance to that package.\n\n\nCode\nlibrary(paws)\nlibrary(here)\n\n\nhere() starts at /Users/airvine/Projects/repo/new_graphiti\n\n\nCode\nlibrary(jsonlite)\nlibrary(stringr)\nlibrary(s3fs)\n\n\nList our current buckets\n\n\nCode\ns3 &lt;- paws::s3()\ns3$list_buckets()\n\n\n$Buckets\n$Buckets[[1]]\n$Buckets[[1]]$Name\n[1] \"23cog\"\n\n$Buckets[[1]]$CreationDate\n[1] \"2023-03-17 00:07:12 GMT\"\n\n\n\n$Owner\n$Owner$DisplayName\n[1] \"al\"\n\n$Owner$ID\n[1] \"f5267b02e31758d1efea79b4eaef5d0423efb3e6a54ab869dc860bcc68ebae2d\"\n\n\n\nCreate Bucket\nLet’s create a bucket called the same name as this repository.\n\n\nCode\nmy_bucket_name &lt;- basename(here::here()) |&gt; \n  stringr::str_replace_all(\"_\", \"-\") \n\nbucket_path &lt;- s3fs::s3_path(my_bucket_name)\n\n\n\n\nCode\ns3$create_bucket(Bucket = my_bucket_name,\n  CreateBucketConfiguration = list(\n    LocationConstraint = Sys.getenv(\"AWS_DEFAULT_REGION\")\n  ))\n\n\n$Location\n[1] \"http://new-graphiti.s3.amazonaws.com/\"\n\n\n\n\nAdd the policy to the bucket.\n\nImportant - First we need to allow “new public policies” to be added to the bucket. We do this by deleteing the public access block. This is a security feature that prevents public access to the bucket. We will remove it so we can add our own policy. Took a while to catch this.\n\n\n\nCode\ns3$delete_public_access_block(\n  Bucket = my_bucket_name\n)\n\n\nlist()\n\n\n\nWrite the policy for the bucket Here is a function to make a generic policy for an s3 bucket that allows public to read from the bucket, but not from a specific directory, and allows a particular aws_account_id to write to the bucket. Plus + it allows you to provide Presigned URLs so we can provide temporary access to private objects without having to change the overall bucket or object permissions.\n\nKey thing here is that if we want a user to override the policy placed on a directory or file after we Deny access we need to add a condition to the policy that exempts the user (paste0(\"arn:aws:iam::\", aws_account_id, \":user/\", user_access_permission)) from the Deny.\n\n\nCode\n# https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example1.html\n#https://chatgpt.com/share/16106509-a34d-4f69-bf95-cd5eb2649707\naws_policy_write &lt;- function(bucket_name, \n                             bucket_dir_private, \n                             aws_account_id, \n                             user_access_permission, \n                             write_json = FALSE, \n                             dir_output = \"policy\", \n                             file_name = \"policy.json\") {\n  policy &lt;- list(\n    Version = \"2012-10-17\",\n    Statement = list(\n      list(\n        Effect = \"Allow\",\n        Principal = \"*\",\n        Action = \"s3:GetObject\",\n        Resource = paste0(\"arn:aws:s3:::\", bucket_name, \"/*\")\n      ),\n      list(\n        Effect = \"Deny\",\n        Principal = \"*\",\n        Action = \"s3:GetObject\",\n        Resource = paste0(\"arn:aws:s3:::\", bucket_name, \"/\", bucket_dir_private, \"/*\"),\n        # IMPORTANT - Denies everyone from getting objects from the private directory except for user_access_permission\n        Condition = list(\n          StringNotEquals = list(\n            \"aws:PrincipalArn\" = paste0(\"arn:aws:iam::\", aws_account_id, \":user/\", user_access_permission)\n          )\n        )\n      ),\n      list(\n        Effect = \"Allow\",\n        Principal = list(AWS = paste0(\"arn:aws:iam::\", aws_account_id, \":root\")),\n        Action = c(\"s3:DeleteObject\", \"s3:PutObject\"),\n        Resource = paste0(\"arn:aws:s3:::\", bucket_name, \"/*\")\n      )\n      #going to leave this here for now\n      # list(\n      #   Effect = \"Allow\",\n      #   Principal = list(AWS = paste0(\"arn:aws:iam::\", aws_account_id, \":user/\", user_access_permission)),\n      #   Action = c(\"s3:GetBucketLocation\", \"s3:ListBucket\"),\n      #   Resource = paste0(\"arn:aws:s3:::\", bucket_name)\n      # ),\n      # list(\n      #   Effect = \"Allow\",\n      #   Principal = list(AWS = paste0(\"arn:aws:iam::\", aws_account_id, \":user/\", user_access_permission)),\n      #   Action = \"s3:GetObject\",\n      #   Resource = paste0(\"arn:aws:s3:::\", bucket_name, \"/*\")\n      # )\n    )\n  )\n  \n  json_policy &lt;- jsonlite::toJSON(policy, pretty = TRUE, auto_unbox = TRUE)\n  \n  if (write_json) {\n    dir.create(dir_output, showWarnings = FALSE)\n    output_path &lt;- file.path(dir_output, file_name)\n    write(json_policy, file = output_path)\n    message(\"Policy written to \", output_path)\n  } else {\n    return(json_policy)\n  }\n}\n\n\nNow we can write the policy to the bucket.\n\n\nCode\n# run the function to build the json policy statement\nmy_policy &lt;- aws_policy_write(bucket_name = my_bucket_name, \n                         bucket_dir_private = \"private\",\n                         aws_account_id = Sys.getenv(\"AWS_ACCOUNT_ID\"),\n                         user_access_permission = \"airvine\",\n                         write_json = FALSE\n                         )\n\n# push the policy to the bucket\ns3$put_bucket_policy(\n  Bucket = my_bucket_name,\n  Policy = my_policy,\n  ExpectedBucketOwner = Sys.getenv(\"AWS_ACCOUNT_ID\")\n)\n\n\nlist()\n\n\n\n\nCode\n# this is cool - Check the policy was added correctly.\ns3$get_bucket_policy(my_bucket_name)\n\n\n\n\nAdd some files to the bucket\nFirst we add a photo to the main bucket. Going to use s3fs for this since I haven’t actually done just one file yet… We are using the here package to get the path to the image due to rendering complexities.\n\n\nCode\ns3fs::s3_file_copy(\n  path = paste0(here::here(), \"/posts/\", params$post_dir_name, \"/image.jpg\"),\n  bucket_path\n)\n\n\n[1] \"s3://new-graphiti/image.jpg\"\n\n\nThen we add one to the private directory.\n\n\nCode\ns3fs::s3_dir_create(\n  path = paste0(bucket_path, \"/private\")\n)\n\n\n[1] \"s3://new-graphiti/private\"\n\n\nCode\ns3fs::s3_file_copy(\n  path = paste0(here::here(), \"/posts/\", params$post_dir_name, \"/image.jpg\"),\n  paste0(bucket_path, \"/private\")\n)\n\n\n[1] \"s3://new-graphiti/private/image.jpg\"\n\n\n\n\nAccess the bucket\nLet’s see if we can add the images to this post.\nCreate the paths to the images.\n\n\nCode\n# s3fs::s3_dir_info(bucket_path, recurse = TRUE)\nimage_path &lt;- paste0(\"https://\", my_bucket_name, \".s3.amazonaws.com/image.jpg\")\nimage_path_private &lt;- paste0(\"https://\", my_bucket_name, \".s3.amazonaws.com/private/image.jpg\")\n\n\nAccess the public image.\n\n\nCode\nknitr::include_graphics(image_path)\n\n\n\n\n\n\n\n\n\nGood to go.\nAnd now access the private image.\n\n\nCode\nknitr::include_graphics(image_path_private)\n\n\n\n\n\n\n\n\n\n💣 Jackpot! We have the image in the “private” bucket so can’t access them from the post without permission.\n\n\nProvide temporary access to an object\nBecause we granted ourselves access to the private directory and our IAM roles have the correct privileges, we can create a Presigned URL to provide temporary access to the private image. We will set the maximum of 7 days for the URL to be valid. That means that at 2024-06-03 14:48 the URL will no longer work and the image below will no longer render in this post!\n\n\nCode\nknitr::include_graphics(\n  s3fs::s3_file_url(\n    s3_dir_ls(paste0(bucket_path, \"/private\")),\n    604800,\n    \"get_object\"\n  )\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# this is the cmd line way\nurl_file_share &lt;- s3_dir_ls(paste0(bucket_path, \"/private\"))\ncommand &lt;- \"aws\"\nargs &lt;- c('s3', 'presign', url_file_share, '--expires-in', '604800')\n\nworking_directory = here::here() #we could just remove from funciton to get the current wd but its nice to have so we leave\n\n# loaded this function from the other file. should put in functions file or package\nsys_call()\n\n\nIn order to rerun our post we need to delete the bucket. When we do rerun - we use the s3fs package to do it\n\n\nCode\n# Dont delete the bucket or the post wont render! ha\n# Burn down the bucket 🔥.  If we try to use `s3$delete_bucket(Bucket = my_bucket_name)` we will get an error because the \n# bucket is not empty. \n\n#`s3fs::s3_bucket_delete(bucket_path)` works fine though.\ns3fs::s3_bucket_delete(bucket_path)"
  },
  {
    "objectID": "posts/2024-06-30-land-cover/index.html",
    "href": "posts/2024-06-30-land-cover/index.html",
    "title": "Mapping Land Cover with R",
    "section": "",
    "text": "Visualize and quantify remotely sense land cover data…. Here is a first start. We will use the European Space Agency’s WorldCover product which provides global land cover maps for the years 2020 and 2021 at 10 meter resolution based on the combination of Sentinel-1 radar data and Sentinel-2 imagery. We will use the 2021 dataset for mapping an area of the Skeena watershed near Houston, British Columbia.\nThis post was inspired - with much of the code copied - from a repository on GitHub from the wonderfully talented Milos Popovic.\nFirst thing we will do is load our packages. If you do not have the packages installed yet you can change the update_pkgs param in the yml of this file to TRUE. Using pak is great because it allows you to update your packages when you want to.\nCode\npkgs_cran &lt;- c(\n  \"usethis\",\n  \"rstac\",\n  \"here\",\n  \"fs\",\n  \"terra\",\n  \"tidyverse\",\n  \"rayshader\",\n  \"sf\",\n  \"classInt\",\n  \"rgl\",\n  \"tidyterra\",\n  \"tabulapdf\",\n  \"bcdata\",\n  \"ggplot\",\n  \"ggdark\",\n  \"knitr\",\n  \"DT\",\n  \"htmlwidgets\")\n\npkgs_gh &lt;- c(\n  \"poissonconsulting/fwapgr\",\n  \"NewGraphEnvironment/rfp\"\n )\n\npkgs &lt;- c(pkgs_cran, pkgs_gh)\n\nif(params$update_pkgs){\n  # install the pkgs\n  lapply(pkgs,\n         pak::pkg_install,\n         ask = FALSE)\n}\n\n# load the pkgs\npkgs_ld &lt;- c(pkgs_cran,\n             basename(pkgs_gh))\ninvisible(\n  lapply(pkgs_ld,\n       require,\n       character.only = TRUE)\n)\n\nsource(here::here(\"scripts/functions.R\"))"
  },
  {
    "objectID": "posts/2024-06-30-land-cover/index.html#downsample-the-land-cover-raster",
    "href": "posts/2024-06-30-land-cover/index.html#downsample-the-land-cover-raster",
    "title": "Mapping Land Cover with R",
    "section": "Downsample the Land Cover Raster",
    "text": "Downsample the Land Cover Raster\nHere we downsample the land cover raster to the same resolution as the DEM for the purposes of rendering our map of the larger area in a reasonable amount of time.\n\n\nCode\n# Here we resample the land cover raster to the same resolution as the DEM.\nland_cover_raster_resampled &lt;- terra::resample(\n    land_cover_raster,\n    dem,\n    method = \"near\",\n    threads = TRUE\n)\n\n# terra::plot(land_cover_raster_resampled)"
  },
  {
    "objectID": "posts/2024-06-30-land-cover/index.html#get-additional-data",
    "href": "posts/2024-06-30-land-cover/index.html#get-additional-data",
    "title": "Mapping Land Cover with R",
    "section": "Get Additional Data",
    "text": "Get Additional Data\nWe could use some data for context such as major streams and the railway. We get the streams and railway from data distribution bc api using the bcdata package. Our rfp package calls just allow some extra sanity checks and convenience moves on the bcdata::bcdc_query_geodata function. It’s not really necessary but can be helpful (ex. can use small cap layer and column names and will throw an informative error if the name of the columns specified are input incorrectly).\n\n\n\nCode\n# grab all the railways\nl_rail &lt;- rfp::rfp_bcd_get_data(\n    bcdata_record_id = \"whse_basemapping.gba_railway_tracks_sp\"\n) |&gt; \n  sf::st_transform(4326) |&gt; \n  janitor::clean_names() \n\n# streams in the bulkley and then filter to just keep the big ones\nl_streams &lt;- rfp::rfp_bcd_get_data(\n  bcdata_record_id = \"whse_basemapping.fwa_stream_networks_sp\",\n  col_filter = \"watershed_group_code\",\n  col_filter_value = \"BULK\",\n  # grab a smaller object by including less columns\n  col_extract = c(\"linear_feature_id\", \"stream_order\", \"gnis_name\", \"downstream_route_measure\", \"blue_line_key\", \"length_metre\")\n) |&gt; \n  sf::st_transform(4326) |&gt; \n  janitor::clean_names() |&gt; \n  dplyr::filter(stream_order &gt; 4)\n\n\nNow we trim up those layers. We have some functions to validate and repair geometries and then we clip them to our area of interest.\n\n\nCode\nlayers_to_trim &lt;- tibble::lst(l_rail, l_streams)\n\n# Function to validate and repair geometries\nvalidate_geometries &lt;- function(layer) {\n  layer &lt;- sf::st_make_valid(layer)\n  layer &lt;- layer[sf::st_is_valid(layer), ]\n  return(layer)\n}\n\n# Apply validation to the AOI and layers\naoi &lt;- validate_geometries(aoi)\nlayers_to_trim &lt;- purrr::map(layers_to_trim, validate_geometries)\n\n# clip them  with purrr and sf\nlayers_trimmed &lt;- purrr::map(\n  layers_to_trim,\n  ~ sf::st_intersection(.x, aoi)\n)"
  },
  {
    "objectID": "posts/2024-06-30-land-cover/index.html#get-legend-values",
    "href": "posts/2024-06-30-land-cover/index.html#get-legend-values",
    "title": "Mapping Land Cover with R",
    "section": "Get Legend Values",
    "text": "Get Legend Values\nSo we need to map the values in the raster to the actual land cover classes. We can do this by extracting the cross reference table from the pdf provided in the metatdata of the data. We will use the tabulapdf package to extract the table and do some work to collapse it into a cross-referenceing tool we can use for land cover classifications and subsequent color schemes.\n\n\nCode\n# extract the cross reference table from the pdf\npdf_file &lt;- \"https://esa-worldcover.s3.eu-central-1.amazonaws.com/v200/2021/docs/WorldCover_PUM_V2.0.pdf\"\npage &lt;- 15\n\n# table_map &lt;- tabulapdf::locate_areas(pdf_file, pages = page)\n# table_coords &lt;- list(as.numeric(unlist(table_map[[1]])))\n\ntable_coords &lt;- list(c(94.55745,  74.66493, 755.06007, 550.41094))\n\n\nxref_raw &lt;- tabulapdf::extract_tables(\n  pdf_file,\n  pages = page,\n  method = \"lattice\",\n  area = table_coords,\n  guess = FALSE\n)\n\n# ##this is how we make a clean dataframe\nxref_raw2 &lt;- xref_raw |&gt; \n  purrr::pluck(1) |&gt;\n  tibble::as_tibble() |&gt;\n  janitor::row_to_names(1) |&gt;\n  janitor::clean_names()\n\nxref_raw3 &lt;- xref_raw2 |&gt; \n  tidyr::fill(code, .direction = \"down\")\n\n# Custom function to concatenate rows within each group\ncollapse_rows &lt;- function(df) {\n  df |&gt; \n    dplyr::summarise(across(everything(), ~ paste(na.omit(.), collapse = \" \")))\n}\n\n# Group by code and apply the custom function\nxref &lt;- xref_raw3 |&gt;\n  dplyr::group_by(code) |&gt;\n  dplyr::group_modify(~ collapse_rows(.x)) |&gt;\n  dplyr::ungroup() |&gt; \n  dplyr::mutate(code = as.numeric(code)) |&gt; \n  dplyr::arrange(code) |&gt; \n  purrr::set_names(c(\"code\", \"land_cover_class\", \"lccs_code\", \"definition\", \"color_code\")) |&gt; \n  # now we make a list of the color codes and convert to hex. Even though we don't actually need them here...\n  dplyr::mutate(color_code = purrr::map(color_code, ~ as.numeric(strsplit(.x, \",\")[[1]])),\n                color = purrr::map_chr(color_code, ~ rgb(.x[1], .x[2], .x[3], maxColorValue = 255))) |&gt; \n  dplyr::relocate(definition, .after = last_col())\n\n\nmy_dt_table(xref, cols_freeze_left = 0, page_length = 5)\n\n\n\n\n\n\nWe seem to get issues when the colors we have in our tiff does not match our cross-reference table. For this reason we will remove any values in the xref object that are not in the rasters that we are plotting.\n\nAlso - looks like when we combined our tiffs together with terra::mosaic we lost the color table associated with the SpatRaster object. We can recover that table with terra::coltab(land_cover_raster_raw[[1]])"
  },
  {
    "objectID": "posts/2024-06-30-land-cover/index.html#plot",
    "href": "posts/2024-06-30-land-cover/index.html#plot",
    "title": "Mapping Land Cover with R",
    "section": "Plot",
    "text": "Plot\nOk. Let’s plot it up. We will use ggplot2 and tidyterra to plot the land cover raster and then add the streams and railway on top of that.\n\n\nCode\ncolor_table &lt;- terra::coltab(land_cover_raster_raw[[1]])[[1]]\n\ncoltab(land_cover_raster_resampled) &lt;- color_table\n\nxref_cleaned &lt;- xref |&gt; \n  filter(code %in% sort(unique(terra::values(land_cover_raster_resampled))))\n\n\nmap &lt;- ggplot() +\n  tidyterra::geom_spatraster(\n    data = as.factor(land_cover_raster_resampled),\n    use_coltab = TRUE,\n    maxcell = Inf\n  ) +\n  tidyterra::scale_fill_coltab(\n    data = as.factor(land_cover_raster_resampled),\n    name = \"ESA Land Cover\",\n    labels = xref_cleaned$land_cover_class\n  ) +\n  # geom_sf(\n  #     data = aoi,\n  #     fill = \"transparent\",\n  #     color = \"white\",\n  #     linewidth = .5\n  # ) +\n  geom_sf(\n    data = layers_trimmed$l_streams,\n    color = \"blue\",\n    size = 1\n  ) +\n  geom_sf(\n    data = layers_trimmed$l_rail,\n    color = \"black\",\n    size = 1\n  ) +\n  ggdark::dark_theme_void()\n\n\nmap"
  },
  {
    "objectID": "posts/2024-06-30-land-cover/index.html#plot-refined-area",
    "href": "posts/2024-06-30-land-cover/index.html#plot-refined-area",
    "title": "Mapping Land Cover with R",
    "section": "Plot Refined Area",
    "text": "Plot Refined Area\nFirst we need to clip the landcover raster to the buffered area. We are not going to use the resampled raster because we want a more detailed view of the land cover classes for this much smaller area. The computational time to render the plot will be fine at the original resolution.\n\n\nCode\nland_cover_sample &lt;- terra::crop(land_cover_raster, aoi_refined_buffered, snap = \"near\", mask = TRUE, extend = TRUE)\n#dimensions  : 259, 951, 1  multiple stream segments\n#dimensions  : 262, 958, 1 single stream segment\n\n\n\nWe lose our color values with the crop. We see that with has.colors(land_cover_sample).\n\n\nCode\nhas.colors(land_cover_sample)\n\n\n[1] FALSE\n\n\n\nLet’s add them back in with the terra::coltab function.\n\n\nCode\ncoltab(land_cover_sample) &lt;- color_table\n\n\n\nNow we should be able to plot what we have. Let’s re-trim up our extra data layers to the refined area of interest and add those in as well.\n\n\nCode\n# clip them  with purrr and sf\nlayers_trimmed &lt;- purrr::map(\n  layers_to_trim,\n  ~ sf::st_intersection(.x, aoi_refined_buffered)\n) \n\nxref_cleaned &lt;- xref |&gt; \n  dplyr::filter(code %in% sort(unique(terra::values(land_cover_sample))))\n\nmap &lt;- ggplot2::ggplot() +\n  tidyterra::geom_spatraster(\n    data = as.factor(land_cover_sample),\n    use_coltab = TRUE,\n    maxcell = Inf\n  ) +\n  tidyterra::scale_fill_coltab(\n    data = as.factor(land_cover_sample),\n    name = \"ESA Land Cover\",\n    labels = xref_cleaned$land_cover_class\n  ) +\n  ggplot2::geom_sf(\n    data = layers_trimmed$l_rail,\n    color = \"black\",\n    size = 1\n  ) +\n  ggplot2::geom_sf(\n    data = layers_trimmed$l_streams,\n    color = \"blue\",\n    size = 1\n  ) +\n  ggdark::dark_theme_void()\n\n# save the plot\n# ggsave(here::here('posts', params$post_dir_name, \"image.jpg\"), width = 10, height = 10)\n    \nmap"
  },
  {
    "objectID": "posts/2024-06-30-land-cover/index.html#neexdzii-kwah",
    "href": "posts/2024-06-30-land-cover/index.html#neexdzii-kwah",
    "title": "Mapping Land Cover with R",
    "section": "Neexdzii Kwah",
    "text": "Neexdzii Kwah\nNow lets make a table and a simple bargraph to present the results.\n\n\nCode\n# Calculate the area of each cell (assuming your raster is in lat/lon coordinates)\ncell_area &lt;- terra::cellSize(land_cover_raster_resampled, unit = \"ha\")\n\n# Summarize the raster values\nland_cover_summary_raw &lt;- terra::freq(land_cover_raster_resampled, digits = 0) |&gt; \n  dplyr::mutate(area_ha = round(count * cell_area[1]$area),1) |&gt; \n  # make a column that is the percentage of the total area\n  dplyr::mutate(percent_area = round((area_ha / sum(area_ha) * 100), 1))\n\n# now we add the xref code and land_cover_class to the summary\nland_cover_summary &lt;- land_cover_summary_raw |&gt; \n  dplyr::left_join(xref, by = c(\"value\" = \"code\")) |&gt; \n  dplyr::select(land_cover_class, area_ha, percent_area) |&gt; \n  dplyr::arrange(desc(area_ha))\n\n\n\n\nCode\nmy_caption &lt;- \"Land Cover Class by Refined Area of Interest\"\n\nland_cover_summary |&gt; \n  knitr::kable(caption = my_caption)\n\n\n\nLand Cover Class by Refined Area of Interest\n\n\nland_cover_class\narea_ha\npercent_area\n\n\n\n\nTree cover\n194158\n84.3\n\n\nGrassland\n28940\n12.6\n\n\nPermanent water bodies\n3731\n1.6\n\n\nMoss and lichen\n2850\n1.2\n\n\nBuilt-up\n329\n0.1\n\n\nCropland\n264\n0.1\n\n\nBare / sparse vegetation\n158\n0.1\n\n\nHerbaceous wetland\n9\n0.0\n\n\nShrubland\n1\n0.0\n\n\n\n\n\n\n\nCode\nplot_title &lt;- \"Land Cover Class for Area of Interest\"\n\n# \nland_cover_summary |&gt; \n  # convert land_cover_class to factor and arrange based on the area_ha\n  dplyr::mutate(land_cover_class = forcats::fct_reorder(land_cover_class, area_ha)) |&gt;\n  ggplot2::ggplot(ggplot2::aes(x = land_cover_class, y = area_ha)) +\n  ggplot2::geom_col() +\n  ggplot2::coord_flip() +\n  ggplot2::labs(title= plot_title,\n                x = \"Land Cover Class\",\n                y = \"Area (ha)\") +\n  ggplot2::theme_minimal() \n\n\n\n\n\n\n\n\n\nCode\n  # cowplot::theme_minimal_grid()"
  },
  {
    "objectID": "posts/2024-06-30-land-cover/index.html#refined-area-of-interest",
    "href": "posts/2024-06-30-land-cover/index.html#refined-area-of-interest",
    "title": "Mapping Land Cover with R",
    "section": "Refined Area of Interest",
    "text": "Refined Area of Interest\nNow lets make a table and a simple bargraph to present the results.\n\n\nCode\n# Calculate the area of each cell (assuming your raster is in lat/lon coordinates)\ncell_area &lt;- terra::cellSize(land_cover_sample, unit = \"ha\")\n\n# Summarize the raster values\nland_cover_summary_raw &lt;- terra::freq(land_cover_sample, digits = 0) |&gt; \n  dplyr::mutate(area_ha = round(count * cell_area[1]$area),1) |&gt; \n  # make a column that is the percentage of the total area\n  dplyr::mutate(percent_area = round((area_ha / sum(area_ha) * 100), 1))\n\n# now we add the xref codde and land_cover_class to the summary\nland_cover_summary &lt;- land_cover_summary_raw |&gt; \n  dplyr::left_join(xref, by = c(\"value\" = \"code\")) |&gt; \n  dplyr::select(land_cover_class, area_ha, percent_area) |&gt; \n  dplyr::arrange(desc(area_ha))\n\n\n\n\nCode\nmy_caption &lt;- \"Land Cover Class for Refined Area\"\n\nland_cover_summary |&gt; \n  knitr::kable(caption = my_caption)\n\n\n\nLand Cover Class for Refined Area\n\n\nland_cover_class\narea_ha\npercent_area\n\n\n\n\nTree cover\n283\n66.6\n\n\nGrassland\n89\n20.9\n\n\nBuilt-up\n19\n4.5\n\n\nPermanent water bodies\n19\n4.5\n\n\nCropland\n10\n2.4\n\n\nBare / sparse vegetation\n4\n0.9\n\n\nMoss and lichen\n1\n0.2\n\n\n\n\n\n\n\nCode\nplot_title &lt;- \"Land Cover Class for Refined Area\"\n\n# \nland_cover_summary |&gt; \n  # convert land_cover_class to factor and arrange based on the area_ha\n  dplyr::mutate(land_cover_class = forcats::fct_reorder(land_cover_class, area_ha)) |&gt;\n  ggplot2::ggplot(ggplot2::aes(x = land_cover_class, y = area_ha)) +\n  ggplot2::geom_col() +\n  ggplot2::coord_flip() +\n  ggplot2::labs(title= plot_title,\n                x = \"Land Cover Class\",\n                y = \"Area (ha)\") +\n  ggplot2::theme_minimal()"
  },
  {
    "objectID": "posts/2024-09-21-aws-cors-cog-leaflet/index.html",
    "href": "posts/2024-09-21-aws-cors-cog-leaflet/index.html",
    "title": "CORS for serving of COGs of UAV imagery on AWS with R",
    "section": "",
    "text": "Whoa Billy. Time to host our UAV imagery on AWS and serve it out through leaflet and do dank moves like put before after images on slippy maps next to each other for world peace. Ha. Well that is a bit dramatic but hey. Still pretty cool.\nFirst thing is to convert the image to a cog and sync it up to a bucket. Not doing that here. Will do soon though. What we do here is - after we are sure there are public permissions allowed to the bucket but we also need to deal with big bad CORS. We can set a viewing Cross-origin resource sharing (CORS). “CORS defines a way for client web applications that are loaded in one domain to interact with resources in a different domain”. This is done with the following JSON.\n[\n    {\n        \"AllowedHeaders\": [\n            \"*\"\n        ],\n        \"AllowedMethods\": [\n            \"GET\"\n        ],\n        \"AllowedOrigins\": [\n            \"*\"\n        ],\n        \"ExposeHeaders\": [\n            \"x-amz-server-side-encryption\",\n            \"x-amz-request-id\",\n            \"x-amz-id-2\"\n        ],\n        \"MaxAgeSeconds\": 3000\n    }\n]\nWe went the daft way here and just copied and pasted this into the CORS section of our bucket in the console but we should be able to use the paws package and the s3_put_bucket_cors function (I would think). We are going to leave that for another day.\n\n\nCode\nlibrary(paws)\n\n\nWarning: package 'paws' was built under R version 4.4.1\n\n\nCode\nlibrary(s3fs)\nlibrary(leaflet)\nlibrary(leafem)\n\n\nList your buckets kid.\n\n\nCode\ns3 &lt;- paws::s3()\nbuckets &lt;- s3$list_buckets()\npurrr::map_chr(buckets$Buckets, \"Name\")\n\n\n[1] \"23cog\"        \"new-graphiti\"\n\n\n\n\nCode\ns3fs::s3_dir_ls(refresh = TRUE) \n\n\n[1] \"s3://23cog\"        \"s3://new-graphiti\"\n\n\nCode\nbucket_path &lt;- s3fs::s3_path(\"23cog\")\n\n\n\n\nCode\n# too much info\ns3fs::s3_dir_tree(bucket_path)\n\n\nNow list em with full paths\n\n\nCode\n# full paths\ns3_dir_ls(bucket_path)\n\n\n [1] \"s3://23cog/.DS_Store\"                                  \n [2] \"s3://23cog/20210906lampreymoricetribv220230317-DEM.tif\"\n [3] \"s3://23cog/20210906lampreymoricetribv220230317.las\"    \n [4] \"s3://23cog/20210906lampreymoricetribv220230317.tif\"    \n [5] \"s3://23cog/FHAP_template.csv\"                          \n [6] \"s3://23cog/bc_093l026_xli1m_utm09_2018.tif\"            \n [7] \"s3://23cog/bc_093l048_xli1m_utm09_2019.tif\"            \n [8] \"s3://23cog/bc_093l056_xli1m_utm09_2019_cog.tif\"        \n [9] \"s3://23cog/bc_093l058_xli1m_utm09_2019.tif\"            \n[10] \"s3://23cog/bc_093m032_xli1m_utm09_2019_cog2.tif\"       \n[11] \"s3://23cog/ept.json\"                                   \n[12] \"s3://23cog/glen_valle_lidar_2019_cog.tif\"              \n[13] \"s3://23cog/xli1m_utm09_2019_093L04811.tif\"             \n[14] \"s3://23cog/xli1m_utm09_2019_093L04812.tif\"             \n[15] \"s3://23cog/xli1m_utm09_2019_093L04813.tif\"             \n[16] \"s3://23cog/xli1m_utm09_2019_093L04814.tif\"             \n[17] \"s3://23cog/xli1m_utm09_2019_093L04821.tif\"             \n[18] \"s3://23cog/xli1m_utm09_2019_093L04822.tif\"             \n[19] \"s3://23cog/xli1m_utm09_2019_093L04823.tif\"             \n[20] \"s3://23cog/xli1m_utm09_2019_093L04824.tif\"             \n[21] \"s3://23cog/xli1m_utm09_2019_093L04831.tif\"             \n[22] \"s3://23cog/xli1m_utm09_2019_093L04832.tif\"             \n[23] \"s3://23cog/xli1m_utm09_2019_093L04833.tif\"             \n[24] \"s3://23cog/xli1m_utm09_2019_093L04834.tif\"             \n[25] \"s3://23cog/xli1m_utm09_2019_093L04841.tif\"             \n[26] \"s3://23cog/xli1m_utm09_2019_093L04842.tif\"             \n[27] \"s3://23cog/xli1m_utm09_2019_093L04843.tif\"             \n[28] \"s3://23cog/xli1m_utm09_2019_093L04844.tif\"             \n[29] \"s3://23cog/private\"                                    \n\n\nBiuld a functshi to give us the actual https url. Sure there is a function somewhere already to do this but couldn’t find it.\n\n\nCode\n# Define your S3 path\ns3_path &lt;-  \"s3://23cog/20210906lampreymoricetribv220230317.tif\"\n\ns3_path_to_https &lt;- function(s3_path) {\n  # Remove the 's3://' prefix\n  path_without_prefix &lt;- sub(\"^s3://\", \"\", s3_path)\n  \n  # Split the path into bucket and key\n  parts &lt;- strsplit(path_without_prefix, \"/\", fixed = TRUE)[[1]]\n  bucket_name &lt;- parts[1]\n  object_key &lt;- paste(parts[-1], collapse = \"/\")\n  \n  # Construct the HTTPS URL\n  https_url &lt;- sprintf(\"https://%s.s3.amazonaws.com/%s\", bucket_name, object_key)\n  return(https_url)\n}\n\nurl &lt;- s3_path_to_https(s3_path)\nprint(url)\n\n\n[1] \"https://23cog.s3.amazonaws.com/20210906lampreymoricetribv220230317.tif\"\n\n\nSince we already have some valid COGs up on AWS we will link to one to be sure it works.\n\n\nCode\n  leaflet::leaflet() |&gt;\n    leaflet::addTiles() |&gt;\n    leafem:::addCOG(\n      url = url\n      , group = \"COG\"\n      , resolution = 512\n      , autozoom = TRUE\n    )\n\n\n\n\n\n\nDope.\nWhen we work with the paws package - when we want to get help we use ?s3 and navigate in from there. This next section doesn’t work yet so we turn eval = F and get on with our lives. Would like to activate this policy from R but can’t seem to pull it of yet. To be continued - maybe.\n\n\nCode\nmy_bucket_name = \"23cog\" \n\n# Define the CORS policy as a list\nmy_policy &lt;- list(\n    list(\n        AllowedHeaders = list(\"*\"),  # Must be a list\n        AllowedMethods = list(\"GET\"),  # Must be a list\n        AllowedOrigins = list(\"*\"),  # Must be a list\n        ExposeHeaders = list(\n            \"x-amz-server-side-encryption\",\n            \"x-amz-request-id\",\n            \"x-amz-id-2\"\n        ),  # Must be a list\n        MaxAgeSeconds = 3000  # Must be a number\n    )\n)\n\n\n# Convert the policy list to a pretty JSON format\nmy_policy_json &lt;- jsonlite::toJSON(my_policy, pretty = TRUE, auto_unbox = TRUE)\n\n\n# Set the CORS configuration directly using the list\npaws::s3()$put_bucket_cors(\n    Bucket = my_bucket_name,\n    CORSConfiguration = my_policy_json  # Pass the list of rules directly\n)"
  },
  {
    "objectID": "posts/snakecase/index.html",
    "href": "posts/snakecase/index.html",
    "title": "snake_case vs Everything_Else",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, dpi=60, out.width = \"100%\")\noptions(scipen=999)\noptions(knitr.kable.NA = '--') #'--'\noptions(knitr.kable.NAN = '--')\n\n\nBeen preferring snakecase naming convention for all files and columns and variables for a long time for the following reasons:\n\nIt’s much easier to type. Reaching for the shift button is a pain.\nArguably easier to read. I find it easier to read snake_case than CamelCase.\nCan make it easy to name things in a fashion that allows you to dissect, what, something, is by the way it is named and allows automatic sorting to group similar things together. This presentation by Jenny Bryan is a good read - https://speakerdeck.com/jennybc/how-to-name-files-the-sequel.\n\n\n\nCode\nknitr::include_graphics(\"thumbnail.jpg\")\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nWorth noting that sometimes-rules-need-to-be-broken sometimes such as when you are naming chunks in Rmarkdown. It breaks our cross-references.\n\nAs we see here though - it doesn’t matter in Quarto (ex. see Figure 1) vs Figure 2).\n\n\nCode\nknitr::include_graphics(\"all.jpeg\")\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\n“no names have an anonymous function”.\n    -Michael Sumner"
  }
]